# 10. 有向图模型（贝叶斯网络）

## 10.1 引言

我基本上知道用简单的方法处理复杂系统的两个原则:第一个是模块化原则，第二个是抽象原则。我是机器学习中计算概率的辩护者，因为我相信概率论以深刻而有趣的方式实现了这两个原则——即分解和平均。在我看来，尽可能充分地利用这两种机制是机器学习的前进方向。——Michael Jordan,1997 (quoted in (Frey 1998)).

假设我们观察到多个相关变量，例如文档中的单词、图像中的像素或微阵列中的基因。我们如何能简洁地表示**联合分布**$p(\mathbf{x}|\mathbf{\theta})$，我们如何使用这个分布在合理的计算时间内基于一组给定的变量来推断另一组变量?我们如何用合理的数据量来学习这个分布的参数呢?这些问题是概率建模、推理和学习的核心，也是本章的主题。

### 10.1.1 链式法则

根据概率论中的链式法则，我们可以用任意变量的顺序，表示如下的联合分布：
$$
p(x_{1:V})=p(x_1)p(x_1|x_2)p(x_3|x_2,x_1)p(x_4|x_1,x_2,x_3)...p(x_V|x_{1:V-1})\tag{10.1}
$$
其中$V$表示变量的数量，符号$1:V$表示集合$\{1,2,...,V\}$，其中为了符号上的简洁，我们省略了固定的条件参数$\mathbf{\theta}$。这种表达方式的问题在于当$t$变得很大时，条件分布$p(x_t|\mathbf{x}_{1:t-1})$会变得越来越复杂。

举例来说，假设所有的变量具有$K$个状态。我们可以使用一个数量级为$O(K)$的分布表表示分布$p(x_1)$，它表示一个离散分布（事实上考虑到和为1的约束,只有$K-1$个自由参数，但为了简洁，我们直接写成$O(K)$)。类似地，通过书写$p(x_2=j|x_1=i)=T_{ij}$，我们可以使用数量级为$O(K^2)$的表格表示$p(x_2|x_1)$，我们称$\mathbf{T}$为$\mathbf{随机矩阵}(stochastic\ matrix)$，因为对于所有的行$i$，都满足$\sum_{j}T_{ij}=1,0\le\ T_{ij}\ \le\ 1$。类似的，我们将$p(x_3|x_1,x_2)$表示为一个数量为$O(K^3)$的3维表格。这些被统称为**条件概率表（conditional probability tables，CPTs）.**我们发现在模型中存在$O(K^V)$个参数。我们将需要相当多的数据来学习如此多的参数。

一种解决方案是将每一个CPT替换为一个更加吝啬的**条件概率分布（conditional probability distribution,CPD）**，比如多项式逻辑回归$p(x_t=k|\mathbf{x}_{1:{t-1}})=\mathcal{S}(\mathbf{W}_t\mathbf{x}_{1:{t-1}})_k$.现在模型的总参数数量级为$O(K^2V^2)$，从而使这个密度模型更加紧凑。如果我们只是想评估完整的观测向量$\mathbf{x}_{1:T}$的概率，这个模型是足够的。举例来说，我们可以使用这个模型来定义一个类条件密度$p(\mathbf{x}|y=c)​$，从而构造出一个生成式分类器。然而，这个模型不适用与其他的预测任务，因为每个变量都与之前的变量相关。所以我们需要其他的方法。**该段未完全理解**

### 10.1.2 条件独立性

有效地表达大规模联合概率分布的关键在于需要做一些**条件独立（conditional independence,CI）**假设。重温2.2.4节，在给定变量$Z​$的情况下，$X​$和$Y​$是条件独立的，符号上表示为$X\perp Y|Z​$，其成立的充要条件是联合分布可以写成条件边缘分布的乘积，也就是说：
$$
X \perp Y|Z \Longleftrightarrow p(X,Y|Z)=p(X|Z)p(Y|Z)\tag{10.2}
$$
下面让我们说明为什么这是有效的。假设我们令$x_{t+1}\perp \mathbf{x}_{1:{t-1}}|x_t​$，也就是说，“在当前值已知的情况下，未来与过去是独立的”。这被称为（一阶）**马尔科夫假设（Markov assumption）**。使用这个假设，配合链式法则，我们可以将联合概率分布写成：
$$
p(\mathbf{x}_{1:V})=p(x_1)\prod_{t=2}^V p(x_t|x_{t-1})\tag{10.3}
$$
这被称为（一阶）**马尔科夫链（Markov chain）**。上式可以通过一个初始状态$p(x_1=i)$外加一个**状态转移矩阵**$p(x_t=j|x_{t-1}=i)$来表示。见章节17.2查看更多信息。

### 10.1.3 图模型

尽管一阶马尔科夫假设对于定义1维序列的分布十分有用，那我们如何定义2维图片或者3维语音上的分布呢，或者，更一般地，对于任意的变量集合（比如属于某个生物路径的基因）？这就是图模型发挥作用的地方。

**图模型（graphical model,GM）**是一种基于条件独立假设对联合概率分布进行表示的方法。特别地，图中的节点代表随机变量，节点之间的边的缺失代表条件独立假设。（事实上这些模型的更好的名字应该是“独立图”，但术语“图模型”现在已经根深蒂固了。）存在几种图模型，取决于图形是否是有向的、无向的或者两种情况的组合。本节，我们只研究有向图。我们将在第19章讨论无向图。

### 10.1.4 图中的术语

在继续后面内容之前，我们必须定义一些基本术语，其中的大部分术语都是符合我们的直觉的。

一个**图(graph)**$G=(\mathcal{V},\mathcal{E})​$由**节点（nodes）**或者**顶点（vertices）**集合$\mathcal{V}=\{1,...,V\}​$和**边缘（edges）**集合$\mathcal{E}=\{(s,t):s,t \in \mathcal{V}\}​$组成。我们可以通过**邻接矩阵（adjacency matrix）**来表示一个图，在这种表示方式中，如果$s\rightarrow t​$，我们令$G(s,t)=1​$表示$(s,t)\in\mathcal{E}​$， 如果$G(s,t)=1​$的同时$G(t,s)=1​$，则称图是**无向的(undirected)**，否则它就是**有向(directed)**的。我们通常假设$G(s,s)=0​$，意味着图中不存在**自循环(self loops)**。

下面是一些我们通常使用的其他术语：

- **父（Parent）节点**，对于有向图而言，一个节点的父节点是指所有指向该节点的节点集合：$pa(s) \triangleq \{t:G(t,s)=1\}$。
- **子(child)节点**，对于有向图而言，一个节点的子节点是指该节点指向的所有节点集合：$pa(s) \triangleq \{t:G(s,t)=1\}$。
- **族（family）**，在有向图中，一个节点的**族**是指该节点和它的父节点，$fam(s)=\{s\}\cup pa(s)$.
- **根(root)节点**，在有向图中，根节点是指没有父节点的节点.
- **叶子(leaf)节点**，在有向图中，叶子节点是指没有子节点的节点.
- **祖宗（Ancestors）节点**，在有向图中，祖宗节点是指一个节点的父节点，祖父节点等等.
- **后代（Descendants）节点**，在有向图中，后代节点是指一个节点的子节点，孙子节点等等.也就是说$s$的后代节点是指一个节点集合，在该集合中，所有节点都可以通过$s:desc(s) \triangleq \{t:s \leadsto t\}$到达.
- **邻居（Neighbors）节点**，对于任意一种图，我们定义一个节点的**邻居**是指所有与其直接连接的节点的集合，即：$nbr(s)\triangleq \{t:G(s,t)=1 \vee G(t,s)=1\}$.对于一个无向图而言，我们用符号$s \sim t$表示$s$和$t$为邻居。（所以$(s,t) \in \mathcal{E}$为图中一个边）.
- **度(Degree)** ，一个节点的**度**是指其邻居节点的数量。对于有向节图而言，我们称**入度(in-degree)**和**出度(out-degree)**分别表示父节点和子节点的数目.
- **循环（cycle or loop）**，在任何一种图中，我们定义一个循环为一系列的节点，通过这些节点的路径，我们可以完成一个循环，回到起始节点，如果图形是有向的，我们称其为有向环。举例来说，在图10.1（a）中，不存在有向环，但$1 \rightarrow 2\rightarrow 4 \rightarrow3 \rightarrow1$是一个无向循环.
- **有向无环图（DAG,directed acyclic graph）**是一个不含有向环的有向图。图10.1（a）就是一个例子.
- **拓扑顺序（Topological ordering）**，在有向无环图中，一个**拓扑顺序**或者**全顺序（total ordering）**是指所有节点的编号方式，在这种方式中父节点的编号小于子节点的编号。举例来说，在图10.1（a)中，我们可以使用$(1,2,3,4,5)$或者$(1,3,2,5,4)​$等等.
- **路径（Path or trail）**，一条路径$s\leadsto t$是指从$s$到$t$的一系列有向边.
- **树(Tree)**，一个无向树是指没有循环的无向图。一个有向树是指没有有向环的有向无环图。如果我们允许一个节点具备多个父节点，我们称其为**多树(polytree)**.
- **森林(Forest)**，一个森林是指一个树的集合.
- **子图(Subgraph)**，一个子图$G_A$是指由在集合$A$中所有节点和其对应的边$G_A=(\mathcal{V}_A,\mathcal{E}_A)$构成的图.
- **团(Clique)**，对于一个无向图而言，一个团就是指在该团中所有节点都互为邻居节点。一个**最大团(maximal clique)**是指一个团，该团在保持团的基本性质的前提下，无法变得更大。举例来说，在图10.1(b)中，{1,2}是一个团但不是最大团，因为如果我们增加了节点3，它依然是一个团。事实上，最大团包括$\{1,2,3\},\{2,3,4\},\{3,5\}$.

### 10.1.5 有向图模型

一个**有向图模型（directed graphical model,DGM）**是一个图模型，其对应的图是一个DAG。这些通常又被称为**贝叶斯网络(Bayesian networks)**.然而，贝叶斯网络本身并没有什么是符合“贝叶斯”思想的：它们只是定义概率分布的一种方式。这些模型又被称为**信念网络（belief networks）**。术语"信念"代表主观的概率值。同样地，对于DGMs所表示的概率分布类型并没有本质上的主观因素。最后，这些模型有时也被称为”**因果网络（causal networks）**“，因为有向的箭头有时也被解释为一种因果关系。然而，DGMs没有内在的因果关系。（章节26.6.1讨论了一种因果DGMs。）基于这些理由，我们通常使用更加中立（但不那么迷惑）的术语DGM.

DAGs的核心性质在于其节点可以进行排序，其父节点出现在子节点之前。这被称为拓扑顺序，它可以从任意的DAG中构造出来。基于这个顺序，我们定义一种被称为**有序马尔科夫性质（ordered Markov property）**的假设，在该假设中，一个节点只与它紧邻的父节点相关，而不是所有的在它之前的节点：
$$
x_s\perp \mathbf{x}_{pred(s)\backslash pa(s)}|\mathbf{x}_{pa(s)} \tag{10.4}
$$
其中$pa(s)$为节点$s$的父节点，$pred(s)$为节点$s$的前置节点（译者注：即按照拓扑顺序排在前面的节点）。这是一阶马尔科夫性质从链式结构向一般DAGs的泛化。

举例来说，图10.1(a)中的DAG对如下的联合分布进行了编码：
$$
\begin{align}
p(\mathbf{x}_{1:5}) & = p(x_1)p(x_2|x_1)p(x_3|x_1,\cancel{x_2})p(x_4|\cancel{x_1},x_2,x_3)p(x_5|\cancel{x_1},\cancel{x_2},x_3,\cancel{x_4}) \tag{10.5} \\
& = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2,x_3)p(x_5|x_3) \tag{10.5}
\end{align}
$$
一般情况下，我们有：
$$
p(\mathbf{x}_{1:V}|G)=\prod_{t=1}^{V} p(x_t|\mathbf{x}_{pa(t)}) \tag{10.6}
$$
其中每一项$p(x_t|\mathbf{x}_{pa(t)})$是一个CPD（条件概率分布）.我们将这个分布写成$p(\mathbf{x}|G)$的形式，以此来强调这个公式仅仅在有向无环图$G$中编码的CI假设成立正确时才成立。然而，为了简洁我们一般会丢掉这个明确的条件项。如果每个节点有$O(F)$个父节点和$K$种状态，那么模型中的参数数量数量级为$O(VK^F)$，这比在没有进行CI假设情况下的$O(K^V)$要少得多。

## 10.2 例子

本节，我们将展式很多经常使用的概率模型可以转换为DGMs。

### 10.2.1 朴素贝叶斯分类器

在3.5节，我们介绍了朴素贝叶斯分类器。这个模型假设当类别已知的情况下，特征之间是条件独立的。这个假设在图10.2(a)中给出说明。这允许我们将联合分布写成如下的形式：
$$
p(y,\mathbf{x})=p(y)\prod_{j=1}^{D}p(x_j|y) \tag{10.8}
$$
朴素贝叶斯假设是相当朴素的，因为它假设特征是条件独立的。一种获取特征之间相关性的方法是使用图模型。特别的，如果模型是一棵树，对应的方法被称为**增强树朴素贝叶斯分类器（tree-augmented naive Bayes classifier,TAN）**模型。这在图10.2（b）中给出说明。之所以使用树而不是一般的图主要基于两方面的原因。首先，使用Chow-Liu算法可以很容易地找到最优的树结构，这一点将在26.3节介绍。第二，在树结构中很容易解决特征缺失的问题，这一点将在20.2节介绍。

### 10.2.2 马尔科夫和隐马尔科夫模型









