# 03 离散数据的生成式模型

## 3.1 引言

在 `第 2.2.3.2 节` 中，讨论了如何将贝叶斯法则应用到生成式分类器，从而实现对一个特征向量 $\mathbf {x}$ 的分类：

$$
p (y=c \mid \mathbf {x}, \boldsymbol {\theta}) \propto p (\mathbf {x} \mid y=c, \boldsymbol {\theta}) p (y=c \mid \boldsymbol {\theta}) \tag {3.1} \label {eqn:3.1}
$$

使用上述模型的关键之处在于为类条件密度（即似然）$p (\mathbf {x}|y=c,\boldsymbol {\theta})$ 指定一个合适的形式，它定义了我们预期在每个类中看到什么样的数据。本章将集中讨论可观测数据属于离散型变量的情况。同时，将讨论在此类模型中如何推断未知参数的值。

## 3.2 贝叶斯概念学习

思考一个问题：一个小孩如何学习理解一个单词的含义，比如单词 `dog` 。

我们可以做如下推测：小孩的父母指向关于此概念 ( 即单词 `dog` ) 的一些正确样例 ( 正例 ) ，然后说 `look at the cute dog!` 或者 `mind the doggy` 等之类的话。很少出现父母指着一个错误的样例，然后说道 `look at that non-dog`。 当然，错误样例在主动学习过程中可能会用到 --- 例如小孩说 `look at the dog` ，然后父母纠正道： `that's a cat, dear, not a dog` --- 然而**心理学研究表明人们可以只从正确样例中去理解概念**。

可以将学习一个单词的含义视为 `概念学习 ( concept learning )` ，后者又等价于二值分类。要看到这一点：如果 $\mathbf {x}$ 是关于概念 $C$ 的一个正例，则定义 $f ( x ) =1$ ，否则，定义 $f ( x ) =0$ 。然后目标是通过学习指示函数 $f$ ，该函数定义了哪些样例属于概念 $C$。通过引入函数 $f$ 自身定义的不确定性 ( 或者说概念 $C$ 中元素的不确定性 )，我们就可以用概率计算来模拟 `模糊集合论 ( fuzzy set theory )`，并得到推断结果。值得注意的是，标准的二值分类问题需要正例和负例同时存在，此处我们将设计一种只从正例中学习的方式。

为了教学目的，考虑一个关于概念学习的简单例子，叫做 `数字游戏 ( number game )` ，此游戏源自 `Josh Tenenbaum` 的博士论文。游戏过程如下：

首先选择一些简单的数学概念 $C$ ，比如 `素数` 或者 `一个在 1 到 10 之间的数字` 。然后给你一系列从概念 $C$ 中随机抽取的正例 $\mathcal{D}=\{x_1,...,x_N\}$，然后问你一些新的测试样例 $\tilde{x}$ 是否属于概念 $C$，即让你对 $\tilde{x}$ 进行分类。

![] ( https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013044375.webp )

图 3.1  8 个参与数字游戏的人的预测结果及其分布。前两行代表看到数据 $\mathcal{D} = \{16\}$ 和 $\mathcal{D} = \{60\}$ 后的统计结果。这两个图说明了分布比较弥散的相似性。第 3 行：看到数据 $\mathcal{D} = \{16,8,2,64\}$ 之后的预测结果。该图说明了某种基于规则的行为 ( 如：2 的幂次方 ) 。第 4 行：看到数据 $\mathcal{D} = \{16,23,19,20\}$ 后的统计结果。这说明了某种聚集的相似度 ( 如：接近 20 的数字 )

为简单起见，假设所有数字都是在 1 和 100 之间的整数。现在告诉你数字 `16` 是概念中的正例。那么你还会觉得哪些数会是正例？ `17? 6? 32?` 还是 `99？` 。显然你很难判断哪个是正确的，所以预测会十分含糊。那些在某些方面与数字 `16` 更相似的数字更有可能是正确的，但 `相似` 又该如何定义呢？`17` 与 `16` 是相似的，因为它离 `16` 近。`6` 与 `16` 是相似的，因为它们有一位数完全相同。`32` 与 `16` 是相似的，因为它们都是偶数且都是 `2 的幂次方`，但数字 `99` 好像不相似。所以存在一些数字与 `16` 的相似度要更高。

我们对上述问题进行形式化表达：

引入概率分布 $p ( \tilde {x} \mid \mathcal {D} )$ ，代表对任意数字 $\tilde {x} \in\{1, \ldots, 100\}$，在给定数据 $\mathcal {D}$ 的情况下，$\tilde {x} \in C$ 的概率，被称为 `后验预测性分布`( posterior predictive distribution ) 。`图 3.1 ( 上 )` 展示了参与该实验的志愿者给出的预测性分布。不难发现，人们基于不同相似度的标准，将与 `16` 相似的不同数作为正例输出。

现在假设告诉你 `8` , `2` 和 `64` 也是正例。你可能会猜测此隐藏的数学概念是 `2 的幂次方` 。这其实就是 `归纳 ( induction )` 过程。基于此假设，预测性分布将会更加明确，如 `图 3.1` 第 3 行所示，大部分概率质量都分配到了 `2 的幂次方` 上。如果给你的数据集 $\mathcal {D}=\{16,23,19,20\}$，你可能会得到一个完全不同的 `泛化梯度 ( generalization gradient )` ，如 `图 3.1 ( 下 )` 所示。

```{note}
泛化梯度指相似性程度不同的刺激引起不同强度的响应，它表明了泛化的水平，是泛化反应强度变化的指标。
```

如何解释此行为并且在机器学习中去模仿它？

归纳的经典方法是假设我们有一个概念的假设空间 $\mathcal {H}$ ，比如：奇数、偶数、所有1到100之间的数、2的幂、所有以数字 $j$ 结尾的数等。在假设空间中，与数据 $\mathcal {D}$ 保持一致的子集被称为`版本空间`。随着观测到的样本增加，版本空间在逐步缩小，我们对概念的理解也越来越确定 (Mitchell 1997) 。

然而，版本空间并非故事的全部。在看到 $\mathcal {D}=\{16\}$ 后，有许多规则能够满足一致性，该如何结合它们来预测 $\tilde {x} \in C$ 呢？ 此外，在看到 $\mathcal {D}= \{16,8,2,64\}$ 后，你为什么选择 「2 的幂」 ，而不是 「所有偶数」 ，或 「除 32 之外 2 的幂」，毕竟这两个规则也能够与证据保持一致啊？ 

下面将对此给出贝叶斯解释。

### 3.2.1 似然

现在必须解释，在看到数据 $\mathcal {D}=\{16,8,2,64\}$ 时，为什么会选择假设 $h_{\text {two }} \triangleq \text{2 的幂次方}$，而不是 $h_{\text {even }} \triangleq \text{偶数}$。比较直观的解释是希望避免 `可疑的巧合 ( suspicious coincidences )` ：如果真实概念是偶数，那为什么我们刚好只看到那些 `2 的幂次方` 呢？

为形式化上述表达，假设样例是在概念的延拓中均匀随机采样得到的。Tenenbaum 称这种采样为 `强采样假设 ( strong sampling assumption )` ，从假设 $h$ 中独立采样 $N$ 个样本的概率为 ( 有放回 ) ：

$$
p ( \mathcal {D} \mid h ) =\left [\frac {1}{\operatorname {size} ( h )}\right]^{N}=\left [\frac {1}{|h|}\right]^{N} \tag {3.2}
$$

此公式体现了 Tenenbaum 所说的 `尺度原则 ( size principle )` ，意思是说模型更倾向于与数据保持一致的最简单假设。此原则被称为 `奥卡姆剃刀 ( Occam's razor )` 。

为了说明上述原则如何奏效，令 $\mathcal {D}=\{16\}$，有 $p\left ( \mathcal {D} \mid h_{\text {two }}\right ) =1 / 6$，因为在小于 100 的整数中，只有 6 个数是 2 的幂次方，但 $p\left ( \mathcal {D} \mid h_{\text {even }}\right ) =1 / 50$，因为有 50 个偶数。所以 $h=h_{\text {two }}$ 的似然大于 $h=h_{\text {even }}$ 的似然。在观察到 4 个样例后，$h_{\text {two }}$ 的似然为 $( 1 / 6 )^{4}=7.7 \times 10^{-4}$ ，而 $h_{\text {even }}$ 的似然为 $( 1 / 50 )^{4}=1.6 \times 10^{-7}$ 。两个假设之间的 `似然比 ( likelihood ratio )` 接近 $5000:1$ ，更倾向于 $h_{\text {two }}$。`似然比`量化了人们的直觉，即数据 $D=\{16,8,2,64\}$ 如果从假设 $h_{\text {even }}$ 中产生的话，会是一个非常可疑的巧合。


```{note}
注：概念的延拓指属于该概念的所有实例构成的集合，例如：
- 概念 ` 偶数 ` 的延拓就是 $\{2,4,6, \ldots, 98,100\}；
- 概念 `以 9 结尾的数` 的延拓是 $\{9,19, \ldots, 99\}$。
```
### 3.2.2 先验

假设 $\mathcal {D}=\{16,8,2,64\}$ ，概念 $h'=\text {“除了 32 以外的 2 的幂次方”}$ 比概念 $h = \text {“ 2 的幂次方”}$ 更有可能，因为不需要解释为什么 32 恰好没出现的巧合 ( 如果我们猜测正确的概念是 $h$ ，那么需要解释为什么偏偏 32 没出现在 $\mathcal {D}$ 中 ) 。

然而，假设 $h'=\text {“除了 32 以外的 2 的幂次方”}$ 在概念上好像并 `不自然` 。为表达这种不自然，我们可以对该概念赋予一个比较低的先验概率。当然，你的先验概率可能与我的不同。这种 `主观性 ( subjective )` 是贝叶斯推断饱受争议的原因所在，因为它意味着，一个小孩和一个数学教授会得到不同的答案。事实上，小孩和教授可能不仅有不同的先验概率，还会有不同的假设空间。然而，可以通过将孩子和数学教授的假设空间定义为相同，然后在某些“高级”概念上将孩子的优先权重设置为 0 来巧妙解决此问题。因此，先验和假设空间之间没有明显的区别。

尽管先验存在争议性，但却十分有用。如果你观测到符合某些数学规则的一些数字，比如 1200, 1500, 900 和 1400，你可能会认为 400 有可能也服从此规则，但 1183 不太可能。但如果你观测到的是一个健康人的胆固醇水平，那你很有可能认为 400 不可能，但 1183 有可能。所以可以发现先验分布形成了一个快速学习的机制，即我们将影响问题的一些背景知识考虑进来，而如果没有这些背景知识，几乎不可能实现快速学习 ( 比如从小的样本中学习 ) 。

那么该如何使用先验呢？出于演示目的，我们使用一个简单的先验分布，该先验分布中 30 个简单的数学概念服从均匀分布，比如说： `偶数` ， `奇数` ， `素数` ， `以 9 结束的数` 等。为了让事情更加有趣，赋予 `偶数` 和 `奇数`两个概念更高的先验权值，同时，包含两个有些不自然的概念，比如说 `2 的幂次方以及 37` 和 `除了 32 的 2 的幂次方` ，但赋予这两个概念较低的先验权值。`图 3.2 ( 左 )` 绘制了该先验分布。

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210610130450c1.webp)

图 3.2 $D=\{16\}$ 情况下的先验分布，似然函数和后验分布。

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013045405.webp)

图 3.3 $D=\{16,8,2,64\}$ 情况下的先验分布，似然函数和后验分布。

### 3.2.3 后验分布

后验分布只是似然与先验乘积并进行归一化后的结果。在当前的数字游戏场景下，有：

$$
p ( h \mid \mathcal {D} ) =\frac {p ( \mathcal {D} \mid h ) p ( h )}{\sum_{h^{\prime} \in \mathcal {H}} p\left ( \mathcal {D}, h^{\prime}\right )}=\frac {p ( h ) \mathbb {I} ( \mathcal {D} \in h ) /|h|^{N}}{\sum_{h^{\prime} \in \mathcal {H}} p\left ( h^{\prime}\right ) \mathbb {I}\left ( \mathcal {D} \in h^{\prime}\right ) /\left|h^{\prime}\right|^{N}} \tag {3.3}
$$

其中 $\mathbb {I}(\mathcal {D} \in h)=1$ 的充要条件是所有数据都在假设 $h$ 的延拓中。图 3.2 绘制了  $\mathcal {D}=\{16\}$ 时，对应的先验、似然和后验。不难发现后验是先验和似然的组合。对于大部分概念，其先验分布都是均匀的，所以后验分布几乎正比于似然。但那些不自然的概念 `2 的幂次方以及 37` 和 `除了 32 的 2 的幂次方` ，尽管有较高的似然，但由于先验很低导致最终后验也很低。相反，`奇数` 和 `偶数` 的先验较高，但由于其似然很低，所以最终的后验较低。

`图 3.3` 绘制了 $\mathcal {D}=\{16，8，2，64\}$ 情况下的先验、似然和后验。此时，似然更加集中在概念 `2 的幂次方` 周围，并最终支配了后验。这样最终可以找出正确的那个概念。此时，可以发现对那些不自然的概念赋予低先验概率的必要性，否则将在现有数据上出现过拟合现象，即选择概念 `除了 32 的 2 的幂次方` 。

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210721184650_a3.webp)

图 3.2 $\mathcal{D}=\{16\}$ 的先验、似然和后验。基于 (Tenenbaum 1999)。

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210721184752_bf.webp)

图 3.3 $\mathcal{D}=\{16，8，2，64\}$ 的先验、似然和后验。基于 (Tenenbaum 1999)。

通常情况下，当拥有足够多数据时，后验 $p (h \mid \mathcal {D})$ 将会集中分布于一个单独的概念，此单独的概念被称为 `最大后验概率 ( maximum a posteriori, MAP ) 估计`：

$$
p ( h \mid \mathcal {D} ) \rightarrow \delta_{\hat {h}^{MAP}} \quad ( h ) \tag {3.4}
$$

其中 $\hat {h}^{MAP}=\operatorname {argmax}_{h} p ( h \mid \mathcal {D} )$ 为后验分布的众数，$\delta$ 为 `狄利克雷测度 ( Dirac measure )` ，定义为：

$$
\begin{align*}
\delta_{x}(A)=\left\{\begin{array}{ll}
1 & \text { if } x \in A \\
0 & \text { if } x \notin A ~~\tag{3.5}
\end{array}\right. 
\end{align*}
$$

值得注意的是，最大后验概率估计 MAP 可以写成：

$$
\hat {h}^{\text {MAP }}=\underset {h}{\operatorname {argmax}} p ( \mathcal {D} \mid h ) p ( h ) =\underset {h}{\operatorname {argmax}}[\log p ( \mathcal {D} \mid h ) +\log p ( h )] \tag {3.6}
$$

因为似然与 $N$ 呈指数幂关系，先验保持不变。当数据量 $N$ 越来越多时，最大后验概率估计将收敛于 `最大似然估计 ( maximum likelihood estimate, MLE )` ：

$$
\hat {h}^{\text {mle }} \triangleq \underset {h}{\operatorname {argmax}} p ( \mathcal {D} \mid h ) =\underset {h}{\operatorname {argmax}} \log p ( \mathcal {D} \mid h ) \tag {3.7}
$$

换言之，如果有足够多数据，会发生 `数据压倒先验 ( data overwhelms the prior )` 的情况，此时 MAP 将收敛于 MLE。

如果真实假设存在于起初的假设空间中，那么 MAP/MLE 将收敛于该真实假设。因此说贝叶斯推断以及最大似然估计是一致性估计 ( `第 6.4.1 节` 将讨论相关细节 ) 。我们也可以称`假设空间在极限情况下是可识别的`，这意味着可以在数据趋于无限时恢复“真理”。如果假设的类别不够充分，不足以表示“真理”，则会聚集在尽可能接近真理的假设上。
### 3.2.4 后验预测性分布

后验分布是我们关于世界的内在 `信念状态( belief state )` ，而检验这种信念是否合理的方法是使用后验去预测客观世界中可观测的量。后验预测性分布定义为：

$$
p ( \tilde {x} \in C \mid \mathcal {D} ) =\sum_{h} p ( y=1 \mid \tilde {x}, h ) p ( h \mid \mathcal {D} ) \tag {3.8}
$$

上式是基于每一个假设所作出的预测的加权平均，称为 `贝叶斯模型平均 ( Bayes model averaging )` 。`图 3.4` 为图示。图中下方的点展示了每个假设的预测结果；图形右边的垂直曲线显示了每个假设的权重值。如果将每一行乘上相应的权重并相加，将得到顶部的分布。

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210610130500b2.webp)

图 3.4 在看到数据 $D = \{16\}$ 的情况下，所有假设 $h$ 的后验分布 $p (h|D)$ ( 图右 ) 和相应的预测分布。图中的点说明该点与相应的假设是一致的。右侧的图表示每个假设 $h$ 的权重。通过对每个点进行加权求和，我们得到上图顶端的 $p (\tilde {x} \in C|\mathcal{D})$ 。

当我们有一个小的或者模糊的数据集时，其后验分布 $p (h|D)$ 也会含糊不清，从而导致概率质量分布较广。然而，一旦 `将问题解决了` ( 即找到了那个概念 ) ，后验分布将退化成脉冲函数，且以 MAP 估计值为中心。此时预测性分布将变成：

$$
p ( \tilde {x} \in C \mid \mathcal {D} ) =\sum_{h} p ( \tilde {x} \mid h ) \delta_{\hat {h}} ( h ) =p ( \tilde {x} \mid \hat {h} ) \tag {3.9}
$$

上式被称为预测概率密度的 `点估计 (plug-in approximation )`，因其简单性而被广泛使用。但这种方式会低估我们的不确定度，与使用贝叶斯模型平均方法相比，预测结果将不会很平滑。


最大后验估计方法简单，但却不能解释从基于相似度的推断 ( 不确定的后验 ) 到基于规则的推断 ( 确定的后验 ) 的逐渐转变。举例来说，假设观测到数据集 $\mathcal{D}=\{16\}$ ，如果基于之前的简单先验，最小一致性假设是 `所有4的幂`，因此只有 4 、16 和 64 获得非零的预测概率，显然这是过拟合的。

随着看到更多数据，比如说给定的数据集变成了 $\mathcal {D}=\{16,8,2,64\}$，最大后验概率估计支持的假设为 `所有 2 的幂次数` 。显然，基于此假设将有更多数值得到的概率值不为 0。也就是说采用点估计的方式，随着看到的数据不断增加，导致后验预测性分布的变化趋势 `由窄变宽`。相反，如果采用贝叶斯方法 ( 即贝叶斯模型平均 ) ，随着看到的数据量增加，后验预测性分布的变化趋势 `由宽变窄`，而这符合人们直观的感受。特别的，当数据集为 $\mathcal{D}=\{16\}$ 时，后验分布中有许多假设的概率值都不为 0，所以相应的预测性分布也比较宽。然而，当看到的数据集变成 $\mathcal {D}=\{16,8,2,64\}$ 时，后验分布的概率质量将主要集中在一个假设上，导致预测性分布变窄。

通过上面一个小的案例，我们发现，点估计与贝叶斯模型平均方法在最终的预测分布上存在很大不同，尽管随着数据量的增加，两者会收敛于同一个结果。

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210610130504c7.webp)

图 3.5 使用了全假设空间的模型预测性分布。对比图 3.1，贝叶斯模型平均得到的预测结果只绘制那些可用的人类数据 $\tilde{x}$；这也是为什么顶行的分布较图 3.4 更稀疏。

### 3.2.5 一种更复杂的先验分布

为了模拟人类的行为，`Tenenbaum` 分析了一些实验数据，这些数据反映了人们是如何衡量数字之间的相似度的，基于这些分析，使用了一个稍微复杂的先验概率分布。在此先验分布中，不仅包含与上文相似的一些数学概念，还包括在 $n$ 和 $m$ 之间的所有区间 ( $1≤ n, m ≤100$ ) 。 ( 值得注意的是，这些假设并不是互斥的 ) 。因此最终的先验分布是两种先验的混合，一种基于数学规则，一种基于区间：

 $$
p ( h ) =\pi_{0} p_{\text {rules }} ( h ) +\left ( 1-\pi_{0}\right ) p_{\text {interval }} ( h ) \tag {3.10}
$$

在上述模型中唯一的自由参数就是相对权重 $\pi_0$ 。只要 $\pi_0 > 0.5$ ( 即更多的人相信最终的假设应该是一个数学规则 ) ，最终的结果对 $\pi_0$ 的大小并不会太敏感。基于此更大的假设空间，模型的预测分布如图 3.5 所示。它与如图 3.1 所示的人类的预测分布惊人的相似，尽管它并不适应人类预测的数据 ( 除了假设空间的选择 ) 。

## 3.3 贝塔 --- 二项式模型

在前面章节中我们介绍了一个数字游戏，在此游戏中，给定了一系列的离散观察值，我们需要根据给定的观测值，在有限的假设空间中推断出一个分布。在此过程中，我们的计算十分简单：我们只需要加法、乘法和除法的一些操作。然而，在很多应用中，未知参数是连续的，也就是说假设空间的大小为 $\mathrm {R}^K$，或者是它的一个子集。其中 $K$ 是参数的数量。这使得数值计算变得复杂，因为我们需要将求和的操作变成积分。然而，其基本的思想是一致的。

为了说明此问题，我们将首先考虑抛硬币的实验，在给定一系列试验结果的前提下，推断出硬币朝上的概率。尽管此看起来很普通，但是结果证明此模型是本书后面将介绍的许多方法的基础，包括朴素贝叶斯分类器、马尔科夫模型等等。同时此试验在历史上也是很重要的，它是贝叶斯在 1763 年的原始论文中所分析的案例。

我们将遵循我们目前所熟悉的方法：`已知先验分布和似然函数，推断出后验分布和后验预测分布`。

### 3.3.1 似然函数

假设  $X_{i} \sim \operatorname {Ber} ( \theta )$( 伯努利分布 ) ，其中 $X_{i}=1$ 代表硬币的正面朝上，$X_{i}=0$ 代表反面朝上。$\theta \in [0,1]$ 为 rate 参数 ( 即正面朝上的概率 ) 。如果试验结果服从独立同分布，则似然函数的形式为：

$$
 p ( \mathcal {D} \mid \theta ) =\theta^{N_{1}} ( 1-\theta )^{N_{0}} \tag {3.11}
$$

其中 $N_{1}=\sum_{i=1}^{N} \mathbb {I}\left (x_{i}=1\right)$ 表示正面朝上的次数，$N_{0}=\sum_{i=1}^{N} \mathbb {I}\left (x_{i}=0\right)$ 表示反面朝上的次数。这两个统计量被称为数据的 `充分统计量` ( sufficient statistics ) ，因为只需要知道这两个统计量就可以从数据 $\mathcal {D}$ 中推断出参数 $\theta$ ( 当然 $N_{1}$ 和 $N=N_{0}+N_{1}$ 也可以作为充分统计量 ) 。

更加正式的表达，我们称如果 $p (\boldsymbol {\theta} \mid \mathcal {D})=p (\boldsymbol {\theta} \mid \mathbf {s}( data ))$ ，则称 $\mathbf {s}(\mathcal {D})$ 为数据 $\mathcal {D}$ 的充分统计量。如果我们采用一个均匀的先验分布，那么上式等价于 $p (\mathcal {D} \mid \boldsymbol {\theta} \propto p (\mathbf {s}(\mathcal {D}) \mid \boldsymbol {\theta})$。因此如果我们拥有两个数据集且具备相同的充分统计量，那么我们将推断得到一样的参数值 $\theta$。

现在假设数据由 $N=N_{1}+N_{0}$ ( $N$ 值大小固定 ) 次试验中观察到的正面朝上的次数 $N_{1}$ 组成。在这种情况下， $N_{1} \sim \operatorname {Bin}(N, \theta)$， 其中 Bin 代表二项式分布，其概率质量函数定义为：

$$
\operatorname {Bin} ( k \mid n, \theta ) \triangleq\left ( \begin {array}{l}
n \\
k
\end {array}\right ) \theta^{k} ( 1-\theta )^{n-k} \tag {3.12}
$$

因为二项式系数 $\left ( \begin {array}{l} n \\ k\end {array}\right )$ 与参数 $\theta$ 无关，所以二项式分布与伯努利分布的似然函数是一样的 ( 差一个系数 ) 。所以无论我们观察到的是统计量 $\mathcal {D}=\left ( N_{1}, N\right )$ ( 对应二项式分布 ) 还是 $N$ 次试验的序列结果 $\mathcal {D}=\left\{x_{1}, \ldots, x_{N}\right\}$ ( 对应伯努利分布 ) ，我们关于参数的推断结果都是一样的。

### 3.3.2 先验分布

我们需要一个先验分布，其定义域为区间 $[0,1]$ 。为了让数学计算更加简单，使先验分布与似然函数的形式保持一致是一件很方便的事情，比如说我们的先验分布具备如下形式：

$$
p ( \theta ) \propto \theta^{\gamma_{1}} ( 1-\theta )^{\gamma_{2}} \tag {3.13}
$$

上式包含一些先验参数和。在这种情况下，我们可以很容易的计算出后验分布：

$$
p ( \theta ) \propto p ( \mathcal {D} \mid \theta ) p ( \theta ) =\theta^{N_{1}} ( 1-\theta )^{N_{0}} \theta^{\gamma_{1}} ( 1-\theta )^{\gamma_{2}}=\theta^{N_{1}+\gamma_{1}} ( 1-\theta )^{N_{0}+\gamma_{2}} \tag {3.14}
$$

如果先验分布与后验分布有相同的形式，我们称先验分布为相应的似然函数的 `共轭先验` ( conjugate prior ) 。共轭先验因其便于计算且容易解释而被广泛使用，我们将在后面看到这一点。

在伯努利分布中，其共轭先验为我们在 2.4.6 节介绍的贝塔分布：

$$
\operatorname {Beta} ( \theta \mid a, b ) \propto \theta^{a-1} ( 1-\theta )^{b-1} \tag {3.15}
$$

先验分布中的参数 $a$ ,$b$ 被称为 `超参数` ( hyper-parameters ) 。通过设置超参数我们可以在模型中融入我们的一些背景知识。比如说，根据经验，我们相信参数的期望值为 0.7，标准差为 0.2，那么我们就可以设置 $a=2.975, b=1.275$ 。或者说我们相信的期望值为 0.15，且位于区间 $( 0.05,0.30 )$ 的概率值为 0.95，则设置 $a=4.5,b=25.5$ 。

如果我们对参数没有一点背景知识，只知道它位于区间 $[0,1]$，那么我们就可以使用一个均匀的先验分布，它是一种不包含任何信息的先验分布 ( 章节 5.4.2 给出更多细节 ) 。在贝塔分布中，我们可以设置 $a=1,b=1$ ，从而实现均匀先验分布。

### 3.3.3 后验分布

似然函数与贝塔先验分布相乘将得到后验分布，我们有：

 $$
  p ( \theta \mid \mathcal {D} ) \propto \operatorname {Bin}\left ( N_{1} \mid \theta, N_{0}+N_{1}\right ) \operatorname {Beta} ( \theta \mid a, b ) \operatorname {Beta}\left ( \theta \mid N_{1}+a, N_{0}+b\right )
 $$

特别的，后验分布是通过将先验分布中的超参数与经验计数 ( `注意此处的经验计数为我们实验过程中的计数`，即 $N$_1 和 $N$_0 ) 相加得到的。出于此原因，我们称超参数为 `伪计数` ( pseudo
counts ) 。先验分布的强度，又被称为 `等价样本尺寸` ( equivalent sample
size ) ，等于伪计数的和，即；这与经验数据的尺寸大小 $N$_1+ $N$_0= $N$ 扮演相同的角色。

![] ( https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013050912.webp )

图 3.6  ( a ) 基于充分统计量 N1=3,N0=17 的二项式分布更新先验分布 Beta ( 2,2 ) ，形成后验分布 Beta ( 5,19 ) 。 ( b ) 基于充分统计量 N1=11,N0=13 的二项式分布更新先验分布 Beta ( 5,2 ) ，形成后验分布 Beta ( 16,15 ) 。

图 3.6 ( a ) 展示了一个例子，我们通过一个尖的似然函数 ( 即有大量的采样样本 ) 更新一个较弱的 Beta ( 2,2 ) 先验分布，不难发现后验分布与似然函数几乎一样，因为采样得到的经验数据 `碾压` 了先验分布。图 3.6 ( b ) 给出了另一个例子，我们通过一个较尖的似然函数更新一个较强的 Beta ( 5,2 ) 先验分布，现在我们发现后验分布处在了先验分布与似然函数之间。

值得注意的是，序列化地更新后验分布等价于在一个单独的批次中进行更新。为了明白这一点，假设我们手上有两个数据集和，且其充分统计量分别为，和。令和为联合数据集的充分统计量。在批次更新模式中，我们有：

$$
p\left ( \theta \mid \mathcal {D}_{a}, \mathcal {D}_{b}\right ) \quad \propto \operatorname {Bin}\left ( N_{1} \mid \theta, N_{1}+N_{0}\right ) \operatorname {Beta} ( \theta \mid a, b ) \propto \operatorname {Beta}\left ( \theta \mid N_{1}+a, N_{0}+b\right )
$$

在序列更新模式中，我们得到的最终结果为：

\begin {align*}
p\left ( \theta \mid \mathcal {D}_{a}, \mathcal {D}_{b}\right ) & \propto p\left ( \mathcal {D}_{b} \mid \theta\right ) p\left ( \theta \mid \mathcal {D}_{a}\right ) \\
& \propto \operatorname {Bin}\left ( N_{1}^{b} \mid \theta, N_{1}^{b}+N_{0}^{b}\right ) \operatorname {Beta}\left ( \theta \mid N_{1}^{a}+a, N_{0}^{a}+b\right ) \\
& \propto \operatorname {Beta}\left ( \theta \mid N_{1}^{a}+N_{1}^{b}+a, N_{0}^{a}+N_{0}^{b}+b\right )
\end {align*}

这使得贝叶斯推断特别适合用在 `在线学习` ( online learning ) 的相关领域，我们将在 8.5.5 节看到更多相关细节。

#### 3.3.3.1 后验分布的期望和众数

根据式 2.56，我们知道最大后验概率估计 ( 即 Beta 分布的众数 ) 为：

$$
\hat {\theta}_{M A P}=\frac {a+N_{1}-1}{a+b+N-2}
$$

如果我们使用均匀先验分布，最大后验估计 ( MAP ) 将退化为最大似然估计 ( MLE ) ，也就是正面朝上的次数的经验占比：

$$
\hat {\theta}_{M L E}=\frac {N_{1}}{N}
$$

上式的结果很直观，当然我们也可以通过最大化似然函数 ( 3.11 ) 得到上式最大似然解 ( `读者可以证明之` ) 。

相比之下，后验分布的期望为：

$$
\bar {\theta}=\frac {a+N_{1}}{a+b+N}
$$

众数和期望之间的差别很重要，这一点将在后文中证明。

现在我们将展示的是后验分布的期望是先验分布期望与最大似然估计 MLE 的凸组合，这就契合了一个概念：在我们开始的信念 ( 先验分布 ) 与实验数据所告诉我们的事实 ( 极大似然 ) 之间，后验分布在这两者之间做出妥协。

令 $\alpha_{0}=a+b$ 为先验分布的等价样本尺寸，它控制着先验分布的强度，令先验分布的期望为 $m_{1}=a / \alpha_{0}$，则后验分布的期望为：

$$ \mathbb {E}[\theta \mid \mathcal {D}]=\frac {\alpha_{0} m_{1}+N_{1}}{N+\alpha_{0}}=\frac {\alpha_{0}}{N+\alpha_{0}} m_{1}+\frac {N}{N+\alpha_{0}} \frac {N_{1}}{N}=\lambda m_{1}+ ( 1-\lambda ) \hat {\theta}_{M L E}
$$

其中为先验分布的等价样本尺寸与后验分布的等价样本尺寸之间的比例。越小，先验分布的强度越弱。所以当，`后验分布的期望值将趋向于最大似然解`。读者也可以自己探索如下结论：后验分布的众数是先验分布的众数与最大似然解之间的凸组合，所以 `最大后验估计同样收敛于最大似然估计`。

#### 3.3.3.2 后验分布的方差

期望和众数都是关于参数的点估计，知道我们在多大程度上信任此估计值是十分有用的。后验分布的方差就是为了衡量这种信任程度的。Beta 后验分布的方差为：

$$
\operatorname {var}[\theta \mid \mathcal {D}]=\frac {\left ( a+N_{1}\right ) \left ( b+N_{0}\right )}{\left ( a+N_{1}+b+N_{0}\right )^{2}\left ( a+N_{1}+b+N_{0}+1\right )}
$$

当 $N \gg a, b$ 时，我们可以对上面公式进行简化：

$$
\operatorname {var}[\theta \mid \mathcal {D}] \approx \frac {N_{1} N_{0}}{N N N}=\frac {\hat {\theta} ( 1-\hat {\theta} )}{N}
$$

所以，估计值 $\hat {\theta}$ 的 `误差条` ( 即后验分布的标准差 ) 为：

$$
\sigma=\sqrt {\operatorname {var}[\theta \mid \mathcal {D}]} \approx \sqrt {\frac {\hat {\theta} ( 1-\hat {\theta} )}{N}}
$$

不难发现不确定度下降的速率为。值得注意的是，当时，不确定度最大，当接近 0 或者 1 时，不确定度最小。`这就意味着` 确定一个硬币是不均匀的 `要比` 确定一个硬币是均匀的 `更加容易。`

### 3.3.4 后验预测分布

截止目前，我们将注意力主要集中在对未知参数的推断上。现在让我们将目光转移到对未来可观察数据的预测上。

考虑在服从 Beta ( a,b ) 的后验分布的情况下，预测下一次抛掷硬币试验中，正面朝上的概率。我们有：

\begin {align*}
p ( \tilde {x}=1 \mid \mathcal {D} ) &=\int_{0}^{1} p ( x=1 \mid \theta ) p ( \theta \mid \mathcal {D} ) d \theta \\
&=\int_{0}^{1} \theta \operatorname {Beta} ( \theta \mid a, b ) d \theta=\mathbb {E}[\theta \mid \mathcal {D}]=\frac {a}{a+b}
\end {align*}

所以不难发现，后验预测分布的期望等价于 ( 在当前情况下 ) 基于后验分布期望的点估计，即：$p ( \tilde {x} \mid \mathcal {D} ) =\operatorname {Ber} ( \tilde {x} \mid \mathbb {E}[\theta \mid \mathcal {D}] )$。

#### 3.3.4.1 过拟合与黑天鹅悖论

假设我们使用最大似然估计进行点估计，即 $p ( \tilde {x} \mid \mathcal {D} ) \approx \operatorname {Ber}\left ( \tilde {x} \mid \hat {\theta}_{M L E}\right )$。不幸的是，这种近似的方式在样本很少的情况下性能十分糟糕。举例来说，我们在连续的 3 次硬币试验中看到 3 次反面朝上，那么根据最大似然估计的原则，我们有。然而，如果使用此参数估计值，我们将认为硬币正面朝上是不可能发生的事情。这被称为 `零计数问题`( zero count problem ) 或者 `稀疏数据问题` ( sparse data problem ) ，这种问题在样本数量很小的情况下经常出现。或许我们会认为在大数据时代，这种情况基本上不会发生，但是一旦我们基于某种准则对数据进行拆分时 ------ 比如某个特定的人已经从事某个特定活动的次数 ------ 样本的尺寸将变得很小。此问题会在诸如在网页上进行个性化推荐的任务中出现。因此贝叶斯方法依然是有用的，哪怕是在大数据时代。

零计数问题与哲学中的 `黑天鹅悖论`( black swan paradox ) 十分相似。这是基于古代西方人的基本观念：天鹅都是白色的。在当时的那种环境下，黑天鹅一般是对那些不可能发生的事的隐喻 ( 黑天鹅于 17 世纪被欧洲探险家在澳大利亚发现 ) 。术语 `黑天鹅悖论` 一词最早由著名的科学哲学家 Karl Popper 创造。此悖论被用于说明在归纳 ( induction ) 中出现的问题，所谓归纳法就是如何基于历史上出现的特定的观测值得到关于未来的一般性结论。

让我们针对上述问题推导出一个简单的贝叶斯解决方案。我们将使用一个均匀先验分布，即 a=b=1，在这种情况下，基于后验分布期望进行点估计，此方法符合 `拉普拉斯继承法则` ( Laplace's rule of succession ) ：

$$
p ( \tilde {x}=1 \mid \mathcal {D} ) =\frac {N_{1}+1}{N_{1}+N_{0}+2}
$$

上述方法向我们展示了一种常规操作：在经验计数上加 1，基于归一化的结果进行点估计，该技术被称为 `加一平滑`( add-one smoothing。 ) ( 值得注意的是加一平滑是针对最大似然解作出的调整，使用最大后验估计参数进行点估计不具备平滑效应，因为最大后验估计参数为 $\hat {\theta}=\frac {N_{1}+a-1}{N+a+b-2}$，如果 $a=b=1$，它将退化成最大似然估计。 )

#### 3.3.4.2 预测未来多次试验的结果

假设我们对未来 $M$ 次试验中正面朝上的次数 $\mathbf {x}$ 感兴趣。定义为：

\begin {align*}
p ( x \mid \mathcal {D}, M ) &=\int_{0}^{1} \operatorname {Bin} ( x \mid \theta, M ) \operatorname {Beta} ( \theta \mid a, b ) d \theta \\
&=\left ( \begin {array}{c}
M \\
x
\end {array}\right ) \frac {1}{B ( a, b )} \int_{0}^{1} \theta^{x} ( 1-\theta )^{M-x} \theta^{a-1} ( 1-\theta )^{b-1} d \theta
\end {align*}

不难发现，上式中的积分项就是分布 $Beta ( a+x, M-x+b )$ 的归一化常数。因此：

$$
\int_{0}^{1} \theta^{x} ( 1-\theta )^{M-x} \theta^{a-1} ( 1-\theta )^{b-1} d \theta=B ( x+a, M-x+b )
$$

所以后验预测分布由下式给出，即所谓的 `贝塔 ------ 二项式` ( beta-binomial ) 分布：

$$
B b ( x \mid a, b, M ) \triangleq\left ( \begin {array}{c}
M \\
x
\end {array}\right ) \frac {B ( x+a, M-x+b )}{B ( a, b )}
$$

该分布的期望和方差为：

$$
\mathbb {E}[x]=M \frac {a}{a+b}, \operatorname {var}[x]=\frac {M a b}{( a+b )^{2}} \frac {( a+b+M )}{a+b+1}
$$

如果 $M =1$，则  $x \in\{0,1\}$ ，不难发现此时期望退化成 $\mathbb {E}[x \mid \mathcal {D}]=p ( x=1 \mid \mathcal {D} ) =\frac {a}{a+b}$ ，与式 3.29 保持一致。

![] ( https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013051521.webp )

图 3.7.
 ( a ) 看到 $N_1=3, N_0=17$ 后的后验预测分布； ( b ) 基于 MAP 的点估计。图形由程序 `betaBinomPostPredDemo` 生成

此过程如图 3.7 ( a ) 所示。我们以 Beta ( 2,2 ) 作为先验分布，当看到 $N_1=3$ 和 $N_0=17$ 时，绘制出后验预测密度。图 3.7 ( b ) 绘制了基于 MAP 估计值作出的预测结果。不难发现，通过贝叶斯方法进行的预测具备较长的尾域，其概率质量分布得更广，因此不太倾向于过拟合和黑天鹅悖论。

`附：关于上述问题的个人解释`

在图 3.7 中我们分别基于贝叶斯模型平均法对未来 10 次试验中 ( 投掷硬币 ) 正面朝上的次数的分布 ( 图 ( a )) 和基于最大后验概率估计进行的点估计。不难发现，当我们看到经验计数 $N_1=3$ 和 $N_0=17$ 时 ( 看到此数据后，我们有理由相信此硬币在未来的 10 次投掷中，正面朝上的次数很多的概率很低 ) ，基于贝叶斯模型平均法进行的预测有较高的尾域概率 ( 即正面朝上的次数较多的概率相较于图 ( b ) 还是比较高的 ) ，而基于 MAP 估计进行的点估计显然很符合我们在看到经验计数后的直观感受，即所谓的过拟合。

## 3.4 狄利克雷 ------ 多项式模型

在上一章节中，我们讨论了如何推断一个硬币正面朝上的概率。本节，我们将泛化这些结果，去推断一个具有 $K$ 个面的骰子面 $k$ 朝上的概率。正如我们在后面将会看到的，此方法被广泛地应用于文本数据，生物序列数据等的分析。

### 3.4.1 似然函数

假设我们抛掷骰子 $N$ 次，观察到的结果为 $\mathcal {D}=\left\{x_{1}, \ldots, x_{N}\right\}$ ，其中 $ x_{i} \in\{1, \ldots, K\} . $。假设所有的数据独立同分布，则似然函数的形式为：

$$
p ( \mathcal {D} \mid \boldsymbol {\theta} ) =\prod_{k=1}^{K} \theta_{k}^{N_{k}}
$$

其中 $N_{k}=\sum_{i=1}^{N} \mathbb {I}\left ( y_{i}=k\right )$ 为面 $k$ 朝上的总次数 ( 这是该模型的充分统计量 ) 。多项式分布的似然函数与上述模型的似然函数形式上是一致的 ( 相差一个与参数无关的常数项 ) 。

### 3.4.2 先验分布

因为参数向量位于一个 $K$ 维空间中，它是一个概率单纯形 ( 详见 2.5.4 ) 。我们需要一个先验分布，其定义域也是一个概率单纯形。理想情况下，我们还希望这是一个共轭先验。幸运的是，狄利克雷分布 ( 详见 2.5.4 ) 满足这两个标准。所以我们使用如下所示的先验分布：

$$
\operatorname {Dir} ( \boldsymbol {\theta} \mid \boldsymbol {\alpha} ) =\frac {1}{B ( \boldsymbol {\alpha} )} \prod_{k=1}^{K} \theta_{k}^{\alpha_{k}-1} \mathbb {I}\left ( \mathbf {x} \in S_{K}\right )
$$

### 3.4.3 后验分布

将先验分布与似然函数相乘，得到的后验分布同样服从狄利克雷分布：

\begin {align*}
p ( \boldsymbol {\theta} \mid \mathcal {D} ) & \propto p ( \mathcal {D} \mid \boldsymbol {\theta} ) p ( \boldsymbol {\theta} ) \\
& \propto \prod_{k=1}^{K} \theta_{k}^{N_{k}} \theta_{k}^{\alpha_{k}-1}=\prod_{k=1}^{K} \theta_{k}^{\alpha_{k}+N_{k}-1} \\
&=\operatorname {Dir}\left ( \boldsymbol {\theta} \mid \alpha_{1}+N_{1}, \ldots, \alpha_{K}+N_{K}\right )
\end {align*}

我们发现后验分布通过将先验分布中的超参数 ( 伪计数 ) 与经验计数 $N_k$ 相加即可得到。

我们可以通过微积分的方式计算出后验分布的众数 ( 即最大后验概率估计 ) ，然而，前提是必须满足约束条件 ( 读者可以思考一下为什么我们不需要显示地表达约束 ) 。我们使用 `拉格朗日乘子法`( Lagrange multiplier ) 来解决此问题。含约束的目标函数，或者 `拉格朗日算符` ( Lagrangian ) 的形式为对数似然函数加上对数先验分布和约束条件：

$$
\ell ( \boldsymbol {\theta}, \lambda ) =\sum_{k} N_{k} \log \theta_{k}+\sum_{k}\left ( \alpha_{k}-1\right ) \log \theta_{k}+\lambda\left ( 1-\sum_{k} \theta_{k}\right )
$$

为了简化符号书写，我们定义 $ N_{k}^{\prime} \triangleq N_{k}+\alpha_{k}-1 $。对上式关于参数 $ \lambda $ 求偏导，得到：

$$
\frac {\partial \ell}{\partial \lambda}=\left ( 1-\sum_{k} \theta_{k}\right ) =0
$$

关于参数 $\theta_{k}$ 求偏导，得到：

\begin {align*}
\frac {\partial \ell}{\partial \theta_{k}} &=\frac {N_{k}^{\prime}}{\theta_{k}}-\lambda=0 \\
N_{k}^{\prime} &=\lambda \theta_{k}
\end {align*}

我们对式 ( 3.44 ) 两边求和，得到：

\begin {align*}
\sum_{k} N_{k}^{\prime} &=\lambda \sum_{k} \theta_{k} \\
N+\alpha_{0}-K &=\lambda
\end {align*}

其中 $\alpha_{0} \triangleq \sum_{k=1}^{K} \alpha_{k}$ 为先验分布的等价样本尺寸。所以，最大后验概率估计为：

$$
\hat {\theta}_{k}=\frac {N_{k}+\alpha_{k}-1}{N+\alpha_{0}-K}
$$

该式与式 2.73 保持一致。如果我们使用一个均匀先验分布，即，我们得到最大似然估计：

 $$
 \theta _ { k } = N _ { k } / N
 $$

这是面 $k$ 朝上次数的经验占比。

### 3.4.4 后验预测分布

对于一个单独的 multinoulli 试验 ( 即一次试验有多次结果 ) ，其后验预测分布为：

\begin {align*}
p ( X=j \mid \mathcal {D} ) &=\int p ( X=j \mid \boldsymbol {\theta} ) p ( \boldsymbol {\theta} \mid \mathcal {D} ) d \boldsymbol {\theta} \\
&=\int p\left ( \boldsymbol {X}=j \mid \theta_{j}\right ) \left [\int p\left ( \boldsymbol {\theta}_{-j}, \theta_{j} \mid \mathcal {D}\right ) d \boldsymbol {\theta}_{-j}\right] d \theta_{j} \\
&=\int \theta_{j} p\left ( \theta_{j} \mid \mathcal {D}\right ) d \theta_{j}=\mathbb {E}\left [\theta_{j} \mid \mathcal {D}\right]=\frac {\alpha_{j}+N_{j}}{\sum_{k}\left ( \alpha_{k}+N_{k}\right )}=\frac {\alpha_{j}+N_{j}}{\alpha_{0}+N}
\end {align*}

其中 $\boldsymbol {\theta}_{-j}$ 表示除了 $\theta_{j}$ 以外的所有分量 $\boldsymbol {\theta}$。

上述表达式避免了我们在 3.3.4.1 节看到的零计数问题。事实上，这种形式的贝叶斯平滑对于 multinomial 情况更加重要，因为当我们将数据分到多个类别 ( 大于 2 ) 中时，数据的稀疏性更容易出现。

#### 3.4.4.1 工作案例：使用词袋法的语言模型

使用狄利克雷 ------ 多项式模型实现贝叶斯平滑的一个应用是 `语言模型`( language modeling ) 。在语言模型中，我们预测在一个序列中下一个可能出现的单词是什么。此处我们将采用一个非常简单的方法，假设第 $i$ 个单词，下一个单词的出现与否与其他所有单词彼此独立，但同时服从分布。这被称为词袋法模型。给定一系列历史单词，我们该如何预测下一个出现的单词是哪一个呢？

举例来说，假设我们观察到了下面的序列 ( 童谣的一部分 ) ：

Mary had a little lamb, little lamb, little lamb,

Mary had a little lamb, its fleece as white as snow

进一步的，假设我们的语料库由如下单词组成：

Mary lamb little big fleece white black snow rain unk

1 2 3 4 5 6 7 8 9 10

其中 `unk` 表示 unknown ( 未知 ) ，代表所有在语料库中没有出现的单词。为了对童谣的每一行进行编码，我们首先去掉所有的标点符号，去掉那些 `停用词` ( stop
words ) ，比如说 `a` ， `as` ， `the` 等。我们也可以进行 `词干提取`( stemming ) ，意味着将单词转换为它最基本的形式，比如将复数单词后的 `s` 去掉，或者将动词后面的 `ing` 去掉 ( 比如 raining 变成 rain ) 。在上面的例子中，没有单词需要进行词干提取。最后，我们将每个单词替换为它们在语料库中的编号：

1 10 3 2 3 2 3 2

1 10 3 2 10 5 6 8

现在我们忽略单词之间的顺序，计算每个单词出现的频次，形成一个关于单词频次的统计直方图：

 编号  1   2   3    4   5    6    7    8   9   10
------ ------ ------ -------- ----- -------- ------- ------- ------ ------ -----
 单词  mary  lamb  little  big  fleece  white  black  snow  rain  unk
 计数  2   4   4    0   1    1    0    1   0   3

利用 $N_j$ 表示单词 $j$ 出现的次数。如果我们使用表示关于参数的先验分布，其后验预测分布为：

$$
p ( \bar X = j | D ) = E [ \theta _ { j } | D ] = \frac { a _ { j } + N _ { j }} { \sum _ { j ' } a _ { j' } + N _ { j' }} = \frac { 1 + N _ { j }} { 10 + 17 }
$$

如果我们设置 $\alpha_{j}=1$，将会得到：

$$
p ( \tilde {X}=j \mid D ) = ( 3 / 27,5 / 27,5 / 27,1 / 27,2 / 27,2 / 27,1 / 27,2 / 27,1 / 27,5 / 27 )
$$

预测模型的众数为 $X=2$( `lamb` ) 和 $X=3$( `little` ) 。值得注意的是单词 `big` ， `black` 和 `rain` 在历史序列中并没有出现，但模型预测的概率值并非等于 0，说明这些单词在未来还是有可能出现的，只是其概率值比较低。我们将会在后面的内容中看到更复杂的语言模型。

## 3.5 朴素贝叶斯分类器

本节，我们将讨论如何对一个具有离散特征值的向量进行分类，其中 $K$ 表示特征可取的数值数量， $D$ 表示特征的数量。我们将使用一个 `生成式模型` 方法。这就要求我们指定类条件概率分布。最简单的方式是假设在给定类别的前提下特征之间是 `条件独立` ( conditionally independent ) 的。这将允许我们将类条件密度写成一维概率密度的乘积：

$$
p ( \mathbf {x} \mid y=c, \boldsymbol {\theta} ) =\prod_{j=1}^{D} p\left ( x_{j} \mid y=c, \boldsymbol {\theta}_{j c}\right )
$$

相应的模型被称为 `朴素贝叶斯分类器` ( naïve Bayes classifier, NBC ) 。

我们称此模型是 `朴素` 的，是因为在实际使用中我们并不要求特征之间是彼此独立的，哪怕是基于类别标签的条件独立。然而，尽管朴素贝叶斯假设并不正确，但这种分类器的工作效果往往很好。一个理由是因为此模型十分简单，其参数数量的数量级只有 $O ( CD )$( $C$ 为类别数量， $D$ 为特征数量 ) ，因此它不太容易过拟合。

类条件概率密度的形式与每个特征的类型有直接关系。我们给出一些不同的概率形式：

-   对于实数域的特征，我们可以使用高斯分布：$p ( \mathbf {x} \mid y=c, \boldsymbol {\theta} ) = \prod_{j=1}^{D} \mathcal {N}\left ( x_{j} \mid \mu_{j c}, \sigma_{j c}^{2}\right )$，其中 $\mu_{j c}$ 表示特征 $j$ 在类别  $c$ 中的期望， $\sigma_{j c}^{2}$ 表示相应的方差。

-   对于二值特征 $x_{j} \in\{0,1\}$ ，我们可以使用伯努利分布：$p ( \mathbf {x} \mid y=  c, \boldsymbol {\theta} ) =\prod_{j=1}^{D} \operatorname {Ber}\left ( x_{j} \mid \mu_{j c}\right )$，其中 $\mu_{j c}$ 表示在类  $c$ 中特征 $j$ 发生的概率。这通常又被称为 `多变量伯努利朴素贝叶斯` ( multivariate Bernoulli naïve Bayes ) 模型。我们将会在下文给出一个应用案例。

-   对于类别特征 $x_{j} \in\{1, \ldots, K\}$，我们可以使用多项式分布：$p ( \mathbf {x} \mid y=c, \boldsymbol {\theta} ) =\prod_{j=1}^{D} \operatorname {Cat}\left ( x_{j} \mid \boldsymbol {\mu}_{j c}\right )$，其中 $\boldsymbol {\mu}_{j c}$ 表示在类 $c$ 中 $x_j$ 可能的 $K$ 个取值的统计直方图 ( 即取不同值的概率 ) 。

显然，我们可以处理不同类型的特征，或者使用不同的概率分布假设。当然，我们也可以很容易将不同类型的概率分布混合起来使用。

### 3.5.1 NBC 模型训练

现在我们讨论如何训练朴素贝叶斯分类器。所谓训练一个模型，往往是意味着求取关于模型中待定参数的 MAP 或者 ML 估计。然而，我们也会讨论如何求解关于参数的整个后验分布。

#### 3.5.1.1 NBC 的 MLE

对于单个数据而言，其概率值为：

$$
p\left ( \mathbf {x}_{i}, y_{i} \mid \boldsymbol {\theta}\right ) =p\left ( y_{i} \mid \boldsymbol {\pi}\right ) \prod_{j} p\left ( x_{i j} \mid \boldsymbol {\theta}_{j}\right ) =\prod_{c} \pi_{c}^{\mathrm {I}\left ( y_{i}=c\right )} \prod_{j} \prod_{c} p\left ( x_{i j} \mid \boldsymbol {\theta}_{j c}\right )^{\mathrm {I}\left ( y_{i}=c\right )}
$$

对于 $N$ 个独立同分布数据构成的数据集，我们有：

对上式取对数，得到 ( `附`：`关于式 3.56 的最后一步推导，读者可以尝试通过先展开再合并的方式完成。` ) ：

$$
\log p ( \mathcal {D} \mid \boldsymbol {\theta} ) =\sum_{c=1}^{C} N_{c} \log \pi_{c}+\sum_{j=1}^{D} \sum_{c=1}^{C} \sum_{i: y_{i}=c} \log p\left ( x_{i j} \mid \boldsymbol {\theta}_{j c}\right )
$$

不难发现对数似然函数分解成多个项，其中一项包含参数 $\pi$，另外包含 $DC$ 个包含参数 $\boldsymbol {\theta}_{j c}$ 的项。因此我们可以对不同的参数进行单独的优化。

根据式 3.48，不难发现类的先验概率的 MLE 为：

$$
\hat {\pi}_{c}=\frac {N_{c}}{N}
$$

其中 $N_{c} \triangleq \sum_{i} \mathbb {I}\left ( y_{i}=c\right )$ 为所有样本中属于类 $c$ 的数量。

参数的 MLE 与特征服从什么样的类条件概率分布有关。为了简单起见，不妨假设所有特征都是二值特征，即 $x_{j} \mid y=c \sim \operatorname {Ber}\left ( \theta_{j c}\right )$ 。在这种情况下，MLE 为：

$$
\hat {\theta}_{j c}=\frac {N_{j c}}{N_{c}}
$$

其中为在类 $c$ 中特征 $j$ 出现的样本的数量。

![] ( https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061014430310.webp )

实现上述模型的训练十分简单：算法 3.1 给出了程序的伪代码。此算法的时间复杂度为 $O ( ND )$ 。这种方法也可以泛化到那些具有混合类型的特征上。这种简单性也是该方法被广泛使用的理由之一。

![] ( https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013052531.webp )

图 3.8 类条件密度，两个类别分别对应 `X windows` 和 `MS windows` 。图形由程序 `naiveBayesBowDemo` 生成。

图 3.8 给出了一个例子，其中数据中拥有 2 个类别，600 个二值特征，分别表示在词袋法模型中某个单词是否出现。图形对在两个类别的情况下向量进行了可视化展示。在索引值为 107 的地方出现的大的峰值对应单词 `subject` ，它在两个类别中出现的概率都为 1。 ( 我们将在 3.5.4 节讨论如何过滤掉这些不提供有价值信息的特征。 ) ( 注：所谓没有价值就是指此单词在两个类中是肯定出现的，那么它对我们的分类任务就起不到作用了 )

#### 3.5.1.2 贝叶斯方法下的朴素贝叶斯

最大似然解的麻烦之处在于它容易过拟合。举例来说，考虑图 3.8 中所展示的例子：对应单词 `subject` 的特征 ( 称其为特征 $j$ ) 在两个类中都出现了，所以我们估计 $ \hat {\theta}_{j c}=1 $。如果我们遇到一封新的邮件但其中并没有此单词，那么将会发生什么情况呢？我们的算法将会失效，因为我们将发现对于任何一个类 $ p ( y=c \mid \mathbf {x}, \hat {\boldsymbol {\theta}} ) =0 $，即新的邮件不属于任何一个类。这是我们在章节 3.3.4.1 中所提及的黑天鹅悖论的另一种体现。

`附：为了方便读者直观的了解上述内容，我们给出了如下的案例：`

 `label`  `Word_1`  `Word_2`  `...`  `Subject ( $j$)`  `Word_n-1`  `Word_n`
----------- ------------ ------------ --------- -------------------- -------------- ------------
 C_1    1      0      ...    1          1       1
       0      1      ...    1          0       0
 C~2~    0      0      ...    1          0       1
       1      1      ...    1          0       0
       0      1      ...    1          1       0

表格中为模型的训练样本，共有两类数据 C_1 和 C~2~，每一类文本中单词 `subject` 始终存在，那么根据 3.5.1.1 介绍的模型训练方法，我们有，那么当我们遇到一个新的文本 `x`，其中没有单词 `subject` 时，我们将遇到如下情况：

同样我们也可以得到。

针对过拟合的一个简单解决方案是使用贝叶斯方法。为简单起见，我们使用一个因子分解形式的先验分布：

$$
p ( \boldsymbol {\theta} ) =p ( \boldsymbol {\pi} ) \prod_{j=1}^{D} \prod_{c=1}^{C} p\left ( \theta_{j c}\right )
$$

针对参数 $\boldsymbol {\pi}$ 我们使用先验分布 $\operatorname {Dir} ( \boldsymbol {\alpha} )$ ，针对每个参数 $\theta_{j c}$，我们使用先验分布 $\operatorname {Beta}\left ( \beta_{0}, \beta_{1}\right )$ 。一般情况下，我们取 $\alpha=1$ and $\boldsymbol {\beta}=1$，此时对应于前文所提及的加一平滑或者拉普拉斯平滑。

将式 3.56 的似然函数与式 3.59 的先验分布组合，得到因子分解形式的后验分布：

\begin {align*}
p ( \boldsymbol {\theta} \mid \mathcal {D} ) &=p ( \boldsymbol {\pi} \mid \mathcal {D} ) \prod_{j=1}^{D} \prod_{c=}^{C} p\left ( \theta_{j c} \mid \mathcal {D}\right ) \\
p ( \boldsymbol {\pi} \mid \mathcal {D} ) &=\operatorname {Dir}\left ( N_{1}+\alpha_{1} \ldots, N_{C}+\alpha_{C}\right ) \\
p\left ( \theta_{j c} \mid \mathcal {D}\right ) &=\operatorname {Beta}\left ( \left ( N_{c}-N_{j c}\right ) +\beta_{0}, N_{j c}+\beta_{1}\right )
\end {align*}

换句话说，为了计算后验分布，我们只需要利用似然函数中的经验计数去更新先验计数。基于算法 3.1 进行必要的调整以使用当前的方法也是很直接的。

### 3.5.2 使用模型进行预测

在测试阶段，我们的目的是计算

$$
p ( y=c \mid \mathbf {x}, \mathcal {D} ) \quad \propto \quad p ( y=c \mid \mathcal {D} ) \prod_{j=1}^{D} p\left ( x_{j} \mid y=c, \mathcal {D}\right )
$$

在贝叶斯模型平均的方法中，我们需要对所有未知参数进行积分：

\begin {align*}
p ( y=c \mid \mathbf {x}, \mathcal {D} ) \propto & \propto\left [\int \operatorname {Cat} ( y=c \mid \boldsymbol {\pi} ) p ( \boldsymbol {\pi} \mid \mathcal {D} ) d \boldsymbol {\pi}\right] \\
& \prod_{j=1}^{D}\left [\int \operatorname {Ber}\left ( x_{j} \mid y=c, \theta_{j c}\right ) p\left ( \boldsymbol {\theta}_{j c} \mid \mathcal {D}\right ) \right]
\end {align*}

幸运的是，计算上式是比较简单的，尤其是当后验分布服从狄利克雷分布。根据式 3.51，我们知道后验预测分布的加权平均可以简单的基于后验分布的期望值进行点估计。所以：

\begin {align*}
p ( y=c \mid \mathbf {x}, \mathcal {D} ) & \propto \bar {\pi}_{c} \prod_{j=1}^{D}\left ( \bar {\theta}_{j c}\right )^{\mathbb {I}\left ( x_{j}=1\right )}\left ( 1-\bar {\theta}_{j c}\right )^{\mathbb {I}\left ( x_{j}=0\right )} \\
\bar {\theta}_{j k} &=\frac {N_{j c}+\beta_{1}}{N_{c}+\beta_{0}+\beta_{1}} \\
\bar {\pi}_{c} &=\frac {N_{c}+\alpha_{c}}{N+\alpha_{0}}
\end {align*}

其中 $\alpha_{0}=\sum_{c} \alpha_{c}$。

如果我们基于一个单独的点作为对后验分布的估计，即 $p ( \boldsymbol {\theta} \mid \mathcal {D} ) \approx \delta_{\hat {\boldsymbol {\theta}}} ( \boldsymbol {\theta} )$，其中 $\hat {\boldsymbol {\theta}}$ 可能是 ML 或者 MAP 估计值，则后验预测分布就可以直接基于这些参数进行计算，从而形成一个几乎相同的准则：

$$
p ( y=c \mid \mathbf {x}, \mathcal {D} ) \propto \hat {\pi}_{c} \prod_{j=1}^{D}\left ( \hat {\theta}_{j c}\right )^{\mathbb {I}\left ( x_{j}=1\right )}\left ( 1-\hat {\theta}_{j c}\right )^{\mathbb {I}\left ( x_{j}=0\right )}
$$

区别在于我们只是将后验分布的期望 $\bar \theta$ 替换成后验分布的众数 $\hat \theta$ ( 即 MAP ) 或者 MLE
。然而，这一点小小的区别在实际过程中却十分重要，因为利用后验分布的期望进行预测所导致的过拟合更小 ( 见 3.3.4.1 ) 。

### 3.5.3 log-sum-exp 技巧

现在我们讨论一个重要的实践细节，此细节在任何一种生成式分类器中都会遇到。我们可以使用式 2.11 计算样本所属类别的后验分布，前提是使用合适的类条件密度 ( 比如点估计 ) 。不幸的是，公式 2.11 在实现过程中可能会因为 `数值下溢` ( numerical
underflow ) 而失败。问题在于通常情况下是非常小的数，尤其当 $\mathrm {x}$ 是一个高维向量时，因为我们要求 $\sum_{\mathbf {x}} p ( \mathbf {x} \mid y ) =1$，所以我们看到任何一个特定的高维向量的概率 $p ( \mathbf {x} \mid y=c )$ 是很小的。该问题的解决方案是当我们使用贝叶斯公式时对上式取对数：

\begin {align*}
\log p ( y=c \mid \mathbf {x} ) =b_{c}-\log \left [\sum_{c^{\prime}=1}^{C} e^{b_{c^{\prime}}}\right] \\
b_{c} \triangleq \log p ( \mathbf {x} \mid y=c ) +\log p ( y=c )
\end {align*}

然而，上式要求我们计算下式：

$$
\log \left [\sum_{c^{\prime}} e^{b_{c^{\prime}}}\right]=\log \sum_{c^{\prime}} p\left ( y=c^{\prime}, \mathbf {x}\right ) =\log p ( \mathbf {x} )
$$

可是我们并不能对对数进行求和 ( 即 ) 。幸运的是，我们可以将真数 ( $logN$ 中 $N$ 为真数 ) 中的最大项提取出来。比如说：

$$
\log \left ( e^{-120}+e^{-121}\right ) =\log \left ( e^{-120}\left ( e^{0}+e^{-1}\right ) \right ) =\log \left ( e^{0}+e^{-1}\right ) -120
$$

一般情况下，我们有：

$$
\log \sum_{c} e^{b_{c}}=\log \left [\left ( \sum_{c} e^{b_{c}-B}\right ) e^{B}\right]=\left [\log \left ( \sum_{c} e^{b_{c}-B}\right ) \right]+B
$$

其中 $B=\max _{c} b_{c}$ 。这被称为 log-sum-exp 技巧，被广泛使用。

该技巧在算法 3.2 中被使用，该算法给出了利用 NBC 计算 $p\left ( y_{i} \mid \mathbf {x}_{i}, \hat {\boldsymbol {\theta}}\right )$ 的伪代码。值得注意的是，如果我们只是为了计算 $\hat {y}_{i}$，我们并不需要使用这种技巧。因为我们只需要最大化非归一化项 $\log p\left ( y_{i}=c\right ) +\log p\left ( \mathbf {x}_{i} \mid y=c\right )$。

![] ( https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202106101458437e.webp )

### 3.5.4 使用互信息进行特征选择

考虑到 NBC 模型对潜在的很多特征的联合概率进行建模，它可能遇到过拟合的问题。除此以外，它的时间复杂度为 $O ( ND )$ ，对于某些应用来说可能是比较高的。

一种解决上述两个问题的常见方法是进行 `特征选择`( feature selection ) ，将那些对分类问题帮助并不大的无关特征去除。特征选择的最简单方法是对每个特征的相关性进行单独的评估，然后选择前 $K$ 个相关度最高的特征，其中 $K$ 的选择是基于对精度和复杂度的一种权衡。这种方式被称为变量 `排名`( ranking ) ，`过滤` ( filtering ) 或者 `筛选` ( screening ) 。

一种衡量相关性的方式是使用特征 $X_j$ 和类标签 $Y$ 之间的互信息：

$$
I ( X, Y ) =\sum_{x_{j}} \sum_{y} p\left ( x_{j}, y\right ) \log \frac {p\left ( x_{j}, y\right )}{p\left ( x_{j}\right ) p ( y )}
$$

互信息可以看作是由于观察到了特征 $j$ 而导致的标签分布的熵的减少。如果特征是二值的，很容易得到互信息的形式为 ( `读者可以证明下式` ) ：

$$
I_{j}=\sum_{c}\left [\theta_{j c} \pi_{c} \log \frac {\theta_{j c}}{\theta_{j}}+\left ( 1-\theta_{j c}\right ) \pi_{c} \log \frac {1-\theta_{j c}}{1-\theta_{j}}\right]
$$

其中 $\pi_{c}=p ( y=c ) , \theta_{j c}=p\left ( x_{j}=1 \mid y=c\right )$ 且 $\theta_{j}=p\left ( x_{j}=1\right ) =\sum_{c} \pi_{c} \theta_{j c}$，所有这些量都可以在我们训练 NBC 模型过程中顺便得到。

表 3.1 展示了将上述方法应用在图 3.8 所示的二值词袋法数据集的结果，不难发现，拥有最高互信息的特征与那些发生概率最高的特征相比，更具备可判别性。比方说，出现频率最高的单词 `subject` 在两个类别中都有出现，它之所以总是出现是因为此数据集是一个新闻类的数据，而每个新闻都有一个主题 ( `subject` ) 。但显然此单词不具备可判别性。与类别之间拥有最高互信息的单词分别是 `windows` ， `microsoft` ， `dos` 和 `motif` ，这是在情理之中的，因为实际数据中的两个类别分别对应 Microsoft
Windows 和 X Windows。

![] ( https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061015012730.webp )

表 3.1 在类 1 ( X windows ) 和类 2 ( MS windows ) 中最有可能出现的 5 个单词。以及与类标签的互信息最高的 5 个单词。表格由程序 `naiveBayesBowDemo` 生成。

### 3.5.5 使用词袋法对文本进行分类 ( `注：本节更多技术细节请读者参考其他文献` )

`文本分类`( Document classification ) 是指将文本分类到不同的类别中。一种简单的方式是将每个文本表示为二值向量，向量中的分量记录着每个单词在文本中是否出现，所以 $x_{ij} =1$ 的充要条件是单词 $j$ 在文本 $i$ 中出现，否则 $x_{ij} =0$。然后我们可以使用下面的类条件密度：

$$
p\left ( \mathbf {x}_{i} \mid y_{i}=c, \boldsymbol {\theta}\right ) =\prod_{j=1}^{D} \operatorname {Ber}\left ( x_{i j} \mid \theta_{j c}\right ) =\prod_{j=1}^{D} \theta_{j c}^{\mathbb {I}\left ( x_{i j}\right )}\left ( 1-\theta_{j c}\right )^{\mathrm {I}\left ( 1-x_{i j}\right )}
$$

上式被称为 `伯努利乘积模型`( Bernoulli product model ) 或者叫 `二值独立模型` ( binary independence model ) 。

然而，上述模型忽略了每个单词在文本中出现的次数。一个更加精确的方式是将每个单词在文本中出现的次数进行考虑。特别的，令 $\mathbf {x}_{i}$ 为文本 $i$ 的表示向量，其分量代表每个单词在文本中出现的次数，所以 $x_{i j} \in\left\{0,1, \ldots, N_{i}\right\}$，其中 $N_i$ 表示文本 $i$ 中的单词数量 ( 所以 $\sum_{j=1}^{D} x_{i j}=N_{i}$ ) 。对于类条件密度，我们可以使用多项式分布：

$$
p\left ( \mathbf {x}_{i} \mid y_{i}=c, \boldsymbol {\theta}\right ) =\operatorname {Mu}\left ( \mathbf {x}_{i} \mid N_{i}, \boldsymbol {\theta}_{c}\right ) =\frac {N_{i} !}{\prod_{j=1}^{D} x_{i j} !} \prod_{j=1}^{D} \theta_{j c}^{x_{i j}}
$$

上式中我们含蓄地表达了文本 $i$ 的长度 $N_i$ 与类别无关。其中 $\theta_{j c}$ 表示在类 $c$ 文档中单词 $j$ 出现的概率；这些参数满足约束 $\sum_{j=1}^{D} \theta_{j c}=1$ 。

尽管多项式分类器很容易训练并且在测试时也很简单，但对于文本分类问题效果并不是很好。原因之一在于它并没有考虑单词使用过程中的 `突发特性` ( burstiness ) 。所谓突发特性是指：大部分单词在任何给定的文本 ( 比如训练样本 ) 中从未出现，但是一旦它们在新的文本中出现过一次，它们很有可能会出现多次。

多项式模型不能适应上述的突发性现象。为了说明原因，注意式 3.78 具备 ( 此处的 $N_{ij}$ 相当于式中的 $x_{ij}$ ) 的形式，因为对于那些罕见单词 ( 如前文所述，在训练样本中这些单词很少出现，导致 ) ，如果基于此参数估计值，那么我们将会相信在类 $C$ 中不可能出现很多罕见单词 ( 这与前文中的突发特性相悖 ) 。对于那些出现频次高的单词 ( 较大 ) ，这种衰减的速率不会太快。为了直观上明白这里面的原因，注意那些出现频次特别高的单词往往是那些功能单词，比如 `and` ， `the` ，和 `but` 等等，这些单词与文本的类别没有关系。单词 `and` 出现的概率基本上保持不变，无论它之前出现的概率如何，所以独立性假设对于这些常用单词来说更加合理。然而，因为那些罕见单词对于我们的分类目的往往影响更大，所以需要我们更加小心的对待。

各种特别的启发式方法已经应用在多项式文本分类器中以提高性能。接下来，我们介绍一种类条件概率密度，它的性能与那些特别的方式相近，然而更具备概率性解释。

假设我们简单地将多项式类条件概率密度替换为 `狄利克雷复合多项式` ( Dirichlet Compound Multinomial, DCM ) 密度，定义如下：

$$
p\left ( \mathbf {x}_{i} \mid y_{i}=c, \boldsymbol {\alpha}\right ) =\int \operatorname {Mu}\left ( \mathbf {x}_{i} \mid N_{i}, \boldsymbol {\theta}_{c}\right ) \operatorname {Dir}\left ( \boldsymbol {\theta}_{c} \mid \boldsymbol {\alpha}_{c}\right ) d \boldsymbol {\theta}_{c}=\frac {N_{i} !}{\prod_{j=1}^{D} x_{i j} !} \frac {B\left ( \mathbf {x}_{i}+\boldsymbol {\alpha}_{c}\right )}{B\left ( \boldsymbol {\alpha}_{c}\right )}
$$

 ( 上式由式 5.24 导出。 ) 令人惊讶的是此简单的变化就可以使模型适应突发特性现象。直观的解释是：当我们发现单词 $j$ 出现过一次时，的后验计数将被更新，使得单词 $j$ 再次出现的可能性增加。相反，如果固定，每个单词的出现是独立的。多项式模型对应于从一个容器中拿出一个球，记录下它的颜色后再放回。相反，DCM 模型对应于拿出一个球，记录下颜色，放回的同时再复制一个相同的球，这被称为 `波利亚坛子模型 ( Polya's urn scheme )` 是一个著名的概率模型：假设坛子中装有 $N$ 个球，其中有 $N_1$ 个黑球、 $N_2$ 个白球。任意取出一个，记下其颜色，并且在下次取球之前把该球连同另外 $r$ 个与它同色的球一起放人坛中，再从坛中取出一个球，如此以往 。

使用 DCM 作为类条件密度得到的性能比多项式好很多，并且与那些先进的方法相比，其性能并不逊色。DCM 模型的唯一缺点是其模型的训练过程更加复杂。
