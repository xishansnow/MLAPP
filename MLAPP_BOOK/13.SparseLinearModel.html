
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>13 稀疏线性模型 &#8212; 机器学习：概率视角</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14 核方法" href="14.KernelMethod.html" />
    <link rel="prev" title="12 隐变量线性模型" href="12.LinearModelwithLatentVariable.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">机器学习：概率视角</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning ：A Probabilistic Perspective
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../preface.html">
   前言
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01.Introduction.html">
   01 引言
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02.Probability.html">
   02 概率论
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03.GenerativeModelsForDiscreteData.html">
   03 离散数据的生成式模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04.GaussianModels.html">
   04 高斯模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05.BayesianStatistics.html">
   05 贝叶斯学派统计思想
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06.FrequentistStatistics.html">
   06 频率学派统计思想
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07.LinearRegression.html">
   07 线性回归
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08.LogisticRegression.html">
   08 逻辑回归
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09.GeneralizedLinearModelsAndTheExponentialFamily.html">
   09 广义线性模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10.DirectedGraphicalModels%20%28BayesNets%29.html">
   10 有向图模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11.MixtureModelandEMAlgorithm.html">
   11 混合模型与 EM 算法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12.LinearModelwithLatentVariable.html">
   12 隐变量线性模型
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   13 稀疏线性模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14.KernelMethod.html">
   14 核方法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15.GaussianProcesses.html">
   15 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16.AdaptiveBasisFunctionModel.html">
   16 自适应基函数模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="17.MarkovAndHiddenMarkovModel.html">
   17 马尔科夫与隐马尔科夫模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="18.StateSpaceModels.html">
   18 状态空间模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="19.UndirectedGraphicalModels%28MarkovRandomFields%29.html">
   19 无向图模型（马尔科夫随机场）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20.ExactInferenceForGraphicalModels.html">
   20 概率图的精确推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="21.VariationalInference.html">
   21 变分推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="22.MoreVariationalInference.html">
   22 更多变分推断方法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="23.MonteCarloInference.html">
   23 蒙特卡洛推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="24.MarkovChainMonteCarlo%28MCMC%29Inference.html">
   24 马尔科夫链蒙特卡洛 MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="25.Clustering.html">
   25 聚簇
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="26.GraphicalModelStructureLearning.html">
   26 概率图的结构学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="27.LatentVariableModelsForDiscreteData.html">
   27 离散数据的隐变量模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="28.DeepLearning.html">
   28 深度学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Abbreviations.html">
   本书常见缩写列表
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/MLAPP_BOOK/13.SparseLinearModel.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   13.1 引言
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   13.2 贝叶斯观点下的变量选择
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spike-and-slab-model">
     13.2.1 spike and slab model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso">
     13.3.2 lasso最优解条件
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-ridge">
     13.3.3 最小二乘，lasso,ridge和子集选择的比较
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>13 稀疏线性模型</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   13.1 引言
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   13.2 贝叶斯观点下的变量选择
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spike-and-slab-model">
     13.2.1 spike and slab model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso">
     13.3.2 lasso最优解条件
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso-ridge">
     13.3.3 最小二乘，lasso,ridge和子集选择的比较
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>13 稀疏线性模型<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<div class="section" id="id2">
<h2>13.1 引言<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>我们在3.5.4节中介绍了关于特征选择的主题，讨论了寻找与输出具有高互信息的输入变量的方法。这种方法的问题在于，它基于一种短视的策略，即一次只看一个变量。如果输入变量之间存在交互作用，则此操作可能会失败。例如，如果<span class="math notranslate nohighlight">\(y = {\rm{xor}}(x_1, x_2)\)</span>,那么<span class="math notranslate nohighlight">\(x_1\)</span>和<span class="math notranslate nohighlight">\(x_2\)</span>都不能单独预测响应，但是它们一起工作就可以进行完美地预测。举一个真实的例子，考虑一下基因关联研究:有时两个基因本身可能是无害的，但当它们一起出现时，就会导致隐性疾病。</p>
<p>在本章中，我们将使用基于模型的方法一次选择一组变量。如果模型是广义线性模型，对于某些形式为<span class="math notranslate nohighlight">\(p(y|\mathbf{x})=p(y|f(\mathbf{w^Tx}))\)</span>的连接函数<span class="math notranslate nohighlight">\(f\)</span>，则可以通过使权重向量<span class="math notranslate nohighlight">\(\mathbf{w}​\)</span>稀疏来进行特征选择，即，向量中很多元素为0。这种方法提供了重要的计算优势，这一点我们将在下文展示。</p>
<p>以下是一些特征选择/稀疏化有用的应用：</p>
<ul class="simple">
<li><p>在许多问题中，我们有比训练样本数量<span class="math notranslate nohighlight">\(N\)</span>更多的特征维度<span class="math notranslate nohighlight">\(D\)</span>。在这种情况下，设计矩阵会显得又矮又胖，而不是又高又苗条。这被称为<strong>小</strong><span class="math notranslate nohighlight">\(N\)</span><strong>大</strong><span class="math notranslate nohighlight">\(D\)</span>问题。当我们开发更多的高吞吐量测量设备时，这种情况变得越来越普遍，例如，使用基因微阵列，通常测量<span class="math notranslate nohighlight">\(D∼10,000\)</span>个基因的表达水平，但是只得到<span class="math notranslate nohighlight">\(N∼100\)</span>个这样的样例。(这也许是一个时代的迹象，甚至我们的数据似乎也在变胖…)。我们可能想要找到能够准确预测响应的最小特征集(例如，细胞的生长速度)，以防止过度拟合，降低制造诊断设备的成本，或帮助科学地洞察问题。</p></li>
<li><p>在第14章，我们将使用以训练样本为中心的基函数，即<span class="math notranslate nohighlight">\(\phi(\mathbf{x})=[\mathcal{k}(\mathbf{x},\mathbf{x}_1),...,\mathcal{k}(\mathbf{x},\mathbf{x}_N)]\)</span>，其中<span class="math notranslate nohighlight">\(\mathcal{k}\)</span>为一个核函数。通过这种方式得到的设计矩阵大小为<span class="math notranslate nohighlight">\(N\times N\)</span>。在这种情况下的特征选择等价于选择训练样本的一个子集，从而缓解过拟合和减少计算成本。这被称为稀疏核机器(sparse kernel machine)。</p></li>
<li><p>在信号处理中，通常会使用小波基函数来表示信号(图像，声音等等)。为了节约时间和空间，寻找关于信号的稀疏表示是十分有用的，也就是使用少量的基函数。这将允许我们使用少量的测量装置来实现对信号的估计，也就是对信号进行了压缩。13.8.3将介绍更多的信息。</p></li>
</ul>
<p>需要注意的是，特征选择和稀疏化是目前机器学习/统计领域最活跃的领域之一。本章，我们将只给出一些主要结论的概述。</p>
</div>
<div class="section" id="id3">
<h2>13.2 贝叶斯观点下的变量选择<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>接下来，我们将介绍解决变量选择问题的一个很自然的方法。如果特征<span class="math notranslate nohighlight">\(j\)</span>被认为是“相关的”，那么令<span class="math notranslate nohighlight">\(\gamma_j=1\)</span>，否则令<span class="math notranslate nohighlight">\(\gamma_j=0\)</span>。我们的目标是计算模型的后验分布
$<span class="math notranslate nohighlight">\(
p(\mathbf{\gamma}|\mathcal{D})=\frac{e^{-f(\gamma)}}{\sum_{\gamma^\prime}e^{-f(\gamma^\prime)}} \tag{13.1}
\)</span><span class="math notranslate nohighlight">\(
其中\)</span>f(\gamma)<span class="math notranslate nohighlight">\(是成本函数：
\)</span><span class="math notranslate nohighlight">\(
f(\gamma)\triangleq -[\log p(\mathcal{D|\gamma})+\log p(\gamma)] \tag{13.2}
\)</span><span class="math notranslate nohighlight">\(
举例来说，假设我们从\)</span>D=10<span class="math notranslate nohighlight">\(的线性回归模型\)</span>y_i \sim \mathcal{N}(\mathbf{w}^T\mathbf{x}_i,\sigma^2)<span class="math notranslate nohighlight">\(中产生了\)</span>N=20<span class="math notranslate nohighlight">\(个样本，其中权重向量\)</span>\mathbf{w}<span class="math notranslate nohighlight">\(中的\)</span>K=5<span class="math notranslate nohighlight">\(个元素为非零元素。特别地，我们使用\)</span>\mathbf{w}=(0.00,-1.67,0.13,0.00,0.00,1.19,0.00,-0.04,0.33,0.00)<span class="math notranslate nohighlight">\(以及\)</span>\sigma^2=1<span class="math notranslate nohighlight">\(。我们可以枚举出所有的\)</span>2^{10}=1024<span class="math notranslate nohighlight">\(个模型，并且对每个模型计算\)</span>p(\gamma|\mathcal{D})$。（我们将在后文介绍如何计算）。我们使用**格雷码(Gray code)**的顺序对所有模型进行排序，从而保证连续的模型之间只相差一个比特位（这么做的理由是考虑计算上的优势，我们将在13.2.3节讨论这一点）。</p>
<p>图13.1(a)展示了最终的比特模式图。每个模型的成本<span class="math notranslate nohighlight">\(f(\gamma)\)</span>如图13.1(b)所示。我们发现目标函数极度地“不平衡”。如果我们计算出关于模型的后验分布<span class="math notranslate nohighlight">\(p(\gamma|\mathcal{D})\)</span>，那么这种情况就可以很容易地解释了。后验分布如图13.1(c)所示。排名前8的模型如下表所示：</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>model</p></th>
<th class="head"><p>prob</p></th>
<th class="head"><p>members</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>4</p></td>
<td><p>0.447</p></td>
<td><p>2,</p></td>
</tr>
<tr class="row-odd"><td><p>61</p></td>
<td><p>0.241</p></td>
<td><p>2,6,</p></td>
</tr>
<tr class="row-even"><td><p>452</p></td>
<td><p>0.103</p></td>
<td><p>2,6,9,</p></td>
</tr>
<tr class="row-odd"><td><p>60</p></td>
<td><p>0.091</p></td>
<td><p>2,3,6,</p></td>
</tr>
<tr class="row-even"><td><p>29</p></td>
<td><p>0.041</p></td>
<td><p>2,5,</p></td>
</tr>
<tr class="row-odd"><td><p>68</p></td>
<td><p>0.021</p></td>
<td><p>2,6,7,</p></td>
</tr>
<tr class="row-even"><td><p>36</p></td>
<td><p>0.015</p></td>
<td><p>2,5,6,</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>0.010</p></td>
<td><p>2,3,</p></td>
</tr>
</tbody>
</table>
<p>“真正”的模型应该是<span class="math notranslate nohighlight">\(\{2,3,6,8,9\}\)</span>。然而，与特征<span class="math notranslate nohighlight">\(3\)</span>和特征<span class="math notranslate nohighlight">\(8\)</span>关联的系数却十分的小（相对于<span class="math notranslate nohighlight">\(\sigma ^ 2\)</span>），所以这些变量很难被检测到。给定足够的数据，这种方法将收敛于真正的模型（假设数据产生于同一个线性模型），但对于有限的数据集而言，后验分布通常存在非常大的不确定性。</p>
<p>当后验分布所基于的模型数量十分大时，解释起来会十分困难，所以我们寻找各种具有总结意义的统计量。一个自然的选择是后验分布的峰值，即MAP估计：
$<span class="math notranslate nohighlight">\(
\hat{\mathbf{\gamma}}=\arg \max p(\gamma|\mathcal{D})=\arg \min f(\gamma) \tag{13.3} 
\)</span><span class="math notranslate nohighlight">\(
然而，对于整个后验分布的概率质量而言，峰值通常不具备代表性（见5.2.1.3节）。一种更好的总结方法是**中值模型(median model)**，即：
\)</span><span class="math notranslate nohighlight">\(
\hat{\mathbf{\gamma}}=\{j:p(\gamma_j=1|\mathcal{D})&gt;0.5\} \tag{13.4}
\)</span><span class="math notranslate nohighlight">\(
这要求我们计算后验边缘**包含概率(inclusion probabilities)** \)</span>p(\gamma_j=1|\mathcal{D})​$。这些结果在图13.1(d)中进行展示。我们发现如果变量2和6被包含，那么模型会很有信心；如果我们将决策阈值降低到0.1，模型将会同时考虑变量3和9.然而，如果我们还想“获取”变量8，那么于此同时会将2个错误的正例(5和7)囊括进来。这种在假正例和假负例之间进行权衡的细节在5.7.2.1节中已有讨论。</p>
<p>上述例子说明了一个变量选择的“金本体制(gold standard)”：当所涉及的问题足够小（只有10个变量）时，我们可以精确地计算出完整的后验分布。当然，变量选择对于维度数量特别大的情况是最有用的。因为存在<span class="math notranslate nohighlight">\(2^D\)</span>个可能的模型（位向量），通常情况下不可能计算出完整的后验分布，以及一些统计量，比如MAP估计或者边缘包含概率。因此，本章我们将集中介绍如何进行算法的加速。但在此之前，我们将介绍在上面的例子中，是如何计算<span class="math notranslate nohighlight">\(p(\gamma|\mathcal{D})​\)</span>。</p>
<div class="section" id="spike-and-slab-model">
<h3>13.2.1 spike and slab model<a class="headerlink" href="#spike-and-slab-model" title="Permalink to this headline">¶</a></h3>
<p>后验分布定义为：
$<span class="math notranslate nohighlight">\(
p(\mathbf{\gamma}|\mathcal{D}) \propto p(\mathbf{\gamma})p(\mathcal{D}|\mathbf{\gamma}) \tag{13.5}
\)</span>$
我们首先考虑先验分布，其次是似然函数。</p>
<p>通常情况下，我们使用如下的关于位向量的先验分布：
$<span class="math notranslate nohighlight">\(
p(\mathbf{\gamma})=\prod_{j=1}^D{\rm{Ber}}(\gamma_j|\pi_0)=\pi_0^{\parallel\mathbf{\gamma}\parallel_0}(1-\pi_0)^{D-\parallel\mathbf{\gamma}\parallel_0} \tag{13.6}
\)</span><span class="math notranslate nohighlight">\(
其中\)</span>\pi_0<span class="math notranslate nohighlight">\(表示一个特征是相关联的概率，\)</span>\parallel\mathbf{\gamma}\parallel_0=\sum_{j=1}^D\gamma_j<span class="math notranslate nohighlight">\(为\)</span>\mathcal{l}_0<span class="math notranslate nohighlight">\(**伪范数(pseudo-norm)**，即向量中非零元素的数量。为了方便与后面模型进行比较，对上式取对数形式是十分有用的：
\)</span>$</p>
<div class="amsmath math notranslate nohighlight" id="equation-1e8b64dd-1e51-41df-886f-2a2560e47c87">
<span class="eqno">(14)<a class="headerlink" href="#equation-1e8b64dd-1e51-41df-886f-2a2560e47c87" title="Permalink to this equation">¶</a></span>\[\begin{align}
\log p(\mathbf{\gamma}|\pi_0) &amp; = \parallel\mathbf{\gamma}\parallel_0 \log \pi_0+(D-\parallel\mathbf{\gamma}\parallel_0)\log(1-\pi_0) \tag{13.7} \\
&amp; =\parallel\mathbf{\gamma}\parallel_0 (\log \pi_0-\log(1-\pi_0))+{\rm{const}} \tag{13.8}\\
&amp; = - \lambda \parallel\mathbf{\gamma}\parallel_0 + {\rm{const}} \tag{13.9}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
其中$\lambda \triangleq \log \frac{1-\pi_0}{\pi_0} $控制着模型的稀疏强度。\\我们可以按照下面的方式定义似然函数：
\end{aligned}\end{align} \]</div>
<p>p(\mathcal{D}|\mathbf{\gamma})=p(\mathbf{y}|\mathbf{X,\gamma})=\iint p(\mathbf{y}|\mathbf{X,w,\gamma})p(\mathbf{w}|\mathbf{\gamma},\sigma^2)p(\sigma^2)d\mathbf{w}d\sigma^2 \tag{13.10}
$<span class="math notranslate nohighlight">\(
出于符号上的方便，我们假设响应值已经被中心化处理，（即\)</span>\bar{y}=0<span class="math notranslate nohighlight">\(）,所以我们可以忽略所有的偏置项\)</span>\mu$。</p>
<p>我们现在讨论先验分布<span class="math notranslate nohighlight">\(p(\mathbf{w}|\mathbf{\gamma},\sigma^2)\)</span>。如果<span class="math notranslate nohighlight">\(\gamma_j=0\)</span>，即特征<span class="math notranslate nohighlight">\(j\)</span>是无关的，此时我们期望<span class="math notranslate nohighlight">\(w_j=0\)</span>。如果<span class="math notranslate nohighlight">\(\gamma_j=1\)</span>,我们期望<span class="math notranslate nohighlight">\(w_j\)</span>是非零的值。如果我们对输入进行标准化处理，一个合理的先验分布应该是<span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{0},\sigma^2\sigma_w^2)\)</span>，其中<span class="math notranslate nohighlight">\(\sigma_w^2\)</span>控制着我们期望系数与相关性变量（其变化范围由全局噪音水平<span class="math notranslate nohighlight">\(\sigma^2\)</span>表征）的相关程度。我们可以将这个先验分布总结为：
$<span class="math notranslate nohighlight">\(
p(w_j|\sigma^2,\gamma_j)=
\begin{cases}
\delta_0(w_j), &amp; \mbox{if } \gamma_j=0 \\
\mathcal{N}(w_j|0,\sigma^2\sigma_w^2) &amp; \mbox{if }\gamma_j=1 
\tag{13.11}
\end{cases}
\)</span><span class="math notranslate nohighlight">\(
第一项为在原点处的一个&quot;spike(针)&quot;。当\)</span>\sigma_w^2 \rightarrow \infty<span class="math notranslate nohighlight">\(，分布\)</span>p(w_j|\gamma_j=1)$将趋向于一个均匀分布，这被认为是一个具有恒定高度的“slab（平板）”。所以这被称为<strong>spike and slab</strong>模型。</p>
<p>我们可以将那些<span class="math notranslate nohighlight">\(w_j=0\)</span>的系数从模型中“清理”处理，因为在先验分布中，它们的取值被限制为0。所以式13.10变成如下的形式（假设似然函数服从高斯分布）：
$<span class="math notranslate nohighlight">\(
p(\mathcal{D}|\mathbf{\gamma})=\iint \mathcal{N}(\mathbf{y}|\mathbf{X}_\gamma\mathbf{w}_\gamma,\sigma^2\mathbf{I}_N)\mathcal{N}(\mathbf{w}_\gamma|\mathbf{0}_\gamma,\sigma^2\sigma_w^2\mathbf{I}_\gamma)p(\sigma^2)d\mathbf{w}_\gamma d\sigma^2 \tag{13.12}
\)</span><span class="math notranslate nohighlight">\(
其中\)</span>\mathbf{X}<em>\gamma<span class="math notranslate nohighlight">\(为新的设计矩阵，其中我们只选择了那些\)</span>\gamma_j=1<span class="math notranslate nohighlight">\(的列的值，\)</span>\mathbf{0}</em>\gamma=\mathbf{0}<em>{\parallel\gamma\parallel_0},\mathbf{I}</em>\gamma=\mathbf{I}<em>{\parallel\gamma\parallel_0}<span class="math notranslate nohighlight">\(。类似地，我们通过符号\)</span>\mathbf{w}</em>\gamma<span class="math notranslate nohighlight">\(来提醒读者该系数只表示那些\)</span>\gamma_j=1<span class="math notranslate nohighlight">\(的情况。在后面的内容中，我们定义一种更具一般性的先验分布\)</span>p(\mathbf{w}|\gamma,\sigma^2)=\mathcal{N}(\mathbf{w}|\mathbf{0}<em>\gamma,\sigma^2\mathbf{\Sigma}</em>\gamma)<span class="math notranslate nohighlight">\(，其中\)</span>\mathbf{\Sigma}<em>\gamma<span class="math notranslate nohighlight">\(表示任意的正定矩阵，而无需要求它满足\)</span>\mathbf{\Sigma}</em>\gamma=\sigma_w^2\mathbf{I}_\mathbf{\gamma}$.</p>
<p>基于上述的先验分布，我们现在可以计算边缘似然。如果噪声的方差已知，我们可以计算出（使用公式13.151）如下的边缘似然：
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-dfc93726-bf78-43e7-b58c-16e13cb18f09">
<span class="eqno">(15)<a class="headerlink" href="#equation-dfc93726-bf78-43e7-b58c-16e13cb18f09" title="Permalink to this equation">¶</a></span>\[\begin{align}
p(\mathcal{D}|\gamma,\sigma^2) &amp; = \int\mathcal{N}(\mathbf{y}|\mathbf{X}_\gamma\mathbf{w}_\gamma,\sigma^2\mathbf{I})\mathcal{N}(\mathbf{0},\sigma^2\mathbf{\Sigma}_\gamma)d\mathbf{w}_\gamma=\mathcal{N}(\mathbf{y}|\mathbf{0},\mathbf{C}_\gamma) \tag{13.13}\\
\mathbf{C}_\gamma &amp; \triangleq \sigma^2\mathbf{X}_\gamma\mathbf{\Sigma}_\gamma\mathbf{X}_\gamma^T+\sigma^2\mathbf{I}_N \tag{13.14}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[
如果噪声是未知的，我们可以为其添加一个先验分布，并且对它进行积分。通常我们使用先验分布$p(\sigma^2)={\rm{IG}}(\sigma^2|a_\sigma,b_\sigma)$。关于超参$a,b$的设置在一些文献中可以找到。如果我们使用$a=b=0$，我们将使用Jeffrey先验分布，即$p(\sigma^2)\propto \sigma^{-2}$。当我们对噪音项进行积分，我们将得到如下的更加复杂的关于边缘似然的表达式：
\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}## 13.3 $\mathcal{l}_1$ 正则：基本原理\\当我们的变量数量特别多时，计算后验分布$p(\gamma|\mathcal{D})$的峰值会十分困难。尽管贪心算法通常会奏效，但同时会陷入局部最优解。\\部分原因是因为变量$\gamma_j$是离散的，即$\gamma_j \in \{0,1\}$。在最优化领域，通常会放松这种hard级别的约束，使用连续变量代替离散变量。在前文介绍的spike-and-slab形式的先验分布中，事件$w_j=0$被赋予的概率质量是有限的，我们可以使用一种连续的先验分布替代它，在这种先验分布中，我们通过将更多的概率密度赋予0周边的区域，从而“鼓励”$w_j=0$。这种想法在7.4节的鲁棒性线性回归中首次被介绍。在那里我们介绍了Laplace分布具有肥尾的事实。此处，我们将探索该分布在$\mu=0$的附近呈现针状的事实。更加精确的表达形式为：
\end{aligned}\end{align} \]</div>
<p>p(\mathbf{w}|\lambda)=\prod_{j=1}^D{\rm{Lap}}(w_j|0,1/\lambda) \propto \prod_{j=1}^De^{-\lambda|w_j|} \tag{13.32}
$<span class="math notranslate nohighlight">\(
我们将赋予偏置量\)</span>w_0<span class="math notranslate nohighlight">\(一个均匀分布，即\)</span>p(w_0) \propto 1<span class="math notranslate nohighlight">\(。我们基于上述先验分布求解MAP估计。含惩罚项的负对数似然形式为：
\)</span><span class="math notranslate nohighlight">\(
f(\mathbf{w})=-\log p(\mathcal{D}|\mathbf{w})-\log p(\mathbf{w}|\lambda)={\rm{NLL}}(\mathbf{w})+\lambda||\mathbf{w}||_1 \tag{13.33}
\)</span><span class="math notranslate nohighlight">\(
其中\)</span>||\mathbf{w}||<em>1=\sum</em>{j=1}^D|w_j|<span class="math notranslate nohighlight">\(表示\)</span>\mathbf{w}<span class="math notranslate nohighlight">\(的\)</span>\mathcal{l}_1<span class="math notranslate nohighlight">\(正则。当\)</span>\lambda<span class="math notranslate nohighlight">\(的值比较合适时，估计量\)</span>\hat{\mathbf{w}}<span class="math notranslate nohighlight">\(将会变得稀疏，其原因我们将会在后文介绍。的确这可以看作是非凸\)</span>\mathcal{l}_0<span class="math notranslate nohighlight">\(目标的凸近似
\)</span><span class="math notranslate nohighlight">\(
\mathop{\arg\min}_{\mathbf{w}} {\rm{NLL}}(\mathbf{w})+\lambda||\mathbf{w}||_0 \tag{13.34}
\)</span><span class="math notranslate nohighlight">\(
在线性回归中，\)</span>\mathcal{l_1}<span class="math notranslate nohighlight">\(目标函数为：
\)</span>$</p>
<div class="amsmath math notranslate nohighlight" id="equation-e26eccd6-9da3-44bd-a9d5-d8a2b4532b18">
<span class="eqno">(16)<a class="headerlink" href="#equation-e26eccd6-9da3-44bd-a9d5-d8a2b4532b18" title="Permalink to this equation">¶</a></span>\[\begin{align}
f(\mathbf{w}) &amp; = \sum_{i=1}^N -\frac{1}{2\sigma^2}(y_i-(w_0+\mathbf{w}^T\mathbf{x}_i))^2+\lambda||\mathbf{w}||_1 \tag{13.35} \\
&amp; = {\rm{RSS}}(\mathbf{w})+\lambda^\prime||\mathbf{w}||_1 \tag{13.36}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
其中$\lambda^\prime=2\lambda\sigma^2$。这种方法被称为**基追踪去噪(basis pursuit denoising, BPDN)**。之所以叫这个名字我们将在后面介绍。一般情况下，为参数赋予一个均值为0的Laplace先验分布并计算MAP估计的技术被称为$\mathcal{l}_1$**正则(regularization)**。它可以与任意的凸或者非凸NLL项结合。许多针对该问题的算法被设计出来，我们将在13.4节介绍其中的一些内容。\\### 13.3.1 为什么$\mathcal{l}_1$正则可以得到稀疏解\\现在我们来解释为什么$\mathcal{l}_1$正则可以导致一个稀疏的解，而$\mathcal{l}_2$不行。我们以线性回归为例，尽管相似的结论同样适用于逻辑回归和其他广义线性模型。\\BPDN目标函数是具备如下形式的非平滑目标函数：
\end{aligned}\end{align} \]</div>
<p>\mathop{\min}_{\mathbf{w}} {\rm{RSS}}(\mathbf{w})+\lambda||\mathbf{w}||_1 \tag{13.37}
$<span class="math notranslate nohighlight">\(
我们可以将上式改写成含约束的光滑的目标函数形式：
\)</span><span class="math notranslate nohighlight">\(
\mathop{\min}_{\mathbf{w}} {\rm{RSS}}\       s.t. ||\mathbf{w}||_1 \le B \tag{13.38}
\)</span><span class="math notranslate nohighlight">\(
其中\)</span>B<span class="math notranslate nohighlight">\(为权重的\)</span>\mathcal{l}_1<span class="math notranslate nohighlight">\(范数的上确界：一个小的（紧的）边界值\)</span>B<span class="math notranslate nohighlight">\(对应于一个大的惩罚系数\)</span>\lambda$，反之亦然。式13.38被称为<strong>lasso</strong>，即”最小绝对收缩和选择操作(least absolute shrinkage and selection operator)”。我们将在后文介绍这个名字的由来。</p>
<p>类似地，我们可以将岭回归的形式写成：
$<span class="math notranslate nohighlight">\(
\mathop{\min}_{\mathbf{w}} {\rm{RSS}}(\mathbf{w})+\lambda||\mathbf{w}||_2^2 \tag{13.39}
\)</span><span class="math notranslate nohighlight">\(
或者一种含边界约束的版本：
\)</span><span class="math notranslate nohighlight">\(
\mathop{\min}_{\mathbf{w}} {\rm{RSS}}(\mathbf{w}) s.t. ||\mathbf{w}||_2^2 \le B \tag{13.40}
\)</span><span class="math notranslate nohighlight">\(
在图13.3中，我们绘制了\)</span>\rm{RSS}<span class="math notranslate nohighlight">\(目标函数以及\)</span>\mathcal{l}_2<span class="math notranslate nohighlight">\(和\)</span>\mathcal{l}_1<span class="math notranslate nohighlight">\(约束表面的轮廓线。根据含约束优化的理论，我们知道当目标函数的最低集合与约束表面（假设约束是激活的）相交时，才会出现最优解。如果我们放松约束\)</span>B<span class="math notranslate nohighlight">\(，那么这一点在几何上会显得更加清晰，我们“放大”\)</span>\mathcal{l}_1<span class="math notranslate nohighlight">\(“球”直到它与目标接触；显然相较于“边”，“球”的角要更容易与椭圆接触，尤其是高维的情况，因为角更加突出。角对应着稀疏解，它坐落在坐标轴上。相反，当我们放大\)</span>\mathcal{l}_2$球的时候，它可以在任何点与目标接触；因为没有角的存在，所以也没有稀疏化的倾向。</p>
<p>为了从另一个角度说明这一点，当我们使用岭回归时，稀疏解，比如<span class="math notranslate nohighlight">\(\mathbf{w}=(1,0)​\)</span>的先验成本，与稠密解，比如<span class="math notranslate nohighlight">\(\mathbf{w}=(1/\sqrt{2},1/\sqrt{2})​\)</span>，只要它们有同样的<span class="math notranslate nohighlight">\(\mathcal{l}_2范数​\)</span>：
$<span class="math notranslate nohighlight">\(
||(1,0)||_2=||(1/\sqrt{2},1/\sqrt{2})||_2=1 \tag{13.41}
\)</span><span class="math notranslate nohighlight">\(
然而，对于lasso,\)</span>\mathbf{w}=(1,0)<span class="math notranslate nohighlight">\(要比\)</span>\mathbf{w}=(1/\sqrt{2},1/\sqrt{2})<span class="math notranslate nohighlight">\(便宜，因为：
\)</span><span class="math notranslate nohighlight">\(
||(1,0)||_1=1\lt||(1/\sqrt{2},1/\sqrt{2})||_1=\sqrt{2} \tag{13.42}
\)</span><span class="math notranslate nohighlight">\(
说明\)</span>\mathcal{l}_1<span class="math notranslate nohighlight">\(正则能导致稀疏解的检验最优解成立时的条件。我们将在13.3.2节介绍。
\)</span><span class="math notranslate nohighlight">\(
Figure 13.4
\)</span>$</p>
</div>
<div class="section" id="lasso">
<h3>13.3.2 lasso最优解条件<a class="headerlink" href="#lasso" title="Permalink to this headline">¶</a></h3>
<p>lasso的目标函数形式为：
$<span class="math notranslate nohighlight">\(
f(\mathbf{\theta})={\rm{RSS}}(\mathbf{\theta})+\lambda||\mathbf{w}||_1 \tag{13.43}
\)</span><span class="math notranslate nohighlight">\(
不幸地是，当\)</span>w_j=0<span class="math notranslate nohighlight">\(时，\)</span>||\mathbf{w}||_1$项是不可微的。这是一个**非平滑(non-smooth)**优化问题的例子。</p>
<p>为了解决非平滑函数，我们需要拓展导数的符号。我们定义一个（凸）函数<span class="math notranslate nohighlight">\(f:\mathcal{I}\rightarrow \mathbb{R}\)</span>在点<span class="math notranslate nohighlight">\(\theta_0\)</span>的**次导数(subderivative)<strong>或者</strong>次梯度(subgradient)**为一个标量<span class="math notranslate nohighlight">\(g\)</span>，使其满足:
$<span class="math notranslate nohighlight">\(
f(\theta)-f(\theta_0)\ge g(\theta-\theta_0) \ \forall\theta\in\mathcal{I} \tag{13.44}
\)</span>$</p>
<p>其中<span class="math notranslate nohighlight">\(\mathcal{I}\)</span>为包含<span class="math notranslate nohighlight">\(\theta_0\)</span>的区间。图13.4给出了示意图。我们定义次导数的集合为区间<span class="math notranslate nohighlight">\([a,b]\)</span>，其中<span class="math notranslate nohighlight">\(a,b\)</span>分别为单侧极限
$<span class="math notranslate nohighlight">\(
a=\mathop{\lim}_{\theta\rightarrow\theta_0^-}\frac{f(\theta)-f(\theta_0)}{\theta-\theta_0},b=\mathop{\lim}_{\theta\rightarrow\theta_0^+}\frac{f(\theta)-f(\theta_0)}{\theta-\theta_0} \tag{13.46}
\)</span><span class="math notranslate nohighlight">\(
所有次导数的集合\)</span>[a,b]<span class="math notranslate nohighlight">\(被称为函数\)</span>f<span class="math notranslate nohighlight">\(在点\)</span>\theta_0<span class="math notranslate nohighlight">\(处的**次微分(subdifferential)**，符号上表示为\)</span>\partial f(\theta)|<em>{\theta_0}<span class="math notranslate nohighlight">\(。举例来说，绝对值函数\)</span>f(\theta)=|\theta|<span class="math notranslate nohighlight">\(，其次导数定义为：
\)</span><span class="math notranslate nohighlight">\(
\partial f(\theta)=
\begin{cases}
\{-1\}, &amp; {\rm{if}}\ \theta\lt0 \\
[-1,1], &amp; {\rm{if}}\ \theta=0 \\
\{+1\}, &amp; {\rm{if}}\ \theta\gt0
\tag{13.47}
\end{cases}
\)</span><span class="math notranslate nohighlight">\(
如果函数在每个点都可微，则\)</span>\partial f(\theta)={\frac{df(\theta)}{d\theta}}<span class="math notranslate nohighlight">\(。类别于标准的计算结果，函数\)</span>f<span class="math notranslate nohighlight">\(局部最小值所在的点\)</span>\hat{\theta}<span class="math notranslate nohighlight">\(满足充要条件\)</span>0\in\partial f(\theta)|</em>\hat{\theta}$。</p>
<p>接下来我们将上述概念应用在lasso问题中。首先忽略非平滑的惩罚项。结果表明：
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-f798d5bc-ff89-4152-be53-fc0e072ec7e0">
<span class="eqno">(17)<a class="headerlink" href="#equation-f798d5bc-ff89-4152-be53-fc0e072ec7e0" title="Permalink to this equation">¶</a></span>\[\begin{align}
\frac{\partial}{\partial w_j} &amp;=a_jw_j-c_j \tag{13.48} \\
a_j &amp; = 2\sum_{i=1}^{n}x_{ij}^2 \tag{13.49} \\
c_j &amp; = 2\sum_{i=1}^{n}x_{ij}(y_i-\mathbf{w}_{-j}^T\mathbf{x}_{i,-j}) \tag{13.50}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
其中$\mathbf{w}_{-j}$表示除了分量$j$以外的元素，$\mathbf{x}_{i,-j}$表示相同的意思。我们发现$c_j$是（正比于）第$j$个特征$\mathbf{x}_{:,j}$与因为其他特征所导致的残差项$\mathbf{r}_{-j}=\mathbf{y}-\mathbf{X}_{:,-j}\mathbf{w}_{-j}$之间的相关系数。所以$c_j$的幅值表征了特征$j$对于预测$\mathbf{y}$的重要性（相对于其他特征和当前参数）。\\在增加了惩罚项之后，我们发现次导数由下式给定：
\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-e2754e45-783e-4065-a181-ba07db7ec23f">
<span class="eqno">(18)<a class="headerlink" href="#equation-e2754e45-783e-4065-a181-ba07db7ec23f" title="Permalink to this equation">¶</a></span>\[\begin{align}
\partial_{w_j}f(\mathbf{w}) &amp; = (a_jw_j-c_j)+\lambda\partial_{w_j}||\mathbf{w}||_1 \tag{13.51} \\
&amp; =
\begin{cases}
\{a_jw_j-c_j-\lambda\} &amp; {\rm{if}}\ w_j \lt 0 \\
[-c_j-\lambda,-c_j+\lambda] &amp; {\rm{if}} \ w_j=0 \\
\{a_jw_j-c_j+\lambda\} &amp; {\rm{if}}  \ w_j \gt 0
\tag{13.52}
\end{cases}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[
因此$\mathbf{w}$是局部最优解的充要条件是:
\]</div>
<p>\mathbf{X}^T(\mathbf{X}\mathbf{w-\mathbf{y}})_j \in
\begin{cases}
{-\lambda}  &amp; {\rm{if}} \ w_j \lt 0 \
[-\lambda, \lambda]  &amp; {\rm{if}} \ w_j = 0 \
{\lambda} &amp; {\rm{if}} \ w_j \gt 0 \tag{13.53}
\end{cases}
$$
<strong>译者注</strong>:个人以为在式(13.53)中在两侧的导数符号反了。</p>
<p>根据<span class="math notranslate nohighlight">\(c_j\)</span>的取值，<span class="math notranslate nohighlight">\(\partial_{w_j}f(\mathbf{w})=0\)</span>的解存在如下的三种情况：</p>
<ol>
<li><p>如果<span class="math notranslate nohighlight">\(c_j \lt -\lambda\)</span>，也就是说特征与残差呈强烈的负相关关系，在这种情况下次导数为0的条件是<span class="math notranslate nohighlight">\(\hat{w}_j=\frac{c_j+\lambda}{a_j}\lt0\)</span>.</p></li>
<li><p>如果<span class="math notranslate nohighlight">\(c_j \in [-\lambda,\lambda]\)</span>，也就是说特征与残差之间呈现弱相关的关系，此时次导数为0的条件是<span class="math notranslate nohighlight">\(\hat{w}_j=0\)</span>.</p></li>
<li><p>如果<span class="math notranslate nohighlight">\(c_j \gt \lambda\)</span>,也就是说特征与残差呈现强烈的正相关关系，此时次导数为0的条件是<span class="math notranslate nohighlight">\(\hat{w}_j=\frac{c_j-\lambda}{a_j}\gt 0\)</span>.</p>
<p>总结下来，我们有：</p>
</li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{w}_j(c_j)=
\begin{cases}
(c_j+\lambda)/a_j &amp; {\rm{if}} \ c_j \lt -\lambda \\
0 &amp; {\rm{if}} \ c_j \in [-\lambda, \lambda] \\
(c_j - \lambda)/a_j &amp; {\rm{if}}\ c_j \gt \lambda
\tag{13.54}
\end{cases}
\end{split}\]</div>
<p>我们可以将上式写成:
$<span class="math notranslate nohighlight">\(
\hat{w}_j = {\rm{soft}}(\frac{c_j}{a_j};\frac{\lambda}{a_j}) \tag{13.55}
\)</span><span class="math notranslate nohighlight">\(
其中
\)</span><span class="math notranslate nohighlight">\(
{\rm{soft}}(a;\delta)\triangleq {\rm{sign}}(a)(|a|-\delta)_+ \tag{13.56}
\)</span><span class="math notranslate nohighlight">\(
其中\)</span>x_+=\max(x,0)<span class="math notranslate nohighlight">\(。上式被称为**柔性阈值(soft thresholding)**。图13.5(a)给出了说明,其中我们绘制了\)</span>\hat{w}_j<span class="math notranslate nohighlight">\(与\)</span>c_j<span class="math notranslate nohighlight">\(的关系曲线。黑色虚线\)</span>w_j=c_j/a_j<span class="math notranslate nohighlight">\(对应于最小二乘解。红色实线，代表了正则估计\)</span>\hat{w}_j(c_j)<span class="math notranslate nohighlight">\(，该解将虚线向下(或向上)移动了\)</span>\lambda<span class="math notranslate nohighlight">\(,除了当\)</span>-\lambda \le c_j  \le \lambda<span class="math notranslate nohighlight">\(，此时的\)</span>\hat{w}_j=0$。</p>
<p>相反，在图13.5(b)中，我们展示了<strong>生硬阈值(hard thresholding)</strong>。在这种情况下，当<span class="math notranslate nohighlight">\(-\lambda \le c_j \le \lambda\)</span>时，<span class="math notranslate nohighlight">\(w_j=0\)</span>，但在这个区间之外的<span class="math notranslate nohighlight">\(w_j\)</span>并没有被压缩。柔性阈值的曲线并没有与对角线相一致，这意味着哪怕是很大的系数都有向0收缩的趋势；因此lasso是一个有偏估计。这并不是我们想要的，因为如果似然值（通过<span class="math notranslate nohighlight">\(c_j\)</span>）表明系数<span class="math notranslate nohighlight">\(w_j\)</span>应该很大，我们并不希望对这些参数进行收缩。我们将在13.6.2节讨论关于这个话题的更多细节。</p>
<p>最后我们可以理解为什么Tibshirani发明了单词”lasso”:它的全称是”least absolute selection and shrinkage operator”,因为它在所有变量中选择了一个子集，并且通过惩罚绝对值的方式对所有系数进行了收缩。如果<span class="math notranslate nohighlight">\(\lambda=0\)</span>，我们将得到<span class="math notranslate nohighlight">\(OLS\)</span>解。如果<span class="math notranslate nohighlight">\(\lambda \ge \lambda_{max}\)</span>，我们将得到<span class="math notranslate nohighlight">\(\hat{\mathbf{w}}=0\)</span>,其中
$<span class="math notranslate nohighlight">\(
\lambda_{max}=||\mathbf{X}^T\mathbf{y}||_{\infty}=\max_j|\mathbf{y}^T\mathbf{x}_{:,j}| \tag{13.57}
\)</span><span class="math notranslate nohighlight">\(
上式的计算依据为：如果对于所有的\)</span>j<span class="math notranslate nohighlight">\(，\)</span>(\mathbf{X^T\mathbf{y}})_j \in [-\lambda, \lambda]<span class="math notranslate nohighlight">\(都成立，那么\)</span>\mathbf{0}<span class="math notranslate nohighlight">\(将会是最优解。一般情况下，对于一个\)</span>\mathcal{l}_1<span class="math notranslate nohighlight">\(正则目标函数，其最大的惩罚为:
\)</span><span class="math notranslate nohighlight">\(
\lambda_{max}=\max_j|\nabla_jNLL(\mathbf{0})| \tag{13.58}
\)</span>$</p>
</div>
<div class="section" id="lasso-ridge">
<h3>13.3.3 最小二乘，lasso,ridge和子集选择的比较<a class="headerlink" href="#lasso-ridge" title="Permalink to this headline">¶</a></h3>
<p>通过对比lasso与最小二乘,含<span class="math notranslate nohighlight">\(\mathcal{l}_2\)</span>正则和<span class="math notranslate nohighlight">\(\mathcal{l}_0\)</span>正则的最小二乘的区别，我们可以更加深入地理解<span class="math notranslate nohighlight">\(\mathcal{l}_1\)</span>正则。为了简单起见，假设<span class="math notranslate nohighlight">\(\mathbf{X}\)</span>所有的特征都已经被正交化，即<span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}=\mathbf{I}\)</span>。在这种情况下，<span class="math notranslate nohighlight">\(RSS\)</span>由下式给定:
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-a60b2c2a-d6f0-4518-af64-592481695897">
<span class="eqno">(19)<a class="headerlink" href="#equation-a60b2c2a-d6f0-4518-af64-592481695897" title="Permalink to this equation">¶</a></span>\[\begin{align}
{\rm{RSS}}(\mathbf{w}) &amp; = ||\mathbf{y}-\mathbf{Xw}||^2=\mathbf{y}^T\mathbf{y}+\mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}-2\mathbf{w}^T\mathbf{X}^T\mathbf{y} \tag{13.59} \\
&amp; = {\rm{const}} + \sum_kw_k^2 -2 \sum_k\sum_iw_kx_{ik}y_i \tag{13.60}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
所以我们发现，上式分解为了很多项的和，其中每个维度代表一项。所以，我们可以写出$MAP$和$ML$估计的解析解，具体如下：\\- $\rm{\mathbf{MLE}}$:$OLS$解为:
  \end{aligned}\end{align} \]</div>
<p>\hat{w}<em>k^{OLS}=\mathbf{x}</em>{:,k}^T\mathbf{y} \tag{13.61}
$$</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./MLAPP_BOOK"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="12.LinearModelwithLatentVariable.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">12 隐变量线性模型</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="14.KernelMethod.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">14 核方法</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Kevin Murphy<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>