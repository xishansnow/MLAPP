
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>03 离散数据的生成式模型 &#8212; 机器学习：概率视角</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e2363ea40746bee74734a24ffefccd78.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="04 高斯模型" href="04.GaussianModels.html" />
    <link rel="prev" title="02 概率论" href="02.Probability.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">机器学习：概率视角</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../preface.html">
   前言
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01.Introduction.html">
   01 引言
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02.Probability.html">
   02 概率论
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   03 离散数据的生成式模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04.GaussianModels.html">
   04 高斯模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05.BayesianStatistics.html">
   05 贝叶斯学派统计思想
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06.FrequentistStatistics.html">
   06 频率学派统计思想
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07.LinearRegression.html">
   07 线性回归
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08.LogisticRegression.html">
   08 逻辑回归
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09.GeneralizedLinearModelsAndTheExponentialFamily.html">
   09 广义线性模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10.DirectedGraphicalModels%20%28BayesNets%29.html">
   10 有向图模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11.MixtureModelandEMAlgorithm.html">
   11 混合模型与EM算法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12.LinearModelwithLatentVariable.html">
   12 隐变量线性模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13.SparseLinearModel.html">
   13 稀疏线性模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14.KernelMethod.html">
   14 核方法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15.GaussianProcesses.html">
   15 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16.AdaptiveBasisFunctionModel.html">
   16 自适应基函数模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="17.MarkovAndHiddenMarkovModel.html">
   17 马尔科夫与隐马尔科夫模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="18.StateSpaceModels.html">
   18 状态空间模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="19.UndirectedGraphicalModels%28MarkovRandomFields%29.html">
   19 无向图模型（马尔科夫随机场）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20.ExactInferenceForGraphicalModels.html">
   20 概率图的精确推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="21.VariationalInference.html">
   21 变分推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="22.MoreVariationalInference.html">
   22 更多变分推断方法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="23.MonteCarloInference.html">
   23 蒙特卡洛推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="24.MarkovChainMonteCarlo%28MCMC%29Inference.html">
   24 马尔科夫链蒙特卡洛 MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="25.Clustering.html">
   25 聚簇
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="26.GraphicalModelStructureLearning.html">
   26 概率图的结构学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="27.LatentVariableModelsForDiscreteData.html">
   27 离散数据的隐变量模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="28.DeepLearning.html">
   28 深度学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Abbreviations.html">
   本书常见缩写列表
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/MLAPP_BOOK/03.GenerativeModelsForDiscreteData.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   3.1 引言
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   3.2 贝叶斯基本概念
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     3.2.1 似然函数
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     3.2.2 先验分布
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     3.2.3 后验分布
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     3.2.4 后验预测性分布
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     3.2.5 一种更复杂的先验分布
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   3.3
   <strong>
    贝塔-二项分布
   </strong>
   模型
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     3.3.1 似然函数
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     3.3.2 先验分布
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     3.3.3 后验分布
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id13">
       3.3.3.1 后验分布的期望和众数
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id14">
       3.3.3.2 后验分布的方差
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     3.3.4 后验预测性分布
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id16">
       3.3.4.1 过拟合与黑天鹅悖论
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id17">
       3.3.4.2 预测未来多次试验的结果
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id18">
   3.4
   <strong>
    狄利克雷-多项分布
   </strong>
   模型
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id19">
     3.4.1 似然函数
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id20">
     3.4.2 先验分布
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id21">
     3.4.3 后验分布
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id22">
     3.4.4 后验预测性分布
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id23">
       3.4.4.1 工作案例：使用词袋法的语言模型
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id24">
   3.5 朴素贝叶斯分类器
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nbc">
     3.5.1 NBC 模型训练
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#nbc-mle">
       3.5.1.1 NBC 的 MLE
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id25">
       3.5.1.2 贝叶斯方法下的朴素贝叶斯
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id26">
     3.5.2 使用模型进行预测
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-sum-exp">
     3.5.3 log-sum-exp 技巧
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id27">
     3.5.4 使用互信息进行特征选择
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id28">
     3.5.5 使用词袋法对文本进行分类 (
     <code class="docutils literal notranslate">
      <span class="pre">
       注：本节更多技术细节请读者参考其他文献
      </span>
     </code>
     )
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>03 离散数据的生成式模型<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2>3.1 引言<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>在 <code class="docutils literal notranslate"><span class="pre">第</span> <span class="pre">2.2.3.2</span> <span class="pre">节</span></code> 中，讨论了如何将贝叶斯法则应用到生成式分类器，从而实现对一个特征向量 <span class="math notranslate nohighlight">\(\mathbf {x}\)</span> 的分类：</p>
<div class="math notranslate nohighlight">
\[
p (y=c \mid \mathbf {x}, \boldsymbol {\theta}) \propto p (\mathbf {x} \mid y=c, \boldsymbol {\theta}) p (y=c \mid \boldsymbol {\theta}) \tag {3.1} \label {eqn:3.1}
\]</div>
<p>使用上述模型的关键之处在于为类条件密度（即似然）<span class="math notranslate nohighlight">\(p (\mathbf {x}|y=c,\boldsymbol {\theta})\)</span> 指定一个合适的形式，它定义了我们预期在每个类中看到什么样的数据。本章将集中讨论可观测数据属于离散型变量的情况。同时，将讨论在此类模型中如何推断未知参数的值。</p>
</div>
<div class="section" id="id3">
<h2>3.2 贝叶斯基本概念<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>思考一个问题：一个小孩如何学习理解一个单词的含义，比如单词 <code class="docutils literal notranslate"><span class="pre">dog</span></code> 。</p>
<p>我们可以做如下推测：小孩的父母指向关于此概念 ( 即单词 <code class="docutils literal notranslate"><span class="pre">dog</span></code> ) 的一些正确样例 ( 正例 ) ，然后说 <code class="docutils literal notranslate"><span class="pre">look</span> <span class="pre">at</span> <span class="pre">the</span> <span class="pre">cute</span> <span class="pre">dog!</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">mind</span> <span class="pre">the</span> <span class="pre">doggy</span></code> 等之类的话。很少出现父母指着一个错误的样例，然后说道 <code class="docutils literal notranslate"><span class="pre">look</span> <span class="pre">at</span> <span class="pre">that</span> <span class="pre">non-dog</span></code>。 当然，错误样例在主动学习过程中可能会用到 — 例如小孩说 <code class="docutils literal notranslate"><span class="pre">look</span> <span class="pre">at</span> <span class="pre">the</span> <span class="pre">dog</span></code> ，然后父母纠正道： <code class="docutils literal notranslate"><span class="pre">that's</span> <span class="pre">a</span> <span class="pre">cat,</span> <span class="pre">dear,</span> <span class="pre">not</span> <span class="pre">a</span> <span class="pre">dog</span></code> — 然而<strong>心理学研究表明人们可以只从正确样例中去理解概念</strong>。</p>
<p>可以将学习一个单词的含义视为 <code class="docutils literal notranslate"><span class="pre">概念学习</span> <span class="pre">(</span> <span class="pre">concept</span> <span class="pre">learning</span> <span class="pre">)</span></code> ，后者又等价于二值分类。要看到这一点：如果 <span class="math notranslate nohighlight">\(\mathbf {x}\)</span> 是关于概念 <span class="math notranslate nohighlight">\(C\)</span> 的一个正例，则定义 <span class="math notranslate nohighlight">\(f ( x ) =1\)</span> ，否则，定义 <span class="math notranslate nohighlight">\(f ( x ) =0\)</span> 。然后目标是通过学习指示函数 <span class="math notranslate nohighlight">\(f\)</span> ，该函数定义了哪些样例属于概念 <span class="math notranslate nohighlight">\(C\)</span>。通过引入函数 <span class="math notranslate nohighlight">\(f\)</span> 自身定义的不确定性 ( 或者说概念 <span class="math notranslate nohighlight">\(C\)</span> 中元素的不确定性 )，我们就可以用概率计算来模拟 <code class="docutils literal notranslate"><span class="pre">模糊集合论</span> <span class="pre">(</span> <span class="pre">fuzzy</span> <span class="pre">set</span> <span class="pre">theory</span> <span class="pre">)</span></code>，并得到推断结果。值得注意的是，标准的二值分类问题需要正例和负例同时存在，此处我们将设计一种只从正例中学习的方式。</p>
<p>为了教学目的，考虑一个关于概念学习的简单例子，叫做 <code class="docutils literal notranslate"><span class="pre">数字游戏</span> <span class="pre">(</span> <span class="pre">number</span> <span class="pre">game</span> <span class="pre">)</span></code> ，此游戏源自 <code class="docutils literal notranslate"><span class="pre">Josh</span> <span class="pre">Tenenbaum</span></code> 的博士论文。游戏过程如下：</p>
<p>首先选择一些简单的数学概念 <span class="math notranslate nohighlight">\(C\)</span> ，比如 <code class="docutils literal notranslate"><span class="pre">素数</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">一个在</span> <span class="pre">1</span> <span class="pre">到</span> <span class="pre">10</span> <span class="pre">之间的数字</span></code> 。然后给你一系列从概念 <span class="math notranslate nohighlight">\(C\)</span> 中随机抽取的正例 <span class="math notranslate nohighlight">\(\mathcal{D}=\{x_1,...,x_N\}\)</span>，然后问你一些新的测试样例 <span class="math notranslate nohighlight">\(\tilde{x}\)</span> 是否属于概念 <span class="math notranslate nohighlight">\(C\)</span>，即让你对 <span class="math notranslate nohighlight">\(\tilde{x}\)</span> 进行分类。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013044375.webp" /></p>
<p><strong>图 3.1</strong>  参与数字游戏的人给出的预测结果及其分布（ 8 个人的平均结果）。前两行代表看到数据 <span class="math notranslate nohighlight">\(\mathcal{D} = \{16\}\)</span> 和 <span class="math notranslate nohighlight">\(\mathcal{D} = \{60\}\)</span> 后的统计结果。这两个图说明了分布比较弥散的相似性。第 3 行：看到数据 <span class="math notranslate nohighlight">\(\mathcal{D} = \{16,8,2,64\}\)</span> 之后的预测结果。该图说明了某种基于规则的行为 ( 如：2 的幂次方 ) 。第 4 行：看到数据 <span class="math notranslate nohighlight">\(\mathcal{D} = \{16,23,19,20\}\)</span> 后的统计结果。这说明了某种聚集的相似度 ( 如：接近 20 的数字 )</p>
<p>为简单起见，假设所有数字都是在 1 和 100 之间的整数。现在告诉你数字 <code class="docutils literal notranslate"><span class="pre">16</span></code> 是概念中的正例。那么你还会觉得哪些数会是正例？ <code class="docutils literal notranslate"><span class="pre">17?</span> <span class="pre">6?</span> <span class="pre">32?</span></code> 还是 <code class="docutils literal notranslate"><span class="pre">99？</span></code> 。显然你很难判断哪个是正确的，所以预测会十分含糊。那些在某些方面与数字 <code class="docutils literal notranslate"><span class="pre">16</span></code> 更相似的数字更有可能是正确的，但 <code class="docutils literal notranslate"><span class="pre">相似</span></code> 又该如何定义呢？<code class="docutils literal notranslate"><span class="pre">17</span></code> 与 <code class="docutils literal notranslate"><span class="pre">16</span></code> 是相似的，因为它离 <code class="docutils literal notranslate"><span class="pre">16</span></code> 近。<code class="docutils literal notranslate"><span class="pre">6</span></code> 与 <code class="docutils literal notranslate"><span class="pre">16</span></code> 是相似的，因为它们有一位数完全相同。<code class="docutils literal notranslate"><span class="pre">32</span></code> 与 <code class="docutils literal notranslate"><span class="pre">16</span></code> 是相似的，因为它们都是偶数且都是 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">的幂次方</span></code>，但数字 <code class="docutils literal notranslate"><span class="pre">99</span></code> 好像不相似。所以存在一些数字与 <code class="docutils literal notranslate"><span class="pre">16</span></code> 的相似度要更高。</p>
<p>我们对上述问题进行形式化表达：</p>
<p>引入概率分布 <span class="math notranslate nohighlight">\(p ( \tilde {x} \mid \mathcal {D} )\)</span> ，代表对任意数字 <span class="math notranslate nohighlight">\(\tilde {x} \in\{1, \ldots, 100\}\)</span>，在给定数据 <span class="math notranslate nohighlight">\(\mathcal {D}\)</span> 的情况下，<span class="math notranslate nohighlight">\(\tilde {x} \in C\)</span> 的概率，被称为 <code class="docutils literal notranslate"><span class="pre">后验预测性分布</span></code>( posterior predictive distribution ) 。<code class="docutils literal notranslate"><span class="pre">图</span> <span class="pre">3.1</span> <span class="pre">(</span> <span class="pre">上</span> <span class="pre">)</span></code> 展示了参与该实验的志愿者给出的预测性分布。不难发现，人们基于不同相似度的标准，将与 <code class="docutils literal notranslate"><span class="pre">16</span></code> 相似的不同数作为正例输出。</p>
<p>现在假设告诉你 <code class="docutils literal notranslate"><span class="pre">8</span></code> , <code class="docutils literal notranslate"><span class="pre">2</span></code> 和 <code class="docutils literal notranslate"><span class="pre">64</span></code> 也是正例。你可能会猜测此隐藏的数学概念是 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">的幂次方</span></code> 。这其实就是 <code class="docutils literal notranslate"><span class="pre">归纳</span> <span class="pre">(</span> <span class="pre">induction</span> <span class="pre">)</span></code> 过程。基于此假设，预测性分布将会更加明确，如 <code class="docutils literal notranslate"><span class="pre">图</span> <span class="pre">3.1</span></code> 第 3 行所示，大部分概率质量都分配到了 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">的幂次方</span></code> 上。如果给你的数据集 <span class="math notranslate nohighlight">\(\mathcal {D}=\{16,23,19,20\}\)</span>，你可能会得到一个完全不同的 <code class="docutils literal notranslate"><span class="pre">泛化梯度</span> <span class="pre">(</span> <span class="pre">generalization</span> <span class="pre">gradient</span> <span class="pre">)</span></code> ，如 <code class="docutils literal notranslate"><span class="pre">图</span> <span class="pre">3.1</span> <span class="pre">(</span> <span class="pre">下</span> <span class="pre">)</span></code> 所示。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>泛化梯度指相似性程度不同的刺激引起不同强度的响应，它表明了泛化的水平，是泛化反应强度变化的指标。</p>
</div>
<p>如何解释此行为并且在机器学习中去模仿它？</p>
<p>归纳的经典方法是假设我们有一个概念的假设空间 <span class="math notranslate nohighlight">\(\mathcal {H}\)</span> ，比如：奇数、偶数、所有1到100之间的数、2的幂、所有以数字 <span class="math notranslate nohighlight">\(j\)</span> 结尾的数等。在假设空间中，与数据 <span class="math notranslate nohighlight">\(\mathcal {D}\)</span> 保持一致的子集被称为<code class="docutils literal notranslate"><span class="pre">版本空间</span></code>。随着观测到的样本增加，版本空间在逐步缩小，我们对概念的理解也越来越确定 (Mitchell 1997) 。</p>
<p>然而，版本空间并非故事的全部。在看到 <span class="math notranslate nohighlight">\(\mathcal {D}=\{16\}\)</span> 后，有许多规则能够满足一致性，该如何结合它们来预测 <span class="math notranslate nohighlight">\(\tilde {x} \in C\)</span> 呢？ 此外，在看到 <span class="math notranslate nohighlight">\(\mathcal {D}= \{16,8,2,64\}\)</span> 后，你为什么选择 「2 的幂」 ，而不是 「所有偶数」 ，或 「除 32 之外 2 的幂」，毕竟这两个规则也能够与证据保持一致啊？</p>
<p>下面将对此给出贝叶斯解释。</p>
<div class="section" id="id4">
<h3>3.2.1 似然函数<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>现在必须解释，在看到数据 <span class="math notranslate nohighlight">\(\mathcal {D}=\{16,8,2,64\}\)</span> 时，为什么会选择假设 <span class="math notranslate nohighlight">\(h_{\text {two }} \triangleq \text{2 的幂次方}\)</span>，而不是 <span class="math notranslate nohighlight">\(h_{\text {even }} \triangleq \text{偶数}\)</span>。比较直观的解释是希望避免 <code class="docutils literal notranslate"><span class="pre">可疑的巧合</span> <span class="pre">(</span> <span class="pre">suspicious</span> <span class="pre">coincidences</span> <span class="pre">)</span></code> ：如果真实概念是偶数，那为什么我们刚好只看到那些 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">的幂次方</span></code> 呢？</p>
<p>为形式化上述表达，假设样例是在概念的延拓中均匀随机采样得到的。Tenenbaum 称这种采样为 <code class="docutils literal notranslate"><span class="pre">强采样假设</span> <span class="pre">(</span> <span class="pre">strong</span> <span class="pre">sampling</span> <span class="pre">assumption</span> <span class="pre">)</span></code> ，从假设 <span class="math notranslate nohighlight">\(h\)</span> 中独立采样 <span class="math notranslate nohighlight">\(N\)</span> 个样本的概率为 ( 有放回 ) ：</p>
<div class="math notranslate nohighlight">
\[
p ( \mathcal {D} \mid h ) =\left [\frac {1}{\operatorname {size} ( h )}\right]^{N}=\left [\frac {1}{|h|}\right]^{N} \tag {3.2}
\]</div>
<p>此公式体现了 Tenenbaum 所说的 <code class="docutils literal notranslate"><span class="pre">尺度原则</span> <span class="pre">(</span> <span class="pre">size</span> <span class="pre">principle</span> <span class="pre">)</span></code> ，意思是说模型更倾向于与数据保持一致的最简单假设。此原则被称为 <code class="docutils literal notranslate"><span class="pre">奥卡姆剃刀</span> <span class="pre">(</span> <span class="pre">Occam's</span> <span class="pre">razor</span> <span class="pre">)</span></code> 。</p>
<p>为了说明上述原则如何奏效，令 <span class="math notranslate nohighlight">\(\mathcal {D}=\{16\}\)</span>，有 <span class="math notranslate nohighlight">\(p\left ( \mathcal {D} \mid h_{\text {two }}\right ) =1 / 6\)</span>，因为在小于 100 的整数中，只有 6 个数是 2 的幂次方，但 <span class="math notranslate nohighlight">\(p\left ( \mathcal {D} \mid h_{\text {even }}\right ) =1 / 50\)</span>，因为有 50 个偶数。所以 <span class="math notranslate nohighlight">\(h=h_{\text {two }}\)</span> 的似然大于 <span class="math notranslate nohighlight">\(h=h_{\text {even }}\)</span> 的似然。在观测到 4 个样例后，<span class="math notranslate nohighlight">\(h_{\text {two }}\)</span> 的似然为 <span class="math notranslate nohighlight">\(( 1 / 6 )^{4}=7.7 \times 10^{-4}\)</span> ，而 <span class="math notranslate nohighlight">\(h_{\text {even }}\)</span> 的似然为 <span class="math notranslate nohighlight">\(( 1 / 50 )^{4}=1.6 \times 10^{-7}\)</span> 。两个假设之间的 <code class="docutils literal notranslate"><span class="pre">似然比</span> <span class="pre">(</span> <span class="pre">likelihood</span> <span class="pre">ratio</span> <span class="pre">)</span></code> 接近 <span class="math notranslate nohighlight">\(5000:1\)</span> ，更倾向于 <span class="math notranslate nohighlight">\(h_{\text {two }}\)</span>。<code class="docutils literal notranslate"><span class="pre">似然比</span></code>量化了人们的直觉，即数据 <span class="math notranslate nohighlight">\(D=\{16,8,2,64\}\)</span> 如果从假设 <span class="math notranslate nohighlight">\(h_{\text {even }}\)</span> 中产生的话，会是一个非常可疑的巧合。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注：概念的延拓指属于该概念的所有实例构成的集合，例如：</p>
<ul class="simple">
<li><p>概念 <code class="docutils literal notranslate"><span class="pre">偶数</span></code> 的延拓就是 ${2,4,6, \ldots, 98,100}；</p></li>
<li><p>概念 <code class="docutils literal notranslate"><span class="pre">以</span> <span class="pre">9</span> <span class="pre">结尾的数</span></code> 的延拓是 <span class="math notranslate nohighlight">\(\{9,19, \ldots, 99\}\)</span>。</p></li>
</ul>
</div>
</div>
<div class="section" id="id5">
<h3>3.2.2 先验分布<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>假设 <span class="math notranslate nohighlight">\(\mathcal {D}=\{16,8,2,64\}\)</span> ，概念 <span class="math notranslate nohighlight">\(h'=\text {“除了 32 以外的 2 的幂次方”}\)</span> 比概念 <span class="math notranslate nohighlight">\(h = \text {“ 2 的幂次方”}\)</span> 更有可能，因为不需要解释为什么 32 恰好没出现的巧合 ( 如果我们猜测正确的概念是 <span class="math notranslate nohighlight">\(h\)</span> ，那么需要解释为什么偏偏 32 没出现在 <span class="math notranslate nohighlight">\(\mathcal {D}\)</span> 中 ) 。</p>
<p>然而，假设 <span class="math notranslate nohighlight">\(h'=\text {“除了 32 以外的 2 的幂次方”}\)</span> 在概念上好像并 <code class="docutils literal notranslate"><span class="pre">不自然</span></code> 。为表达这种不自然，我们可以对该概念赋予一个比较低的先验概率。当然，你的先验概率可能与我的不同。这种 <code class="docutils literal notranslate"><span class="pre">主观性</span> <span class="pre">(</span> <span class="pre">subjective</span> <span class="pre">)</span></code> 是贝叶斯推断饱受争议的原因所在，因为它意味着，一个小孩和一个数学教授会得到不同的答案。事实上，小孩和教授可能不仅有不同的先验概率，还会有不同的假设空间。然而，可以通过将孩子和数学教授的假设空间定义为相同，然后在某些“高级”概念上将孩子的优先权重设置为 0 来巧妙解决此问题。因此，先验和假设空间之间没有明显的区别。</p>
<p>尽管先验存在争议性，但却十分有用。如果你观测到符合某些数学规则的一些数字，比如 1200, 1500, 900 和 1400，你可能会认为 400 有可能也服从此规则，但 1183 不太可能。但如果你观测到的是一个健康人的胆固醇水平，那你很有可能认为 400 不可能，但 1183 有可能。所以可以发现先验分布形成了一个快速学习的机制，即我们将影响问题的一些背景知识考虑进来，而如果没有这些背景知识，几乎不可能实现快速学习 ( 比如从小的样本中学习 ) 。</p>
<p>那么该如何使用先验呢？出于演示目的，我们使用一个简单的先验分布，该先验分布中 30 个简单的数学概念服从均匀分布，比如说： <code class="docutils literal notranslate"><span class="pre">偶数</span></code> ， <code class="docutils literal notranslate"><span class="pre">奇数</span></code> ， <code class="docutils literal notranslate"><span class="pre">素数</span></code> ， <code class="docutils literal notranslate"><span class="pre">以</span> <span class="pre">9</span> <span class="pre">结束的数</span></code> 等。为了让事情更加有趣，赋予 <code class="docutils literal notranslate"><span class="pre">偶数</span></code> 和 <code class="docutils literal notranslate"><span class="pre">奇数</span></code>两个概念更高的先验权值，同时，包含两个有些不自然的概念，比如说 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">的幂次方以及</span> <span class="pre">37</span></code> 和 <code class="docutils literal notranslate"><span class="pre">除了</span> <span class="pre">32</span> <span class="pre">的</span> <span class="pre">2</span> <span class="pre">的幂次方</span></code> ，但赋予这两个概念较低的先验权值。<code class="docutils literal notranslate"><span class="pre">图</span> <span class="pre">3.2</span> <span class="pre">(</span> <span class="pre">左</span> <span class="pre">)</span></code> 绘制了该先验分布。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210610130450c1.webp" /></p>
<p><strong>图 3.2</strong> <span class="math notranslate nohighlight">\(\mathcal{D}=\{16\}\)</span> 情况下的先验分布、似然函数和后验分布。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210721184752_bf.webp" /></p>
<p><strong>图 3.3</strong> <span class="math notranslate nohighlight">\(\mathcal{D}=\{16，8，2，64\}\)</span> 的先验分布、似然函数和后验分布。基于 (Tenenbaum 1999)。</p>
</div>
<div class="section" id="id6">
<h3>3.2.3 后验分布<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>后验分布只是似然与先验乘积并进行归一化后的结果。在当前的数字游戏中，有：</p>
<div class="math notranslate nohighlight">
\[
p ( h \mid \mathcal {D} ) =\frac {p ( \mathcal {D} \mid h ) p ( h )}{\sum_{h^{\prime} \in \mathcal {H}} p\left ( \mathcal {D}, h^{\prime}\right )}=\frac {p ( h ) \mathbb {I} ( \mathcal {D} \in h ) /|h|^{N}}{\sum_{h^{\prime} \in \mathcal {H}} p\left ( h^{\prime}\right ) \mathbb {I}\left ( \mathcal {D} \in h^{\prime}\right ) /\left|h^{\prime}\right|^{N}} \tag {3.3}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathbb {I}(\mathcal {D} \in h)=1\)</span> 的充要条件是所有数据都在假设 <span class="math notranslate nohighlight">\(h\)</span> 的延拓中。图 3.2 绘制了  <span class="math notranslate nohighlight">\(\mathcal {D}=\{16\}\)</span> 时，对应的先验、似然和后验。不难发现后验是先验和似然的组合。对于大部分概念，其先验分布都是均匀的，所以后验分布几乎正比于似然。但那些不自然的概念 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">的幂次方以及</span> <span class="pre">37</span></code> 和 <code class="docutils literal notranslate"><span class="pre">除了</span> <span class="pre">32</span> <span class="pre">的</span> <span class="pre">2</span> <span class="pre">的幂次方</span></code> ，尽管有较高的似然，但由于先验很低导致最终后验也很低。相反，<code class="docutils literal notranslate"><span class="pre">奇数</span></code> 和 <code class="docutils literal notranslate"><span class="pre">偶数</span></code> 的先验较高，但由于其似然很低，所以最终的后验较低。</p>
<p><code class="docutils literal notranslate"><span class="pre">图</span> <span class="pre">3.3</span></code> 绘制了 <span class="math notranslate nohighlight">\(\mathcal {D}=\{16，8，2，64\}\)</span> 情况下的先验、似然和后验。此时，似然更加集中在概念 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">的幂次方</span></code> 周围，并最终支配了后验。这样最终可以找出正确的那个概念。此时，可以发现对那些不自然的概念赋予低先验概率的必要性，否则将在现有数据上出现过拟合现象，即选择概念 <code class="docutils literal notranslate"><span class="pre">除了</span> <span class="pre">32</span> <span class="pre">的</span> <span class="pre">2</span> <span class="pre">的幂次方</span></code> 。</p>
<p>通常情况下，当拥有足够多数据时，后验 <span class="math notranslate nohighlight">\(p (h \mid \mathcal {D})\)</span> 将会集中分布于一个单独的概念，此单独的概念被称为 <code class="docutils literal notranslate"><span class="pre">最大后验概率</span> <span class="pre">(</span> <span class="pre">maximum</span> <span class="pre">a</span> <span class="pre">posteriori,</span> <span class="pre">MAP</span> <span class="pre">)</span> <span class="pre">估计</span></code>：</p>
<div class="math notranslate nohighlight">
\[
p ( h \mid \mathcal {D} ) \rightarrow \delta_{\hat {h}^{MAP}} \quad ( h ) \tag {3.4}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\hat {h}^{MAP}=\operatorname {argmax}_{h} p ( h \mid \mathcal {D} )\)</span> 为后验分布的众数，<span class="math notranslate nohighlight">\(\delta\)</span> 为 <code class="docutils literal notranslate"><span class="pre">狄利克雷测度</span> <span class="pre">(</span> <span class="pre">Dirac</span> <span class="pre">measure</span> <span class="pre">)</span></code> ，定义为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\delta_{x}(A)=\left\{\begin{array}{ll}
1 &amp; \text { if } x \in A \\
0 &amp; \text { if } x \notin A ~~\tag{3.5}
\end{array}\right. 
\end{align*}
\end{split}\]</div>
<p>值得注意的是，最大后验概率估计 MAP 可以写成：</p>
<div class="math notranslate nohighlight">
\[
\hat {h}^{\text {MAP }}=\underset {h}{\operatorname {argmax}} p ( \mathcal {D} \mid h ) p ( h ) =\underset {h}{\operatorname {argmax}}[\log p ( \mathcal {D} \mid h ) +\log p ( h )] \tag {3.6}
\]</div>
<p>因为似然与 <span class="math notranslate nohighlight">\(N\)</span> 呈指数幂关系，先验保持不变。当数据量 <span class="math notranslate nohighlight">\(N\)</span> 越来越多时，最大后验概率估计将收敛于 <code class="docutils literal notranslate"><span class="pre">最大似然估计</span> <span class="pre">(</span> <span class="pre">maximum</span> <span class="pre">likelihood</span> <span class="pre">estimate,</span> <span class="pre">MLE</span> <span class="pre">)</span></code> ：</p>
<div class="math notranslate nohighlight">
\[
\hat {h}^{\text {mle }} \triangleq \underset {h}{\operatorname {argmax}} p ( \mathcal {D} \mid h ) =\underset {h}{\operatorname {argmax}} \log p ( \mathcal {D} \mid h ) \tag {3.7}
\]</div>
<p>换言之，如果有足够多数据，会发生 <code class="docutils literal notranslate"><span class="pre">数据压倒先验</span> <span class="pre">(</span> <span class="pre">data</span> <span class="pre">overwhelms</span> <span class="pre">the</span> <span class="pre">prior</span> <span class="pre">)</span></code> 的情况，此时 MAP 将收敛于 MLE。</p>
<p>如果真实假设存在于起初的假设空间中，那么 MAP/MLE 将收敛于该真实假设。因此说贝叶斯推断以及最大似然估计是一致性估计 ( <code class="docutils literal notranslate"><span class="pre">第</span> <span class="pre">6.4.1</span> <span class="pre">节</span></code> 将讨论相关细节 ) 。我们也可以称<code class="docutils literal notranslate"><span class="pre">假设空间在极限情况下是可识别的</span></code>，这意味着可以在数据趋于无限时恢复“真理”。如果假设的类别不够充分，不足以表示“真理”，则会聚集在尽可能接近真理的假设上。</p>
</div>
<div class="section" id="id7">
<h3>3.2.4 后验预测性分布<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>后验分布是我们关于世界的内在 <code class="docutils literal notranslate"><span class="pre">信念状态(</span> <span class="pre">belief</span> <span class="pre">state</span> <span class="pre">)</span></code> ，而检验这种信念是否合理的方法是使用后验去预测客观世界中可观测的量。后验预测性分布定义为：</p>
<div class="math notranslate nohighlight">
\[
p ( \tilde {x} \in C \mid \mathcal {D} ) =\sum_{h} p ( y=1 \mid \tilde {x}, h ) p ( h \mid \mathcal {D} ) \tag {3.8}
\]</div>
<p>上式是基于每一个假设所作出的预测的加权平均，称为 <code class="docutils literal notranslate"><span class="pre">贝叶斯模型平均</span> <span class="pre">(</span> <span class="pre">Bayes</span> <span class="pre">model</span> <span class="pre">averaging</span> <span class="pre">)</span></code> 。<code class="docutils literal notranslate"><span class="pre">图</span> <span class="pre">3.4</span></code> 为图示。图中下方的点展示了每个假设的预测结果；图形右边的垂直曲线显示了每个假设的权重值。如果将每一行乘上相应的权重并相加，将得到顶部的分布。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210610130500b2.webp" /></p>
<p><strong>图 3.4</strong> 在看到数据 <span class="math notranslate nohighlight">\(\mathcal {D} = \{16\}\)</span> 的情况下，所有假设 <span class="math notranslate nohighlight">\(h\)</span> 的后验分布 <span class="math notranslate nohighlight">\(p (h|D)\)</span> ( 图右 ) 和相应的预测分布。图中的点说明该点与相应的假设是一致的。右侧的图表示每个假设 <span class="math notranslate nohighlight">\(h\)</span> 的权重。通过对每个点进行加权求和，我们得到上图顶端的 <span class="math notranslate nohighlight">\(p (\tilde {x} \in C|\mathcal{D})\)</span> 。</p>
<p>当我们有一个小的或者模糊的数据集时，其后验分布 <span class="math notranslate nohighlight">\(p (h|D)\)</span> 也会含糊不清，从而导致概率质量分布较广。然而，一旦 <code class="docutils literal notranslate"><span class="pre">将问题解决了</span></code> ( 即找到了那个概念 ) ，后验分布将退化成脉冲函数，且以 MAP 估计值为中心。此时预测性分布将变成：</p>
<div class="math notranslate nohighlight">
\[
p ( \tilde {x} \in C \mid \mathcal {D} ) =\sum_{h} p ( \tilde {x} \mid h ) \delta_{\hat {h}} ( h ) =p ( \tilde {x} \mid \hat {h} ) \tag {3.9}
\]</div>
<p>上式被称为预测概率密度的 <code class="docutils literal notranslate"><span class="pre">点估计</span> <span class="pre">(plug-in</span> <span class="pre">approximation</span> <span class="pre">)</span></code>，因其简单性而被广泛使用。但这种方式会低估我们的不确定度，与使用贝叶斯模型平均方法相比，预测结果将不会很平滑。</p>
<p>最大后验估计方法简单，但却不能解释从基于相似度的推断 ( 不确定的后验 ) 到基于规则的推断 ( 确定的后验 ) 的逐渐转变。举例来说，假设观测到数据集 <span class="math notranslate nohighlight">\(\mathcal{D}=\{16\}\)</span> ，如果基于之前的简单先验，最小一致性假设是 <code class="docutils literal notranslate"><span class="pre">所有4的幂</span></code>，因此只有 4 、16 和 64 获得非零的预测概率，显然这是过拟合的。</p>
<p>随着看到更多数据，比如说给定的数据集变成了 <span class="math notranslate nohighlight">\(\mathcal {D}=\{16,8,2,64\}\)</span>，最大后验概率估计支持的假设为 <code class="docutils literal notranslate"><span class="pre">所有</span> <span class="pre">2</span> <span class="pre">的幂次数</span></code> 。显然，基于此假设将有更多数值得到的概率值不为 0。也就是说采用点估计的方式，随着看到的数据不断增加，导致后验预测性分布的变化趋势 <code class="docutils literal notranslate"><span class="pre">由窄变宽</span></code>。相反，如果采用贝叶斯方法 ( 即贝叶斯模型平均 ) ，随着看到的数据量增加，后验预测性分布的变化趋势 <code class="docutils literal notranslate"><span class="pre">由宽变窄</span></code>，而这符合人们直观的感受。特别的，当数据集为 <span class="math notranslate nohighlight">\(\mathcal{D}=\{16\}\)</span> 时，后验分布中有许多假设的概率值都不为 0，所以相应的预测性分布也比较宽。然而，当看到的数据集变成 <span class="math notranslate nohighlight">\(\mathcal {D}=\{16,8,2,64\}\)</span> 时，后验分布的概率质量将主要集中在一个假设上，导致预测性分布变窄。</p>
<p>通过上面一个小的案例，我们发现，点估计与贝叶斯模型平均方法在最终的预测分布上存在很大不同，尽管随着数据量的增加，两者会收敛于同一个结果。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210610130504c7.webp" /></p>
<p><strong>图 3.5</strong> 使用了全假设空间的模型预测性分布。对比图 3.1，贝叶斯模型平均得到的预测结果只绘制那些可用的人类数据 <span class="math notranslate nohighlight">\(\tilde{x}\)</span>；这也是为什么顶行的分布较图 3.4 更稀疏。</p>
</div>
<div class="section" id="id8">
<h3>3.2.5 一种更复杂的先验分布<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>为模拟人类的行为，Tenenbaum 分析了一些实验数据，这些数据反映了人们是如何衡量数字之间相似度的，基于这些分析，使用了一个稍微复杂的先验概率分布。在此先验中，不仅包含与上文相似的一些数学概念，还包括在 <span class="math notranslate nohighlight">\(n\)</span> 和 <span class="math notranslate nohighlight">\(m\)</span> 之间的所有区间 ( <span class="math notranslate nohighlight">\(1≤ n, m ≤100\)</span> ) 。 ( 值得注意的是，这些假设并不是互斥的 ) 。因此最终的先验分布是两种先验的混合，一种基于数学规则，一种基于区间：</p>
<div class="math notranslate nohighlight">
\[
p ( h ) =\pi_{0} p_{\text {rules }} ( h ) +\left ( 1-\pi_{0}\right ) p_{\text {interval }} ( h ) \tag {3.10}
\]</div>
<p>在上述模型中唯一的自由参数就是相对权重 <span class="math notranslate nohighlight">\(\pi_0\)</span> 。只要 <span class="math notranslate nohighlight">\(\pi_0 &gt; 0.5\)</span> ( 即更多的人相信最终的假设应该是一个数学规则 ) ，最终的结果对 <span class="math notranslate nohighlight">\(\pi_0\)</span> 的大小并不会太敏感。基于此更大的假设空间，模型的预测分布如图 3.5 所示。它与如图 3.1 所示的人类的预测分布惊人的相似，尽管它并不适应人类预测的数据 ( 除了假设空间的选择 ) 。</p>
</div>
</div>
<div class="section" id="id9">
<h2>3.3 <strong>贝塔-二项分布</strong> 模型<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>在前面章节中介绍了数字游戏，在此游戏中，给定一系列的离散观测值，需要根据给定的观测值，在有限假设空间中推断出一个分布。在此过程中，我们的计算十分简单：只需要加法、乘法和除法的一些操作。然而，在很多应用中，未知参数是连续的，也就是说假设空间的大小为 <span class="math notranslate nohighlight">\(\mathrm {R}^K\)</span>，或者是它的子集。其中 <span class="math notranslate nohighlight">\(K\)</span> 是参数的数量。这使得数值计算变复杂，因为需要将求和的操作变成积分。然而，其基本思想是一致的。</p>
<p>为了说明此问题，首先考虑抛硬币实验，在给定一系列试验结果情况下，推断出硬币朝上的概率。尽管此模型看起来很简单，但它是本书后面将介绍的许多方法的基础，包括朴素贝叶斯分类器、马尔科夫模型等。同时此试验也具有重要的历史意义，因为它是贝叶斯在 1763 年的原始论文中分析过的例子。</p>
<p>我们将遵循已经熟悉的方法：<code class="docutils literal notranslate"><span class="pre">已知先验分布和似然函数，推断出后验分布和后验预测性分布</span></code>。</p>
<div class="section" id="id10">
<h3>3.3.1 似然函数<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>假设  <span class="math notranslate nohighlight">\(X_{i} \sim \operatorname {Ber} ( \theta )\)</span>( 伯努利分布 ) ，其中 <span class="math notranslate nohighlight">\(X_{i}=1\)</span> 代表硬币正面朝上，<span class="math notranslate nohighlight">\(X_{i}=0\)</span> 代表反面朝上。<span class="math notranslate nohighlight">\(\theta \in [0,1]\)</span> 为参数 ( 即正面朝上的概率 ) 。如果试验结果服从独立同分布（iid），则似然函数的形式为：</p>
<div class="math notranslate nohighlight">
\[
 p ( \mathcal {D} \mid \theta ) =\theta^{N_{1}} ( 1-\theta )^{N_{0}} \tag {3.11}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(N_{1}=\sum_{i=1}^{N} \mathbb {I}\left (x_{i}=1\right)\)</span> 表示正面朝上的次数，<span class="math notranslate nohighlight">\(N_{0}=\sum_{i=1}^{N} \mathbb {I}\left (x_{i}=0\right)\)</span> 表示反面朝上的次数。这两个统计量被称为数据的 <code class="docutils literal notranslate"><span class="pre">充分统计量</span></code> ( sufficient statistics ) ，因为只需要知道这两个统计量就可以从数据 <span class="math notranslate nohighlight">\(\mathcal {D}\)</span> 中推断出参数 <span class="math notranslate nohighlight">\(\theta\)</span> ( 当然 <span class="math notranslate nohighlight">\(N_{1}\)</span> 和 <span class="math notranslate nohighlight">\(N=N_{0}+N_{1}\)</span> 也可以作为充分统计量 ) 。</p>
<p>更加正式的表达，如果 <span class="math notranslate nohighlight">\(p (\boldsymbol {\theta} \mid \mathcal {D})=p (\boldsymbol {\theta} \mid \mathbf {s}( data ))\)</span> ，则称 <span class="math notranslate nohighlight">\(\mathbf {s}(\mathcal {D})\)</span> 为数据 <span class="math notranslate nohighlight">\(\mathcal {D}\)</span> 的充分统计量。如果先验采用均匀分布，那么上式等价于 <span class="math notranslate nohighlight">\(p (\mathcal {D}) \mid \boldsymbol {\theta} \propto p (\mathbf {s}(\mathcal {D}) \mid \boldsymbol {\theta})\)</span>。进而如果我们拥有两个具备相等充分统计量的数据集，那么最终将推断出相同的参数值 <span class="math notranslate nohighlight">\(\theta\)</span>。</p>
<p>现在假设数据由 <span class="math notranslate nohighlight">\(N=N_{1}+N_{0}\)</span> ( <span class="math notranslate nohighlight">\(N\)</span> 值大小固定 ) 次试验中观测到的正面朝上的次数 <span class="math notranslate nohighlight">\(N_{1}\)</span> 组成。在这种情况下， <span class="math notranslate nohighlight">\(N_{1} \sim \operatorname {Bin}(N, \theta)\)</span>， 其中 <span class="math notranslate nohighlight">\(\operatorname{Bin}\)</span> 代表二项分布，其概率质量函数定义为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\operatorname {Bin} ( k \mid n, \theta ) \triangleq\left ( \begin {array}{l}
n \\
k
\end {array}\right ) \theta^{k} ( 1-\theta )^{n-k} \tag {3.12}
\end{align*}
\end{split}\]</div>
<p>由于二项分布中的系数 <span class="math notranslate nohighlight">\(\left ( \begin {array}{l} n \\ k\end {array}\right )\)</span> 独立于参数 <span class="math notranslate nohighlight">\(\theta\)</span> ，二项模型的似然函数也与伯努利模型一致 ( 事实上只差一个系数 ) 。所以不管观测到的是统计量 <span class="math notranslate nohighlight">\(\mathcal {D}=\left ( N_{1}, N\right )\)</span> ，还是结果序列 <span class="math notranslate nohighlight">\(\mathcal {D}=\left\{x_{1}, \ldots, x_{N}\right\}\)</span> ，我们有关参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的推断结果应该都是一样的。</p>
</div>
<div class="section" id="id11">
<h3>3.3.2 先验分布<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>我们需要一个先验分布，其定义域为区间 <span class="math notranslate nohighlight">\([0,1]\)</span> 。为了让数学计算更加简单，使先验分布与似然函数的形式保持一致是一件很方便的事情，比如说先验分布具备如下形式：</p>
<div class="math notranslate nohighlight">
\[
p ( \theta ) \propto \theta^{\gamma_{1}} ( 1-\theta )^{\gamma_{2}} \tag {3.13}
\]</div>
<p>上式包含两个先验参数 <span class="math notranslate nohighlight">\(\gamma_1\)</span> 和 <span class="math notranslate nohighlight">\(\gamma_2\)</span> 。根据先验和似然，可以很容易计算出后验分布：</p>
<div class="math notranslate nohighlight">
\[
p ( \theta | \mathcal{D}) \propto p ( \mathcal {D} \mid \theta ) p ( \theta ) =\theta^{N_{1}} ( 1-\theta )^{N_{0}} \theta^{\gamma_{1}} ( 1-\theta )^{\gamma_{2}}=\theta^{N_{1}+\gamma_{1}} ( 1-\theta )^{N_{0}+\gamma_{2}} \tag {3.14}
\]</div>
<p>当先验与后验有相同形式时，我们称先验为相应似然函数的 <code class="docutils literal notranslate"><span class="pre">共轭先验</span></code> ( conjugate prior ) 。共轭先验有助于贝叶斯推断和计算，所以曾经被广泛使用。</p>
<p>在伯努利模型中，其共轭先验为<code class="docutils literal notranslate"><span class="pre">第</span> <span class="pre">2.4.6</span> <span class="pre">节</span></code>介绍的贝塔分布（不推导，直接使用）：</p>
<div class="math notranslate nohighlight">
\[
\operatorname {Beta} ( \theta \mid a, b ) \propto \theta^{a-1} ( 1-\theta )^{b-1} \tag {3.15}\label{3.15}
\]</div>
<p>贝塔先验中的参数 <span class="math notranslate nohighlight">\(a\)</span> ,<span class="math notranslate nohighlight">\(b\)</span> 被称为 <code class="docutils literal notranslate"><span class="pre">超参数</span></code> ( hyper-parameters ) 。我们可以以调整超参数的方式在模型中嵌入先验信念。例如：根据经验，我们相信参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的期望值为 0.7，标准差为 0.2，那么就可以尝试设置 <span class="math notranslate nohighlight">\(a=2.975\)</span>, <span class="math notranslate nohighlight">\(b=1.275\)</span> 。或者说我们相信参数 <span class="math notranslate nohighlight">\(\theta\)</span> 的期望值为 0.15，且高概率位于区间 <span class="math notranslate nohighlight">\(( 0.05,0.30 )\)</span> 之间，则可以尝试设置 <span class="math notranslate nohighlight">\(a=4.5\)</span>, <span class="math notranslate nohighlight">\(b=25.5\)</span> 。</p>
<p>如果我们对参数没有任何背景知识，只知道它位于区间 <span class="math notranslate nohighlight">\([0,1]\)</span>，那么就可以使用均匀分布作为先验，它是一个不包含任何信息的分布 ( <code class="docutils literal notranslate"><span class="pre">第</span> <span class="pre">5.4.2</span> <span class="pre">节</span></code> 会给出更多细节 ) 。 而在贝塔分布中，可以通过设置 <span class="math notranslate nohighlight">\(a=1\)</span>，<span class="math notranslate nohighlight">\(b=1\)</span> 来实现均匀分布。</p>
</div>
<div class="section" id="id12">
<h3>3.3.3 后验分布<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>伯努利（或二项）似然函数与贝塔先验分布相乘将得到后验分布：</p>
<div class="math notranslate nohighlight">
\[
  p(\theta \mid \mathcal {D} ) \propto \operatorname {Bin}\left ( N_{1} \mid \theta, N_{0}+N_{1}\right ) \operatorname {Beta} ( \theta \mid a, b )  \propto \operatorname{Beta}\left ( \theta \mid N_{1}+a, N_{0}+b\right ) \tag {3.16}\label{3.16}
\]</div>
<p>仔细观察会发现，后验分布是通过将先验的超参数与经验计数 ( 即 <span class="math notranslate nohighlight">\(N\)</span>_1 和 <span class="math notranslate nohighlight">\(N\)</span>_0 ) 相加得到的。出于此原因，超参数有时也被称为 <code class="docutils literal notranslate"><span class="pre">伪计数</span></code> ( pseudo counts ) 。先验的强度（也被称为先验的<code class="docutils literal notranslate"><span class="pre">等价样本尺寸</span></code>） 等于两种伪计数之和 <span class="math notranslate nohighlight">\(a + b\)</span>，其角色类似于数据集大小 <span class="math notranslate nohighlight">\(N_1 + N_0= N\)</span>。</p>
<p>图 3.6 ( a ) 展示了一个例子，通过一个尖的似然函数 ( 即有大量的采样样本 ) 更新一个较弱的 Beta ( 2,2 ) 先验分布，不难发现后验分布与似然函数几乎一样，因为采样得到的经验数据 <code class="docutils literal notranslate"><span class="pre">碾压</span></code> 了先验分布。图 3.6 ( b ) 给出了另一个例子，我们通过一个较尖的似然函数更新一个较强的 Beta ( 5,2 ) 先验分布，现在我们发现后验分布处在了先验分布与似然函数之间。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013050912.webp" /></p>
<p><strong>图 3.6</strong>  ( a ) 用充分统计量为 <span class="math notranslate nohighlight">\(N_1=3\)</span>，<span class="math notranslate nohighlight">\(N_0=17\)</span> 的二项分布来更新先验 <span class="math notranslate nohighlight">\(\operatorname{Beta} ( 2,2 )\)</span> ，形成了后验分布 <span class="math notranslate nohighlight">\(\operatorname{Beta} ( 5,19 )\)</span> 。 ( b ) 用充分统计量为 <span class="math notranslate nohighlight">\(N_1=11\)</span>，<span class="math notranslate nohighlight">\(N_0=13\)</span> 的二项分布来更新先验分布 <span class="math notranslate nohighlight">\(\operatorname{Beta} ( 5,2 )\)</span> ，形成了后验分布 <span class="math notranslate nohighlight">\(\operatorname{Beta} ( 16,15 )\)</span> 。</p>
<p>值得注意的是，序列化地多次更新后验分布等价于在一个批次中进行更新。为了明白这一点，假设我们手上有两个数据集 <span class="math notranslate nohighlight">\(\mathcal{D}_a\)</span> 和 <span class="math notranslate nohighlight">\(\mathcal{D}_b\)</span> ，且其充分统计量分别为 <span class="math notranslate nohighlight">\(N^a_1\)</span>，<span class="math notranslate nohighlight">\(N^a_0\)</span> 和 <span class="math notranslate nohighlight">\(N^b_1\)</span>，<span class="math notranslate nohighlight">\(N^b_0\)</span>。令 <span class="math notranslate nohighlight">\(N_1=N^a_1+N^b_1\)</span> 和 <span class="math notranslate nohighlight">\(N_0=N^a_0+N^b_0\)</span> 为联合数据集的充分统计量。则在批更新模式中有：</p>
<div class="math notranslate nohighlight">
\[
p\left ( \theta \mid \mathcal {D}_{a}, \mathcal {D}_{b}\right ) \quad \propto \operatorname {Bin}\left ( N_{1} \mid \theta, N_{1}+N_{0}\right ) \operatorname {Beta} ( \theta \mid a, b ) \propto \operatorname {Beta}\left ( \theta \mid N_{1}+a, N_{0}+b\right ) \tag {3.17}\label{3.17}
\]</div>
<p>在序列更新模式中，我们有：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
p\left ( \theta \mid \mathcal {D}_{a}, \mathcal {D}_{b}\right ) &amp; \propto p\left ( \mathcal {D}_{b} \mid \theta\right ) p\left ( \theta \mid \mathcal {D}_{a}\right ) \tag {3.18}\label{3.18}\\
&amp; \propto \operatorname {Bin}\left ( N_{1}^{b} \mid \theta, N_{1}^{b}+N_{0}^{b}\right ) \operatorname {Beta}\left ( \theta \mid N_{1}^{a}+a, N_{0}^{a}+b\right ) \tag {3.19}\label{3.19}\\
&amp; \propto \operatorname {Beta}\left ( \theta \mid N_{1}^{a}+N_{1}^{b}+a, N_{0}^{a}+N_{0}^{b}+b\right ) \tag {3.20}\label{3.20}
\end {align*}
\end{split}\]</div>
<p>这种序列更新与单批次更新等价的特点，使贝叶斯推断特别适合用在 <code class="docutils literal notranslate"><span class="pre">在线学习</span></code> ( online learning ) 的相关领域，我们将在<code class="docutils literal notranslate"><span class="pre">第</span> <span class="pre">8.5.5</span> <span class="pre">节</span></code>看到更多细节。</p>
<div class="section" id="id13">
<h4>3.3.3.1 后验分布的期望和众数<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h4>
<p><strong>（1）众数</strong></p>
<p>根据贝塔分布的属性计算公式（式 2.56 ），可知上述模型的最大后验概率估计 ( 即后验贝塔分布的众数 ) 为：</p>
<div class="math notranslate nohighlight">
\[
\hat {\theta}_{M A P}=\frac {a+N_{1}-1}{a+b+N-2} \tag {3.21}\label{3.21}
\]</div>
<p>当使用均匀先验分布时，<span class="math notranslate nohighlight">\(a=b=1\)</span> ，最大后验概率估计将退化为最大似然估计 ( MLE ) ，也就是正面朝上的次数占比：</p>
<div class="math notranslate nohighlight">
\[
\hat {\theta}_{M L E}=\frac {N_{1}}{N} \tag {3.22}\label{3.22}
\]</div>
<p>上式结果很直观，当然也可以按照频率学派的做法，通过最大化似然函数 ( 3.11 ) 的方法得到上式最大似然解。</p>
<p><strong>（2）期望</strong></p>
<p>相比之下，上述模型后验分布的期望为：</p>
<div class="math notranslate nohighlight">
\[
\bar {\theta}=\frac {a+N_{1}}{a+b+N}  \tag {3.23}\label{3.23}
\]</div>
<p>众数和期望之间的差别很重要，这一点将在后文中证明。</p>
<p>现在我们想展示的是：<code class="docutils literal notranslate"><span class="pre">后验分布的期望</span></code>是<code class="docutils literal notranslate"><span class="pre">先验分布的期望</span></code>与<code class="docutils literal notranslate"><span class="pre">最大似然估计</span></code>的凸组合。或者说体现这样的概念：<code class="docutils literal notranslate"><span class="pre">后验分布</span></code>是在<code class="docutils literal notranslate"><span class="pre">先验信念</span></code>与<code class="docutils literal notranslate"><span class="pre">实验数据描述的事实</span> <span class="pre">(极大似然)</span></code> 之间做出妥协。</p>
<p>令 <span class="math notranslate nohighlight">\(\alpha_{0}=a+b\)</span> 为先验分布的强度（等价样本尺寸），它控制着先验分布的强度，令先验的期望为 <span class="math notranslate nohighlight">\(m_{1}=a / \alpha_{0}\)</span>，则后验分布的期望为：</p>
<div class="math notranslate nohighlight">
\[
 \mathbb {E}[\theta \mid \mathcal {D}]=\frac {\alpha_{0} m_{1}+N_{1}}{N+\alpha_{0}}=\frac {\alpha_{0}}{N+\alpha_{0}} m_{1}+\frac {N}{N+\alpha_{0}} \frac {N_{1}}{N}=\lambda m_{1}+ ( 1-\lambda ) \hat {\theta}_{M L E}   \tag {3.24}\label{3.24}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\lambda=\frac{\alpha_0}{N+\alpha_0}\)</span> 为<code class="docutils literal notranslate"><span class="pre">先验分布的等价样本尺寸</span></code>与<code class="docutils literal notranslate"><span class="pre">后验分布的等价样本尺寸</span></code>之比。<span class="math notranslate nohighlight">\(\lambda\)</span> 越小，先验分布强度越弱，后验的期望值越趋近于最大似然解。</p>
<p>上述有关期望的结论对众数也适用，读者可以自行探索如下结论：<code class="docutils literal notranslate"><span class="pre">后验的众数</span></code>是<code class="docutils literal notranslate"><span class="pre">先验的众数</span></code>与<code class="docutils literal notranslate"><span class="pre">最大似然解</span></code>之间的凸组合，<code class="docutils literal notranslate"><span class="pre">后验众数（即最大后验估计值）</span></code>也收敛于<code class="docutils literal notranslate"><span class="pre">最大似然估计解</span></code>。</p>
</div>
<div class="section" id="id14">
<h4>3.3.3.2 后验分布的方差<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
<p>期望和众数都是参数的点估计，但有时知道我们在多大程度上信任该估计值是十分有用的。后验分布的方差就为衡量这种信任程度提供了测度。根据贝塔分布的方差公式（式 2.56），上述后验分布的方差为：</p>
<div class="math notranslate nohighlight">
\[
\operatorname {var}[\theta \mid \mathcal {D}]=\frac {\left ( a+N_{1}\right ) \left ( b+N_{0}\right )}{\left ( a+N_{1}+b+N_{0}\right )^{2}\left ( a+N_{1}+b+N_{0}+1\right )}   \tag {3.25}\label{3.25}
\]</div>
<p>当 <span class="math notranslate nohighlight">\(N \gg a, b\)</span> 时，可以对上面公式进行简化：</p>
<div class="math notranslate nohighlight">
\[
\operatorname {var}[\theta \mid \mathcal {D}] \approx \frac{1}{N} \frac {N_{1}}{N} \frac{N_{0}}{N}=\frac {\hat {\theta} ( 1-\hat {\theta} )}{N}   \tag {3.26}\label{3.26}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\hat {\theta}\)</span> 为最大似然估计值。所以后验分布的标准差为：</p>
<div class="math notranslate nohighlight">
\[
\sigma=\sqrt {\operatorname {var}[\theta \mid \mathcal {D}]} \approx \sqrt {\frac {\hat {\theta} ( 1-\hat {\theta} )}{N}}   \tag {3.27}\label{3.27}
\]</div>
<p>不难发现不确定度（方差）下降的速率为 <span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span>。值得注意的是，当 <span class="math notranslate nohighlight">\(\hat \theta =0.5\)</span> 时，不确定度（方差）最大，而当 <span class="math notranslate nohighlight">\(\hat \theta\)</span> 接近 0 或者 1 时，不确定度（方差）最小。这意味着确定<code class="docutils literal notranslate"><span class="pre">一个硬币是不均匀的</span></code>要比确定<code class="docutils literal notranslate"><span class="pre">一个硬币是均匀的</span> </code>更加容易。</p>
</div>
</div>
<div class="section" id="id15">
<h3>3.3.4 后验预测性分布<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>截止目前，我们将注意力主要集中在对未知参数的推断上。现在将目光转移到对未来可观测数据的预测上。</p>
<p>考虑在服从 <span class="math notranslate nohighlight">\(\operatorname{Beta} ( a,b )\)</span> 的后验分布的情况下，预测下一次抛掷硬币试验中正面朝上的概率。我们有：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
p ( \tilde {x}=1 \mid \mathcal {D} ) &amp;=\int_{0}^{1} p ( x=1 \mid \theta ) p ( \theta \mid \mathcal {D} ) d \theta   \tag {3.28}\label{3.28}\\
&amp;=\int_{0}^{1} \theta \operatorname {Beta} ( \theta \mid a, b ) d \theta=\mathbb {E}[\theta \mid \mathcal {D}]=\frac {a}{a+b}   \tag {3.29}\label{3.29}
\end {align*}
\end{split}\]</div>
<p>所以不难发现，后验预测性分布的期望等价于 ( 在当前情况下 ) 后验分布的期望的点估计，即：<span class="math notranslate nohighlight">\(p ( \tilde {x} \mid \mathcal {D} ) =\operatorname {Ber} ( \tilde {x} \mid \mathbb {E}[\theta \mid \mathcal {D}] )\)</span>。</p>
<div class="section" id="id16">
<h4>3.3.4.1 过拟合与黑天鹅悖论<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h4>
<p>假设使用最大似然估计进行点估计，即 <span class="math notranslate nohighlight">\(p ( \tilde {x} \mid \mathcal {D} ) \approx \operatorname {Ber}\left ( \tilde {x} \mid \hat {\theta}_{M L E}\right )\)</span>。不幸的是，这种近似的方式在样本很少的情况下性能十分糟糕。</p>
<p>举例来说，在连续的 3 次硬币试验中看到 3 次反面朝上，那么根据最大似然估计原则，我们有 <span class="math notranslate nohighlight">\(\hat \theta=0/3=0\)</span>。然而，如果使用此估计值，我们将认为硬币正面朝上是不可能发生的事情。这被称为 <code class="docutils literal notranslate"><span class="pre">零计数问题</span></code>( zero count problem ) 或者 <code class="docutils literal notranslate"><span class="pre">稀疏数据问题</span></code> ( sparse data problem ) 。这种问题在样本数量很小时经常出现。或许有人会认为在大数据时代，这种情况基本不会发生，但一旦我们基于某种准则对数据进行拆分时（例如：某个特定的人已经从事某个特定活动的次数），样本的尺寸将变得很小。此问题在诸如网页个性化推荐任务中特别容易出现。从这个角度来看，贝叶斯方法依然是有用的，哪怕是在大数据时代。</p>
<p>零计数问题与哲学中的 <code class="docutils literal notranslate"><span class="pre">黑天鹅悖论</span></code>( black swan paradox ) 十分相似。</p>
<p>这是基于古代西方人的基本观念：天鹅都是白色的。在当时那种环境下，黑天鹅一般是对不可能发生事件的隐喻。术语 <code class="docutils literal notranslate"><span class="pre">黑天鹅悖论</span></code> 一词最早由著名的科学哲学家 Karl Popper 创造，被用于说明在归纳中出现的问题。（归纳法就是如何基于历史上出现的特定观测值得到关于未来的一般性结论）。</p>
<p>针对上述问题，可以推导出一个简单的贝叶斯解决方案。我们使用一个均匀先验分布，即 <span class="math notranslate nohighlight">\(a=b=1\)</span>，后验期望（均值）的点估计符合 <code class="docutils literal notranslate"><span class="pre">拉普拉斯继承法则</span></code> ( Laplace’s rule of succession ) ：</p>
<div class="math notranslate nohighlight">
\[
p ( \tilde {x}=1 \mid \mathcal {D} ) =\frac {N_{1}+1}{N_{1}+N_{0}+2}   \tag {3.30}\label{3.30}
\]</div>
<p>上述方法展示了一种常规操作：在经验计数上加 1 和归一化后，再进行后验期望（均值）点估计，该技术被称为 <code class="docutils literal notranslate"><span class="pre">加一平滑</span></code>( add-one smoothing。 ) 。值得注意的是加一平滑是针对最大似然解作出的调整，使用最大后验估计参数进行点估计不具备平滑效应，因为最大后验估计参数为 <span class="math notranslate nohighlight">\(\hat {\theta}=\frac {N_{1}+a-1}{N+a+b-2}\)</span>，如果 <span class="math notranslate nohighlight">\(a=b=1\)</span>，它将退化成最大似然估计。</p>
</div>
<div class="section" id="id17">
<h4>3.3.4.2 预测未来多次试验的结果<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h4>
<p>假设我们对未来 <span class="math notranslate nohighlight">\(M\)</span> 次试验中正面朝上的次数 <span class="math notranslate nohighlight">\(\mathbf {x}\)</span> 感兴趣。定义为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
p ( x \mid \mathcal {D}, M ) &amp;=\int_{0}^{1} \operatorname {Bin} ( x \mid \theta, M ) \operatorname {Beta} ( \theta \mid a, b ) d \theta   \tag {3.31}\label{3.31}\\
&amp;=\left ( \begin {array}{c}   
M \\
x
\end {array}\right ) \frac {1}{B ( a, b )} \int_{0}^{1} \theta^{x} ( 1-\theta )^{M-x} \theta^{a-1} ( 1-\theta )^{b-1} d \theta  \tag {3.32}\label{3.32}
\end {align*}
\end{split}\]</div>
<p>不难发现，上式中的积分项就是分布 <span class="math notranslate nohighlight">\(Beta ( a+x, M-x+b )\)</span> 的归一化常数。因此：</p>
<div class="math notranslate nohighlight">
\[
\int_{0}^{1} \theta^{x} ( 1-\theta )^{M-x} \theta^{a-1} ( 1-\theta )^{b-1} d \theta=B ( x+a, M-x+b )  \tag {3.33}\label{3.33}
\]</div>
<p>所以后验预测性分布由下式给出，即所谓的 <code class="docutils literal notranslate"><span class="pre">贝塔-二项分布</span></code>：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
B b ( x \mid a, b, M ) \triangleq\left ( \begin {array}{c}
M \\
x
\end {array}\right ) \frac {B ( x+a, M-x+b )}{B ( a, b )}  \tag {3.34}\label{3.34}
\end{align*}
\end{split}\]</div>
<p>该分布的期望和方差为：</p>
<div class="math notranslate nohighlight">
\[
\mathbb {E}[x]=M \frac {a}{a+b}, \operatorname {var}[x]=\frac {M a b}{( a+b )^{2}} \frac {( a+b+M )}{a+b+1}   \tag {3.35}\label{3.35}
\]</div>
<p>如果 <span class="math notranslate nohighlight">\(M =1\)</span>，则  <span class="math notranslate nohighlight">\(x \in\{0,1\}\)</span> ，不难发现此时期望退化成 <span class="math notranslate nohighlight">\(\mathbb {E}[x \mid \mathcal {D}]=p ( x=1 \mid \mathcal {D} ) =\frac {a}{a+b}\)</span> ，与式 3.29 保持一致。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013051521.webp" /></p>
<p><strong>图 3.7</strong>  ( a ) 看到 <span class="math notranslate nohighlight">\(N_1=3, N_0=17\)</span> 后的后验预测性分布； ( b ) 基于 MAP 的点估计。图形由程序 <code class="docutils literal notranslate"><span class="pre">betaBinomPostPredDemo</span></code> 生成</p>
<p>此过程如图 3.7 ( a ) 所示。我们以 <span class="math notranslate nohighlight">\(\operatorname{Beta} ( 2,2 )\)</span> 作为先验分布，当看到 <span class="math notranslate nohighlight">\(N_1=3\)</span> 和 <span class="math notranslate nohighlight">\(N_0=17\)</span> 时，绘制出后验预测性分布。图 3.7 ( b ) 绘制了基于最大后验概率估计值作出的预测结果。不难发现，通过贝叶斯方法进行的预测具备长尾，其概率质量分布得更广，因此不太倾向于过拟合和黑天鹅悖论。</p>
<p><code class="docutils literal notranslate"><span class="pre">附：关于上述问题的个人解释</span></code></p>
<p>在图 3.7 中分别基于贝叶斯模型平均法对未来 10 次试验中正面朝上的次数的分布 ( 图 ( a )) 和基于最大后验概率估计进行的点估计。不难发现，当我们看到经验计数 <span class="math notranslate nohighlight">\(N_1=3\)</span> 和 <span class="math notranslate nohighlight">\(N_0=17\)</span> 时 ( 看到此数据后，有理由相信此硬币在未来 10 次投掷中，正面朝上的次数较多的概率很低 ) ，基于贝叶斯模型平均法进行的预测尾部具有相对较高的概率 ( 即正面朝上的次数较多的概率相对较高 ) ，而基于最大后验概率估计进行的点估计显然很符合我们看到经验计数后的直观感受，即所谓的过拟合。</p>
</div>
</div>
</div>
<div class="section" id="id18">
<h2>3.4 <strong>狄利克雷-多项分布</strong> 模型<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h2>
<p>在上一章节中，我们讨论了如何推断一个硬币正面朝上的概率。本节，我们将泛化这些结果，去推断一个具有 <span class="math notranslate nohighlight">\(K\)</span> 个面的骰子面 <span class="math notranslate nohighlight">\(k\)</span> 朝上的概率。正如我们在后面将会看到的，此方法被广泛地应用于文本数据，生物序列数据等的分析。</p>
<div class="section" id="id19">
<h3>3.4.1 似然函数<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<p>假设我们抛掷骰子 <span class="math notranslate nohighlight">\(N\)</span> 次，观测到的结果为 <span class="math notranslate nohighlight">\(\mathcal {D}=\left\{x_{1}, \ldots, x_{N}\right\}\)</span> ，其中 <span class="math notranslate nohighlight">\( x_{i} \in\{1, \ldots, K\} . \)</span>。假设所有的数据独立同分布，则似然函数的形式为：</p>
<div class="math notranslate nohighlight">
\[
p ( \mathcal {D} \mid \boldsymbol {\theta} ) =\prod_{k=1}^{K} \theta_{k}^{N_{k}}   \tag {3.36}\label{3.36}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(N_{k}=\sum_{i=1}^{N} \mathbb {I}\left ( y_{i}=k\right )\)</span> 为面 <span class="math notranslate nohighlight">\(k\)</span> 朝上的总次数 ( 这是该模型的充分统计量 ) 。多项分布的似然函数与上述模型的似然函数形式上是一致的 ( 相差一个与参数无关的常数项 ) 。</p>
</div>
<div class="section" id="id20">
<h3>3.4.2 先验分布<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<p>因为参数向量位于一个 <span class="math notranslate nohighlight">\(K\)</span> 维空间中，它是一个概率单纯形 ( 详见 2.5.4 ) 。我们需要一个先验分布，其定义域也是一个概率单纯形。理想情况下，我们还希望这是一个共轭先验。幸运的是，狄利克雷分布 ( 详见 2.5.4 ) 满足这两个标准。所以我们使用如下所示的先验分布：</p>
<div class="math notranslate nohighlight">
\[
\operatorname {Dir} ( \boldsymbol {\theta} \mid \boldsymbol {\alpha} ) =\frac {1}{B ( \boldsymbol {\alpha} )} \prod_{k=1}^{K} \theta_{k}^{\alpha_{k}-1} \mathbb {I}\left ( \mathbf {x} \in S_{K}\right )  \tag {3.37}\label{3.37}
\]</div>
</div>
<div class="section" id="id21">
<h3>3.4.3 后验分布<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<p>将先验分布与似然函数相乘，得到的后验分布同样服从狄利克雷分布：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
p ( \boldsymbol {\theta} \mid \mathcal {D} ) &amp; \propto p ( \mathcal {D} \mid \boldsymbol {\theta} ) p ( \boldsymbol {\theta} )   \tag {3.38}\label{3.38}\\
&amp; \propto \prod_{k=1}^{K} \theta_{k}^{N_{k}} \theta_{k}^{\alpha_{k}-1}=\prod_{k=1}^{K} \theta_{k}^{\alpha_{k}+N_{k}-1}   \tag {3.39}\label{3.39}\\
&amp;=\operatorname {Dir}\left ( \boldsymbol {\theta} \mid \alpha_{1}+N_{1}, \ldots, \alpha_{K}+N_{K}\right )  \tag {3.40}\label{3.40}
\end {align*}
\end{split}\]</div>
<p>我们发现后验分布通过将先验分布中的超参数 ( 伪计数 ) 与经验计数 <span class="math notranslate nohighlight">\(N_k\)</span> 相加即可得到。</p>
<p>我们可以通过微积分的方式计算出后验分布的众数 ( 即最大后验概率估计 ) ，然而，前提是必须满足约束条件 ( 读者可以思考一下为什么我们不需要显示地表达约束 ) 。我们使用 <code class="docutils literal notranslate"><span class="pre">拉格朗日乘子法</span></code>( Lagrange multiplier ) 来解决此问题。含约束的目标函数，或者 <code class="docutils literal notranslate"><span class="pre">拉格朗日算符</span></code> ( Lagrangian ) 的形式为对数似然函数加上对数先验分布和约束条件：</p>
<div class="math notranslate nohighlight">
\[
\ell ( \boldsymbol {\theta}, \lambda ) =\sum_{k} N_{k} \log \theta_{k}+\sum_{k}\left ( \alpha_{k}-1\right ) \log \theta_{k}+\lambda\left ( 1-\sum_{k} \theta_{k}\right ) \tag {3.41}\label{3.41}
\]</div>
<p>为了简化符号书写，我们定义 <span class="math notranslate nohighlight">\( N_{k}^{\prime} \triangleq N_{k}+\alpha_{k}-1 \)</span>。对上式关于参数 <span class="math notranslate nohighlight">\( \lambda \)</span> 求偏导，得到：</p>
<div class="math notranslate nohighlight">
\[
\frac {\partial \ell}{\partial \lambda}=\left ( 1-\sum_{k} \theta_{k}\right ) =0 \tag {3.42}\label{3.42}
\]</div>
<p>关于参数 <span class="math notranslate nohighlight">\(\theta_{k}\)</span> 求偏导，得到：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
\frac {\partial \ell}{\partial \theta_{k}} &amp;=\frac {N_{k}^{\prime}}{\theta_{k}}-\lambda=0  \tag {3.43}\label{3.43}\\
N_{k}^{\prime} &amp;=\lambda \theta_{k} \tag {3.44}\label{3.44}
\end {align*}
\end{split}\]</div>
<p>我们对式 ( 3.44 ) 两边求和，得到：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
\sum_{k} N_{k}^{\prime} &amp;=\lambda \sum_{k} \theta_{k}  \tag {3.45}\label{3.45}\\
N+\alpha_{0}-K &amp;=\lambda \tag {3.46}\label{3.46}
\end {align*}
\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\alpha_{0} \triangleq \sum_{k=1}^{K} \alpha_{k}\)</span> 为先验分布的等价样本尺寸。所以，最大后验概率估计为：</p>
<div class="math notranslate nohighlight">
\[
\hat {\theta}_{k}=\frac {N_{k}+\alpha_{k}-1}{N+\alpha_{0}-K} \tag {3.47}\label{3.47}
\]</div>
<p>该式与式 2.73 保持一致。如果我们使用一个均匀先验分布，即，我们得到最大似然估计：</p>
<div class="math notranslate nohighlight">
\[
 \theta _ { k } = N _ { k } / N \tag {3.48}\label{3.48}
\]</div>
<p>这是面 <span class="math notranslate nohighlight">\(k\)</span> 朝上次数的经验占比。</p>
</div>
<div class="section" id="id22">
<h3>3.4.4 后验预测性分布<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
<p>对于一个单独的 multinoulli 试验 ( 即一次试验有多次结果 ) ，其后验预测性分布为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
p ( X=j \mid \mathcal {D} ) &amp;=\int p ( X=j \mid \boldsymbol {\theta} ) p ( \boldsymbol {\theta} \mid \mathcal {D} ) d \boldsymbol {\theta}  \tag {3.49}\label{3.49}\\
&amp;=\int p\left ( \boldsymbol {X}=j \mid \theta_{j}\right ) \left [\int p\left ( \boldsymbol {\theta}_{-j}, \theta_{j} \mid \mathcal {D}\right ) d \boldsymbol {\theta}_{-j}\right] d \theta_{j}  \tag {3.50}\label{3.50}\\
&amp;=\int \theta_{j} p\left ( \theta_{j} \mid \mathcal {D}\right ) d \theta_{j}=\mathbb {E}\left [\theta_{j} \mid \mathcal {D}\right]=\frac {\alpha_{j}+N_{j}}{\sum_{k}\left ( \alpha_{k}+N_{k}\right )}=\frac {\alpha_{j}+N_{j}}{\alpha_{0}+N} \tag {3.51}\label{3.51}
\end {align*}
\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\boldsymbol {\theta}_{-j}\)</span> 表示除了 <span class="math notranslate nohighlight">\(\theta_{j}\)</span> 以外的所有分量 <span class="math notranslate nohighlight">\(\boldsymbol {\theta}\)</span>。</p>
<p>上述表达式避免了我们在 3.3.4.1 节看到的零计数问题。事实上，这种形式的贝叶斯平滑对于 multinomial 情况更加重要，因为当我们将数据分到多个类别 ( 大于 2 ) 中时，数据的稀疏性更容易出现。</p>
<div class="section" id="id23">
<h4>3.4.4.1 工作案例：使用词袋法的语言模型<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h4>
<p>使用狄利克雷 —— 多项式模型实现贝叶斯平滑的一个应用是 <code class="docutils literal notranslate"><span class="pre">语言模型</span></code>( language modeling ) 。在语言模型中，我们预测在一个序列中下一个可能出现的单词是什么。此处我们将采用一个非常简单的方法，假设第 <span class="math notranslate nohighlight">\(i\)</span> 个单词，下一个单词的出现与否与其他所有单词彼此独立，但同时服从分布。这被称为词袋法模型。给定一系列历史单词，我们该如何预测下一个出现的单词是哪一个呢？</p>
<p>举例来说，假设我们观测到了下面的序列 ( 童谣的一部分 ) ：</p>
<p>Mary had a little lamb, little lamb, little lamb,</p>
<p>Mary had a little lamb, its fleece as white as snow</p>
<p>进一步的，假设我们的语料库由如下单词组成：</p>
<p>Mary lamb little big fleece white black snow rain unk</p>
<p>1 2 3 4 5 6 7 8 9 10</p>
<p>其中 <code class="docutils literal notranslate"><span class="pre">unk</span></code> 表示 unknown ( 未知 ) ，代表所有在语料库中没有出现的单词。为了对童谣的每一行进行编码，我们首先去掉所有的标点符号，去掉那些 <code class="docutils literal notranslate"><span class="pre">停用词</span></code> ( stop
words ) ，比如说 <code class="docutils literal notranslate"><span class="pre">a</span></code> ， <code class="docutils literal notranslate"><span class="pre">as</span></code> ， <code class="docutils literal notranslate"><span class="pre">the</span></code> 等。我们也可以进行 <code class="docutils literal notranslate"><span class="pre">词干提取</span></code>( stemming ) ，意味着将单词转换为它最基本的形式，比如将复数单词后的 <code class="docutils literal notranslate"><span class="pre">s</span></code> 去掉，或者将动词后面的 <code class="docutils literal notranslate"><span class="pre">ing</span></code> 去掉 ( 比如 raining 变成 rain ) 。在上面的例子中，没有单词需要进行词干提取。最后，我们将每个单词替换为它们在语料库中的编号：</p>
<p>1 10 3 2 3 2 3 2</p>
<p>1 10 3 2 10 5 6 8</p>
<p>现在我们忽略单词之间的顺序，计算每个单词出现的频次，形成一个关于单词频次的统计直方图：</p>
<p>编号  1   2   3    4   5    6    7    8   9   10</p>
<hr class="docutils" />
<p>单词  mary  lamb  little  big  fleece  white  black  snow  rain  unk
计数  2   4   4    0   1    1    0    1   0   3</p>
<p>利用 <span class="math notranslate nohighlight">\(N_j\)</span> 表示单词 <span class="math notranslate nohighlight">\(j\)</span> 出现的次数。如果我们使用表示关于参数的先验分布，其后验预测性分布为：</p>
<div class="math notranslate nohighlight">
\[
p ( \bar X = j | D ) = E [ \theta _ { j } | D ] = \frac { a _ { j } + N _ { j }} { \sum _ { j ' } a _ { j' } + N _ { j' }} = \frac { 1 + N _ { j }} { 10 + 17 } \tag {3.52}\label{3.52}
\]</div>
<p>如果我们设置 <span class="math notranslate nohighlight">\(\alpha_{j}=1\)</span>，将会得到：</p>
<div class="math notranslate nohighlight">
\[
p ( \tilde {X}=j \mid D ) = ( 3 / 27,5 / 27,5 / 27,1 / 27,2 / 27,2 / 27,1 / 27,2 / 27,1 / 27,5 / 27 )  \tag {3.53}\label{3.53}
\]</div>
<p>预测模型的众数为 <span class="math notranslate nohighlight">\(X=2\)</span>( <code class="docutils literal notranslate"><span class="pre">lamb</span></code> ) 和 <span class="math notranslate nohighlight">\(X=3\)</span>( <code class="docutils literal notranslate"><span class="pre">little</span></code> ) 。值得注意的是单词 <code class="docutils literal notranslate"><span class="pre">big</span></code> ， <code class="docutils literal notranslate"><span class="pre">black</span></code> 和 <code class="docutils literal notranslate"><span class="pre">rain</span></code> 在历史序列中并没有出现，但模型预测的概率值并非等于 0，说明这些单词在未来还是有可能出现的，只是其概率值比较低。我们将会在后面的内容中看到更复杂的语言模型。</p>
</div>
</div>
</div>
<div class="section" id="id24">
<h2>3.5 朴素贝叶斯分类器<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h2>
<p>本节，我们将讨论如何对一个具有离散特征值的向量进行分类，其中 <span class="math notranslate nohighlight">\(K\)</span> 表示特征可取的数值数量， <span class="math notranslate nohighlight">\(D\)</span> 表示特征的数量。我们将使用一个 <code class="docutils literal notranslate"><span class="pre">生成式模型</span></code> 方法。这就要求我们指定类条件概率分布。最简单的方式是假设在给定类别的前提下特征之间是 <code class="docutils literal notranslate"><span class="pre">条件独立</span></code> ( conditionally independent ) 的。这将允许我们将类条件密度写成一维概率密度的乘积：</p>
<div class="math notranslate nohighlight">
\[
p ( \mathbf {x} \mid y=c, \boldsymbol {\theta} ) =\prod_{j=1}^{D} p\left ( x_{j} \mid y=c, \boldsymbol {\theta}_{j c}\right )  \tag {3.54}\label{3.54}
\]</div>
<p>相应的模型被称为 <code class="docutils literal notranslate"><span class="pre">朴素贝叶斯分类器</span></code> ( naïve Bayes classifier, NBC ) 。</p>
<p>我们称此模型是 <code class="docutils literal notranslate"><span class="pre">朴素</span></code> 的，是因为在实际使用中我们并不要求特征之间是彼此独立的，哪怕是基于类别标签的条件独立。然而，尽管朴素贝叶斯假设并不正确，但这种分类器的工作效果往往很好。一个理由是因为此模型十分简单，其参数数量的数量级只有 <span class="math notranslate nohighlight">\(O ( CD )\)</span>( <span class="math notranslate nohighlight">\(C\)</span> 为类别数量， <span class="math notranslate nohighlight">\(D\)</span> 为特征数量 ) ，因此它不太容易过拟合。</p>
<p>类条件概率密度的形式与每个特征的类型有直接关系。我们给出一些不同的概率形式：</p>
<ul class="simple">
<li><p>对于实数域的特征，我们可以使用高斯分布：<span class="math notranslate nohighlight">\(p ( \mathbf {x} \mid y=c, \boldsymbol {\theta} ) = \prod_{j=1}^{D} \mathcal {N}\left ( x_{j} \mid \mu_{j c}, \sigma_{j c}^{2}\right )\)</span>，其中 <span class="math notranslate nohighlight">\(\mu_{j c}\)</span> 表示特征 <span class="math notranslate nohighlight">\(j\)</span> 在类别  <span class="math notranslate nohighlight">\(c\)</span> 中的期望， <span class="math notranslate nohighlight">\(\sigma_{j c}^{2}\)</span> 表示相应的方差。</p></li>
<li><p>对于二值特征 <span class="math notranslate nohighlight">\(x_{j} \in\{0,1\}\)</span> ，我们可以使用伯努利分布：<span class="math notranslate nohighlight">\(p ( \mathbf {x} \mid y=  c, \boldsymbol {\theta} ) =\prod_{j=1}^{D} \operatorname {Ber}\left ( x_{j} \mid \mu_{j c}\right )\)</span>，其中 <span class="math notranslate nohighlight">\(\mu_{j c}\)</span> 表示在类  <span class="math notranslate nohighlight">\(c\)</span> 中特征 <span class="math notranslate nohighlight">\(j\)</span> 发生的概率。这通常又被称为 <code class="docutils literal notranslate"><span class="pre">多变量伯努利朴素贝叶斯</span></code> ( multivariate Bernoulli naïve Bayes ) 模型。我们将会在下文给出一个应用案例。</p></li>
<li><p>对于类别特征 <span class="math notranslate nohighlight">\(x_{j} \in\{1, \ldots, K\}\)</span>，我们可以使用多项分布：<span class="math notranslate nohighlight">\(p ( \mathbf {x} \mid y=c, \boldsymbol {\theta} ) =\prod_{j=1}^{D} \operatorname {Cat}\left ( x_{j} \mid \boldsymbol {\mu}_{j c}\right )\)</span>，其中 <span class="math notranslate nohighlight">\(\boldsymbol {\mu}_{j c}\)</span> 表示在类 <span class="math notranslate nohighlight">\(c\)</span> 中 <span class="math notranslate nohighlight">\(x_j\)</span> 可能的 <span class="math notranslate nohighlight">\(K\)</span> 个取值的统计直方图 ( 即取不同值的概率 ) 。</p></li>
</ul>
<p>显然，我们可以处理不同类型的特征，或者使用不同的概率分布假设。当然，我们也可以很容易将不同类型的概率分布混合起来使用。</p>
<div class="section" id="nbc">
<h3>3.5.1 NBC 模型训练<a class="headerlink" href="#nbc" title="Permalink to this headline">¶</a></h3>
<p>现在我们讨论如何训练朴素贝叶斯分类器。所谓训练一个模型，往往是意味着求取关于模型中待定参数的 MAP 或者 ML 估计。然而，我们也会讨论如何求解关于参数的整个后验分布。</p>
<div class="section" id="nbc-mle">
<h4>3.5.1.1 NBC 的 MLE<a class="headerlink" href="#nbc-mle" title="Permalink to this headline">¶</a></h4>
<p>对于单个数据而言，其概率值为：</p>
<div class="math notranslate nohighlight">
\[
p\left ( \mathbf {x}_{i}, y_{i} \mid \boldsymbol {\theta}\right ) =p\left ( y_{i} \mid \boldsymbol {\pi}\right ) \prod_{j} p\left ( x_{i j} \mid \boldsymbol {\theta}_{j}\right ) =\prod_{c} \pi_{c}^{\mathrm {I}\left ( y_{i}=c\right )} \prod_{j} \prod_{c} p\left ( x_{i j} \mid \boldsymbol {\theta}_{j c}\right )^{\mathrm {I}\left ( y_{i}=c\right )} \tag {3.55}\label{3.55}
\]</div>
<p>对于 <span class="math notranslate nohighlight">\(N\)</span> 个独立同分布数据构成的数据集，我们有：</p>
<p>对上式取对数，得到 ( <code class="docutils literal notranslate"><span class="pre">附</span></code>：<code class="docutils literal notranslate"><span class="pre">关于式</span> <span class="pre">3.56</span> <span class="pre">的最后一步推导，读者可以尝试通过先展开再合并的方式完成。</span></code> ) ：</p>
<div class="math notranslate nohighlight">
\[
\log p ( \mathcal {D} \mid \boldsymbol {\theta} ) =\sum_{c=1}^{C} N_{c} \log \pi_{c}+\sum_{j=1}^{D} \sum_{c=1}^{C} \sum_{i: y_{i}=c} \log p\left ( x_{i j} \mid \boldsymbol {\theta}_{j c}\right ) \tag {3.56}\label{3.56}
\]</div>
<p>不难发现对数似然函数分解成多个项，其中一项包含参数 <span class="math notranslate nohighlight">\(\pi\)</span>，另外包含 <span class="math notranslate nohighlight">\(DC\)</span> 个包含参数 <span class="math notranslate nohighlight">\(\boldsymbol {\theta}_{j c}\)</span> 的项。因此我们可以对不同的参数进行单独的优化。</p>
<p>根据式 3.48，不难发现类的先验概率的 MLE 为：</p>
<div class="math notranslate nohighlight">
\[
\hat {\pi}_{c}=\frac {N_{c}}{N} \tag {3.57}\label{3.57}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(N_{c} \triangleq \sum_{i} \mathbb {I}\left ( y_{i}=c\right )\)</span> 为所有样本中属于类 <span class="math notranslate nohighlight">\(c\)</span> 的数量。</p>
<p>参数的 MLE 与特征服从什么样的类条件概率分布有关。为了简单起见，不妨假设所有特征都是二值特征，即 <span class="math notranslate nohighlight">\(x_{j} \mid y=c \sim \operatorname {Ber}\left ( \theta_{j c}\right )\)</span> 。在这种情况下，MLE 为：</p>
<div class="math notranslate nohighlight">
\[
\hat {\theta}_{j c}=\frac {N_{j c}}{N_{c}} \tag {3.58}\label{3.58}
\]</div>
<p>其中为在类 <span class="math notranslate nohighlight">\(c\)</span> 中特征 <span class="math notranslate nohighlight">\(j\)</span> 出现的样本的数量。</p>
<p>实现上述模型的训练十分简单：算法 3.1 给出了程序的伪代码。此算法的时间复杂度为 <span class="math notranslate nohighlight">\(O ( ND )\)</span> 。这种方法也可以泛化到那些具有混合类型的特征上。这种简单性也是该方法被广泛使用的理由之一。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061014430310.webp" /></p>
<p>图 3.8 给出了一个例子，其中数据中拥有 2 个类别，600 个二值特征，分别表示在词袋法模型中某个单词是否出现。图形对在两个类别的情况下向量进行了可视化展示。在索引值为 107 的地方出现的大的峰值对应单词 <code class="docutils literal notranslate"><span class="pre">subject</span></code> ，它在两个类别中出现的概率都为 1。 ( 我们将在 3.5.4 节讨论如何过滤掉这些不提供有价值信息的特征。 ) ( 注：所谓没有价值就是指此单词在两个类中是肯定出现的，那么它对我们的分类任务就起不到作用了 )</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061013052531.webp" /></p>
<p><strong>图 3.8</strong> 类条件密度，两个类别分别对应 <code class="docutils literal notranslate"><span class="pre">X</span> <span class="pre">windows</span></code> 和 <code class="docutils literal notranslate"><span class="pre">MS</span> <span class="pre">windows</span></code> 。图形由程序 <code class="docutils literal notranslate"><span class="pre">naiveBayesBowDemo</span></code> 生成。</p>
</div>
<div class="section" id="id25">
<h4>3.5.1.2 贝叶斯方法下的朴素贝叶斯<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h4>
<p>最大似然解的麻烦之处在于它容易过拟合。举例来说，考虑图 3.8 中所展示的例子：对应单词 <code class="docutils literal notranslate"><span class="pre">subject</span></code> 的特征 ( 称其为特征 <span class="math notranslate nohighlight">\(j\)</span> ) 在两个类中都出现了，所以我们估计 <span class="math notranslate nohighlight">\( \hat {\theta}_{j c}=1 \)</span>。如果我们遇到一封新的邮件但其中并没有此单词，那么将会发生什么情况呢？我们的算法将会失效，因为我们将发现对于任何一个类 <span class="math notranslate nohighlight">\( p ( y=c \mid \mathbf {x}, \hat {\boldsymbol {\theta}} ) =0 \)</span>，即新的邮件不属于任何一个类。这是我们在章节 3.3.4.1 中所提及的黑天鹅悖论的另一种体现。</p>
<p><code class="docutils literal notranslate"><span class="pre">附：为了方便读者直观的了解上述内容，我们给出了如下的案例：</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">label</span></code>  <code class="docutils literal notranslate"><span class="pre">Word_1</span></code>  <code class="docutils literal notranslate"><span class="pre">Word_2</span></code>  <code class="docutils literal notranslate"><span class="pre">...</span></code>  <code class="docutils literal notranslate"><span class="pre">Subject</span> <span class="pre">(</span> <span class="pre">$j$)</span></code>  <code class="docutils literal notranslate"><span class="pre">Word_n-1</span></code>  <code class="docutils literal notranslate"><span class="pre">Word_n</span></code></p>
<hr class="docutils" />
<p>C_1    1      0      …    1          1       1
0      1      …    1          0       0
C~2~    0      0      …    1          0       1
1      1      …    1          0       0
0      1      …    1          1       0</p>
<p>表格中为模型的训练样本，共有两类数据 C_1 和 C~2~，每一类文本中单词 <code class="docutils literal notranslate"><span class="pre">subject</span></code> 始终存在，那么根据 3.5.1.1 介绍的模型训练方法，我们有，那么当我们遇到一个新的文本 <code class="docutils literal notranslate"><span class="pre">x</span></code>，其中没有单词 <code class="docutils literal notranslate"><span class="pre">subject</span></code> 时，我们将遇到如下情况：</p>
<p>同样我们也可以得到。</p>
<p>针对过拟合的一个简单解决方案是使用贝叶斯方法。为简单起见，我们使用一个因子分解形式的先验分布：</p>
<div class="math notranslate nohighlight">
\[
p ( \boldsymbol {\theta} ) =p ( \boldsymbol {\pi} ) \prod_{j=1}^{D} \prod_{c=1}^{C} p\left ( \theta_{j c}\right ) \tag {3.59}\label{3.59}
\]</div>
<p>针对参数 <span class="math notranslate nohighlight">\(\boldsymbol {\pi}\)</span> 我们使用先验分布 <span class="math notranslate nohighlight">\(\operatorname {Dir} ( \boldsymbol {\alpha} )\)</span> ，针对每个参数 <span class="math notranslate nohighlight">\(\theta_{j c}\)</span>，我们使用先验分布 <span class="math notranslate nohighlight">\(\operatorname {Beta}\left ( \beta_{0}, \beta_{1}\right )\)</span> 。一般情况下，我们取 <span class="math notranslate nohighlight">\(\alpha=1\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol {\beta}=1\)</span>，此时对应于前文所提及的加一平滑或者拉普拉斯平滑。</p>
<p>将式 3.56 的似然函数与式 3.59 的先验分布组合，得到因子分解形式的后验分布：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
p ( \boldsymbol {\theta} \mid \mathcal {D} ) &amp;=p ( \boldsymbol {\pi} \mid \mathcal {D} ) \prod_{j=1}^{D} \prod_{c=}^{C} p\left ( \theta_{j c} \mid \mathcal {D}\right )  \tag {3.60}\label{3.60}\\
p ( \boldsymbol {\pi} \mid \mathcal {D} ) &amp;=\operatorname {Dir}\left ( N_{1}+\alpha_{1} \ldots, N_{C}+\alpha_{C}\right )   \tag {3.61}\label{3.61}\\
p\left ( \theta_{j c} \mid \mathcal {D}\right ) &amp;=\operatorname {Beta}\left ( \left ( N_{c}-N_{j c}\right ) +\beta_{0}, N_{j c}+\beta_{1}\right )  \tag {3.62}\label{3.62}
\end {align*}
\end{split}\]</div>
<p>换句话说，为了计算后验分布，我们只需要利用似然函数中的经验计数去更新先验计数。基于算法 3.1 进行必要的调整以使用当前的方法也是很直接的。</p>
</div>
</div>
<div class="section" id="id26">
<h3>3.5.2 使用模型进行预测<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<p>在测试阶段，我们的目的是计算</p>
<div class="math notranslate nohighlight">
\[
p ( y=c \mid \mathbf {x}, \mathcal {D} ) \quad \propto \quad p ( y=c \mid \mathcal {D} ) \prod_{j=1}^{D} p\left ( x_{j} \mid y=c, \mathcal {D}\right )  \tag {3.63}\label{3.63}
\]</div>
<p>在贝叶斯模型平均的方法中，我们需要对所有未知参数进行积分：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
p ( y=c \mid \mathbf {x}, \mathcal {D} ) \propto &amp; \propto\left [\int \operatorname {Cat} ( y=c \mid \boldsymbol {\pi} ) p ( \boldsymbol {\pi} \mid \mathcal {D} ) d \boldsymbol {\pi}\right]   \tag {3.64}\label{3.64}\\
&amp; \prod_{j=1}^{D}\left [\int \operatorname {Ber}\left ( x_{j} \mid y=c, \theta_{j c}\right ) p\left ( \boldsymbol {\theta}_{j c} \mid \mathcal {D}\right ) \right]  \tag {3.65}\label{3.65}
\end {align*}
\end{split}\]</div>
<p>幸运的是，计算上式是比较简单的，尤其是当后验分布服从狄利克雷分布。根据式 3.51，我们知道后验预测性分布的加权平均可以简单的基于后验分布的期望值进行点估计。所以：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
p ( y=c \mid \mathbf {x}, \mathcal {D} ) &amp; \propto \bar {\pi}_{c} \prod_{j=1}^{D}\left ( \bar {\theta}_{j c}\right )^{\mathbb {I}\left ( x_{j}=1\right )}\left ( 1-\bar {\theta}_{j c}\right )^{\mathbb {I}\left ( x_{j}=0\right )}   \tag {3.66}\label{3.66}\\
\bar {\theta}_{j k} &amp;=\frac {N_{j c}+\beta_{1}}{N_{c}+\beta_{0}+\beta_{1}}   \tag {3.67}\label{3.67}\\
\bar {\pi}_{c} &amp;=\frac {N_{c}+\alpha_{c}}{N+\alpha_{0}}  \tag {3.68}\label{3.68}
\end {align*}
\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\alpha_{0}=\sum_{c} \alpha_{c}\)</span>。</p>
<p>如果我们基于一个单独的点作为对后验分布的估计，即 <span class="math notranslate nohighlight">\(p ( \boldsymbol {\theta} \mid \mathcal {D} ) \approx \delta_{\hat {\boldsymbol {\theta}}} ( \boldsymbol {\theta} )\)</span>，其中 <span class="math notranslate nohighlight">\(\hat {\boldsymbol {\theta}}\)</span> 可能是 ML 或者 MAP 估计值，则后验预测性分布就可以直接基于这些参数进行计算，从而形成一个几乎相同的准则：</p>
<div class="math notranslate nohighlight">
\[
p ( y=c \mid \mathbf {x}, \mathcal {D} ) \propto \hat {\pi}_{c} \prod_{j=1}^{D}\left ( \hat {\theta}_{j c}\right )^{\mathbb {I}\left ( x_{j}=1\right )}\left ( 1-\hat {\theta}_{j c}\right )^{\mathbb {I}\left ( x_{j}=0\right )}  \tag {3.69}\label{3.69}
\]</div>
<p>区别在于我们只是将后验分布的期望 <span class="math notranslate nohighlight">\(\bar \theta\)</span> 替换成后验分布的众数 <span class="math notranslate nohighlight">\(\hat \theta\)</span> ( 即 MAP ) 或者 MLE
。然而，这一点小小的区别在实际过程中却十分重要，因为利用后验分布的期望进行预测所导致的过拟合更小 ( 见 3.3.4.1 ) 。</p>
</div>
<div class="section" id="log-sum-exp">
<h3>3.5.3 log-sum-exp 技巧<a class="headerlink" href="#log-sum-exp" title="Permalink to this headline">¶</a></h3>
<p>现在我们讨论一个重要的实践细节，此细节在任何一种生成式分类器中都会遇到。我们可以使用式 2.11 计算样本所属类别的后验分布，前提是使用合适的类条件密度 ( 比如点估计 ) 。不幸的是，公式 2.11 在实现过程中可能会因为 <code class="docutils literal notranslate"><span class="pre">数值下溢</span></code> ( numerical
underflow ) 而失败。问题在于通常情况下是非常小的数，尤其当 <span class="math notranslate nohighlight">\(\mathrm {x}\)</span> 是一个高维向量时，因为我们要求 <span class="math notranslate nohighlight">\(\sum_{\mathbf {x}} p ( \mathbf {x} \mid y ) =1\)</span>，所以我们看到任何一个特定的高维向量的概率 <span class="math notranslate nohighlight">\(p ( \mathbf {x} \mid y=c )\)</span> 是很小的。该问题的解决方案是当我们使用贝叶斯公式时对上式取对数：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin {align*}
\log p ( y=c \mid \mathbf {x} ) =b_{c}-\log \left [\sum_{c^{\prime}=1}^{C} e^{b_{c^{\prime}}}\right]   \tag {3.70}\label{3.70}\\
b_{c} \triangleq \log p ( \mathbf {x} \mid y=c ) +\log p ( y=c )  \tag {3.71}\label{3.71}
\end {align*}
\end{split}\]</div>
<p>然而，上式要求我们计算下式：</p>
<div class="math notranslate nohighlight">
\[
\log \left [\sum_{c^{\prime}} e^{b_{c^{\prime}}}\right]=\log \sum_{c^{\prime}} p\left ( y=c^{\prime}, \mathbf {x}\right ) =\log p ( \mathbf {x} )  \tag {3.72}\label{3.72}
\]</div>
<p>可是我们并不能对对数进行求和 ( 即 ) 。幸运的是，我们可以将真数 ( <span class="math notranslate nohighlight">\(logN\)</span> 中 <span class="math notranslate nohighlight">\(N\)</span> 为真数 ) 中的最大项提取出来。比如说：</p>
<div class="math notranslate nohighlight">
\[
\log \left ( e^{-120}+e^{-121}\right ) =\log \left ( e^{-120}\left ( e^{0}+e^{-1}\right ) \right ) =\log \left ( e^{0}+e^{-1}\right ) -120  \tag {3.73}\label{3.73}
\]</div>
<p>一般情况下，我们有：</p>
<div class="math notranslate nohighlight">
\[
\log \sum_{c} e^{b_{c}}=\log \left [\left ( \sum_{c} e^{b_{c}-B}\right ) e^{B}\right]=\left [\log \left ( \sum_{c} e^{b_{c}-B}\right ) \right]+B  \tag {3.74}\label{3.74}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(B=\max _{c} b_{c}\)</span> 。这被称为 log-sum-exp 技巧，被广泛使用。</p>
<p>该技巧在算法 3.2 中被使用，该算法给出了利用 NBC 计算 <span class="math notranslate nohighlight">\(p\left ( y_{i} \mid \mathbf {x}_{i}, \hat {\boldsymbol {\theta}}\right )\)</span> 的伪代码。值得注意的是，如果我们只是为了计算 <span class="math notranslate nohighlight">\(\hat {y}_{i}\)</span>，我们并不需要使用这种技巧。因为我们只需要最大化非归一化项 <span class="math notranslate nohighlight">\(\log p\left ( y_{i}=c\right ) +\log p\left ( \mathbf {x}_{i} \mid y=c\right )\)</span>。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202106101458437e.webp" /></p>
</div>
<div class="section" id="id27">
<h3>3.5.4 使用互信息进行特征选择<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<p>考虑到 NBC 模型对潜在的很多特征的联合概率进行建模，它可能遇到过拟合的问题。除此以外，它的时间复杂度为 <span class="math notranslate nohighlight">\(O ( ND )\)</span> ，对于某些应用来说可能是比较高的。</p>
<p>一种解决上述两个问题的常见方法是进行 <code class="docutils literal notranslate"><span class="pre">特征选择</span></code>( feature selection ) ，将那些对分类问题帮助并不大的无关特征去除。特征选择的最简单方法是对每个特征的相关性进行单独的评估，然后选择前 <span class="math notranslate nohighlight">\(K\)</span> 个相关度最高的特征，其中 <span class="math notranslate nohighlight">\(K\)</span> 的选择是基于对精度和复杂度的一种权衡。这种方式被称为变量 <code class="docutils literal notranslate"><span class="pre">排名</span></code>( ranking ) ，<code class="docutils literal notranslate"><span class="pre">过滤</span></code> ( filtering ) 或者 <code class="docutils literal notranslate"><span class="pre">筛选</span></code> ( screening ) 。</p>
<p>一种衡量相关性的方式是使用特征 <span class="math notranslate nohighlight">\(X_j\)</span> 和类标签 <span class="math notranslate nohighlight">\(Y\)</span> 之间的互信息：</p>
<div class="math notranslate nohighlight">
\[
I ( X, Y ) =\sum_{x_{j}} \sum_{y} p\left ( x_{j}, y\right ) \log \frac {p\left ( x_{j}, y\right )}{p\left ( x_{j}\right ) p ( y )}  \tag {3.75}\label{3.75}
\]</div>
<p>互信息可以看作是由于观测到了特征 <span class="math notranslate nohighlight">\(j\)</span> 而导致的标签分布的熵的减少。如果特征是二值的，很容易得到互信息的形式为 ( <code class="docutils literal notranslate"><span class="pre">读者可以证明下式</span></code> ) ：</p>
<div class="math notranslate nohighlight">
\[
I_{j}=\sum_{c}\left [\theta_{j c} \pi_{c} \log \frac {\theta_{j c}}{\theta_{j}}+\left ( 1-\theta_{j c}\right ) \pi_{c} \log \frac {1-\theta_{j c}}{1-\theta_{j}}\right]  \tag {3.76}\label{3.76}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\pi_{c}=p ( y=c ) , \theta_{j c}=p\left ( x_{j}=1 \mid y=c\right )\)</span> 且 <span class="math notranslate nohighlight">\(\theta_{j}=p\left ( x_{j}=1\right ) =\sum_{c} \pi_{c} \theta_{j c}\)</span>，所有这些量都可以在我们训练 NBC 模型过程中顺便得到。</p>
<p>表 3.1 展示了将上述方法应用在图 3.8 所示的二值词袋法数据集的结果，不难发现，拥有最高互信息的特征与那些发生概率最高的特征相比，更具备可判别性。比方说，出现频率最高的单词 <code class="docutils literal notranslate"><span class="pre">subject</span></code> 在两个类别中都有出现，它之所以总是出现是因为此数据集是一个新闻类的数据，而每个新闻都有一个主题 ( <code class="docutils literal notranslate"><span class="pre">subject</span></code> ) 。但显然此单词不具备可判别性。与类别之间拥有最高互信息的单词分别是 <code class="docutils literal notranslate"><span class="pre">windows</span></code> ， <code class="docutils literal notranslate"><span class="pre">microsoft</span></code> ， <code class="docutils literal notranslate"><span class="pre">dos</span></code> 和 <code class="docutils literal notranslate"><span class="pre">motif</span></code> ，这是在情理之中的，因为实际数据中的两个类别分别对应 Microsoft
Windows 和 X Windows。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021061015012730.webp" /></p>
<p><strong>表 3.1</strong> 在类 1 ( X windows ) 和类 2 ( MS windows ) 中最有可能出现的 5 个单词。以及与类标签的互信息最高的 5 个单词。表格由程序 <code class="docutils literal notranslate"><span class="pre">naiveBayesBowDemo</span></code> 生成。</p>
</div>
<div class="section" id="id28">
<h3>3.5.5 使用词袋法对文本进行分类 ( <code class="docutils literal notranslate"><span class="pre">注：本节更多技术细节请读者参考其他文献</span></code> )<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">文本分类</span></code>( Document classification ) 是指将文本分类到不同的类别中。一种简单的方式是将每个文本表示为二值向量，向量中的分量记录着每个单词在文本中是否出现，所以 <span class="math notranslate nohighlight">\(x_{ij} =1\)</span> 的充要条件是单词 <span class="math notranslate nohighlight">\(j\)</span> 在文本 <span class="math notranslate nohighlight">\(i\)</span> 中出现，否则 <span class="math notranslate nohighlight">\(x_{ij} =0\)</span>。然后我们可以使用下面的类条件密度：</p>
<div class="math notranslate nohighlight">
\[
p\left ( \mathbf {x}_{i} \mid y_{i}=c, \boldsymbol {\theta}\right ) =\prod_{j=1}^{D} \operatorname {Ber}\left ( x_{i j} \mid \theta_{j c}\right ) =\prod_{j=1}^{D} \theta_{j c}^{\mathbb {I}\left ( x_{i j}\right )}\left ( 1-\theta_{j c}\right )^{\mathrm {I}\left ( 1-x_{i j}\right )}  \tag {3.77}\label{3.77}
\]</div>
<p>上式被称为 <code class="docutils literal notranslate"><span class="pre">伯努利乘积模型</span></code>( Bernoulli product model ) 或者叫 <code class="docutils literal notranslate"><span class="pre">二值独立模型</span></code> ( binary independence model ) 。</p>
<p>然而，上述模型忽略了每个单词在文本中出现的次数。一个更加精确的方式是将每个单词在文本中出现的次数进行考虑。特别的，令 <span class="math notranslate nohighlight">\(\mathbf {x}_{i}\)</span> 为文本 <span class="math notranslate nohighlight">\(i\)</span> 的表示向量，其分量代表每个单词在文本中出现的次数，所以 <span class="math notranslate nohighlight">\(x_{i j} \in\left\{0,1, \ldots, N_{i}\right\}\)</span>，其中 <span class="math notranslate nohighlight">\(N_i\)</span> 表示文本 <span class="math notranslate nohighlight">\(i\)</span> 中的单词数量 ( 所以 <span class="math notranslate nohighlight">\(\sum_{j=1}^{D} x_{i j}=N_{i}\)</span> ) 。对于类条件密度，我们可以使用多项分布：</p>
<div class="math notranslate nohighlight">
\[
p\left ( \mathbf {x}_{i} \mid y_{i}=c, \boldsymbol {\theta}\right ) =\operatorname {Mu}\left ( \mathbf {x}_{i} \mid N_{i}, \boldsymbol {\theta}_{c}\right ) =\frac {N_{i} !}{\prod_{j=1}^{D} x_{i j} !} \prod_{j=1}^{D} \theta_{j c}^{x_{i j}}   \tag {3.78}\label{3.78}
\]</div>
<p>上式中我们含蓄地表达了文本 <span class="math notranslate nohighlight">\(i\)</span> 的长度 <span class="math notranslate nohighlight">\(N_i\)</span> 与类别无关。其中 <span class="math notranslate nohighlight">\(\theta_{j c}\)</span> 表示在类 <span class="math notranslate nohighlight">\(c\)</span> 文档中单词 <span class="math notranslate nohighlight">\(j\)</span> 出现的概率；这些参数满足约束 <span class="math notranslate nohighlight">\(\sum_{j=1}^{D} \theta_{j c}=1\)</span> 。</p>
<p>尽管多项式分类器很容易训练并且在测试时也很简单，但对于文本分类问题效果并不是很好。原因之一在于它并没有考虑单词使用过程中的 <code class="docutils literal notranslate"><span class="pre">突发特性</span></code> ( burstiness ) 。所谓突发特性是指：大部分单词在任何给定的文本 ( 比如训练样本 ) 中从未出现，但是一旦它们在新的文本中出现过一次，它们很有可能会出现多次。</p>
<p>多项式模型不能适应上述的突发性现象。为了说明原因，注意式 3.78 具备 ( 此处的 <span class="math notranslate nohighlight">\(N_{ij}\)</span> 相当于式中的 <span class="math notranslate nohighlight">\(x_{ij}\)</span> ) 的形式，因为对于那些罕见单词 ( 如前文所述，在训练样本中这些单词很少出现，导致 ) ，如果基于此参数估计值，那么我们将会相信在类 <span class="math notranslate nohighlight">\(C\)</span> 中不可能出现很多罕见单词 ( 这与前文中的突发特性相悖 ) 。对于那些出现频次高的单词 ( 较大 ) ，这种衰减的速率不会太快。为了直观上明白这里面的原因，注意那些出现频次特别高的单词往往是那些功能单词，比如 <code class="docutils literal notranslate"><span class="pre">and</span></code> ， <code class="docutils literal notranslate"><span class="pre">the</span></code> ，和 <code class="docutils literal notranslate"><span class="pre">but</span></code> 等等，这些单词与文本的类别没有关系。单词 <code class="docutils literal notranslate"><span class="pre">and</span></code> 出现的概率基本上保持不变，无论它之前出现的概率如何，所以独立性假设对于这些常用单词来说更加合理。然而，因为那些罕见单词对于我们的分类目的往往影响更大，所以需要我们更加小心的对待。</p>
<p>各种特别的启发式方法已经应用在多项式文本分类器中以提高性能。接下来，我们介绍一种类条件概率密度，它的性能与那些特别的方式相近，然而更具备概率性解释。</p>
<p>假设我们简单地将多项式类条件概率密度替换为 <code class="docutils literal notranslate"><span class="pre">狄利克雷复合多项式</span></code> ( Dirichlet Compound Multinomial, DCM ) 密度，定义如下：</p>
<div class="math notranslate nohighlight">
\[
p\left ( \mathbf {x}_{i} \mid y_{i}=c, \boldsymbol {\alpha}\right ) =\int \operatorname {Mu}\left ( \mathbf {x}_{i} \mid N_{i}, \boldsymbol {\theta}_{c}\right ) \operatorname {Dir}\left ( \boldsymbol {\theta}_{c} \mid \boldsymbol {\alpha}_{c}\right ) d \boldsymbol {\theta}_{c}=\frac {N_{i} !}{\prod_{j=1}^{D} x_{i j} !} \frac {B\left ( \mathbf {x}_{i}+\boldsymbol {\alpha}_{c}\right )}{B\left ( \boldsymbol {\alpha}_{c}\right )}  \tag {3.79}\label{3.79}
\]</div>
<p>( 上式由式 5.24 导出。 ) 令人惊讶的是此简单的变化就可以使模型适应突发特性现象。直观的解释是：当我们发现单词 <span class="math notranslate nohighlight">\(j\)</span> 出现过一次时，的后验计数将被更新，使得单词 <span class="math notranslate nohighlight">\(j\)</span> 再次出现的可能性增加。相反，如果固定，每个单词的出现是独立的。多项式模型对应于从一个容器中拿出一个球，记录下它的颜色后再放回。相反，DCM 模型对应于拿出一个球，记录下颜色，放回的同时再复制一个相同的球，这被称为 <code class="docutils literal notranslate"><span class="pre">波利亚坛子模型</span> <span class="pre">(</span> <span class="pre">Polya's</span> <span class="pre">urn</span> <span class="pre">scheme</span> <span class="pre">)</span></code> 是一个著名的概率模型：假设坛子中装有 <span class="math notranslate nohighlight">\(N\)</span> 个球，其中有 <span class="math notranslate nohighlight">\(N_1\)</span> 个黑球、 <span class="math notranslate nohighlight">\(N_2\)</span> 个白球。任意取出一个，记下其颜色，并且在下次取球之前把该球连同另外 <span class="math notranslate nohighlight">\(r\)</span> 个与它同色的球一起放人坛中，再从坛中取出一个球，如此以往 。</p>
<p>使用 DCM 作为类条件密度得到的性能比多项式好很多，并且与那些先进的方法相比，其性能并不逊色。DCM 模型的唯一缺点是其模型的训练过程更加复杂。</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./MLAPP_BOOK"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="02.Probability.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">02 概率论</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="04.GaussianModels.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">04 高斯模型</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Kevin Murphy<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>