
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12 隐变量线性模型 &#8212; 机器学习：概率视角</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="13 稀疏线性模型" href="13.SparseLinearModel.html" />
    <link rel="prev" title="11 混合模型与 EM 算法" href="11.MixtureModelandEMAlgorithm.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">机器学习：概率视角</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning ：A Probabilistic Perspective
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../preface.html">
   前言
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01.Introduction.html">
   01 引言
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02.Probability.html">
   02 概率论
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03.GenerativeModelsForDiscreteData.html">
   03 离散数据的生成式模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04.GaussianModels.html">
   04 高斯模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05.BayesianStatistics.html">
   05 贝叶斯学派统计思想
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06.FrequentistStatistics.html">
   06 频率学派统计思想
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07.LinearRegression.html">
   07 线性回归
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08.LogisticRegression.html">
   08 逻辑回归
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09.GeneralizedLinearModelsAndTheExponentialFamily.html">
   09 广义线性模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10.DirectedGraphicalModels%20%28BayesNets%29.html">
   10 有向图模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11.MixtureModelandEMAlgorithm.html">
   11 混合模型与 EM 算法
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   12 隐变量线性模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13.SparseLinearModel.html">
   13 稀疏线性模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14.KernelMethod.html">
   14 核方法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15.GaussianProcesses.html">
   15 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16.AdaptiveBasisFunctionModel.html">
   16 自适应基函数模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="17.MarkovAndHiddenMarkovModel.html">
   17 马尔科夫与隐马尔科夫模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="18.StateSpaceModels.html">
   18 状态空间模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="19.UndirectedGraphicalModels%28MarkovRandomFields%29.html">
   19 无向图模型（马尔科夫随机场）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="20.ExactInferenceForGraphicalModels.html">
   20 概率图的精确推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="21.VariationalInference.html">
   21 变分推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="22.MoreVariationalInference.html">
   22 更多变分推断方法
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="23.MonteCarloInference.html">
   23 蒙特卡洛推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="24.MarkovChainMonteCarlo%28MCMC%29Inference.html">
   24 马尔科夫链蒙特卡洛 MCMC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="25.Clustering.html">
   25 聚簇
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="26.GraphicalModelStructureLearning.html">
   26 概率图的结构学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="27.LatentVariableModelsForDiscreteData.html">
   27 离散数据的隐变量模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="28.DeepLearning.html">
   28 深度学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Abbreviations.html">
   本书常见缩写列表
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/MLAPP_BOOK/12.LinearModelwithLatentVariable.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#factor-analysis">
   12.1 因子分析(Factor analysis)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#famvn">
     12.1.1 FA是MVN的一个低秩版的参数化
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     12.1.2 隐藏因子的推理
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>12 隐变量线性模型</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#factor-analysis">
   12.1 因子分析(Factor analysis)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#famvn">
     12.1.1 FA是MVN的一个低秩版的参数化
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     12.1.2 隐藏因子的推理
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>12 隐变量线性模型<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<div class="section" id="factor-analysis">
<h2>12.1 因子分析(Factor analysis)<a class="headerlink" href="#factor-analysis" title="Permalink to this headline">¶</a></h2>
<p>混合模型的局限性在于它只能使用一个隐变量来生成观察值。特别地，每个观察值只能属于<span class="math notranslate nohighlight">\(K\)</span>个原型之一。我们可以认为混合模型中使用的隐变量是某个聚簇身份的one-hot编码，其中包含了<span class="math notranslate nohighlight">\(K\)</span>个二元隐变量。但因为这些变量之间是互斥的，所以模型的表达能力仍然是有限的。</p>
<p>一种替代方案是使用一个实数隐变量<span class="math notranslate nohighlight">\(\mathbf{z}_i\in \mathbb{R}^L\)</span>，并使用最简单的先验分布：高斯分布（我们会在下文考虑其他的先验分布），即：
$<span class="math notranslate nohighlight">\(
p(\mathbf{z}_i)=\mathcal{N}(\mathbf{z}_i|\mathbf{\mu}_0,\mathbf{\Sigma}_0) \tag{12.1}
\)</span><span class="math notranslate nohighlight">\(
如果观察值是连续值，即\)</span>\mathbf{x}_i \in \mathbb{R}^D<span class="math notranslate nohighlight">\(，我们或许可以使用一个高斯分布作为似然函数。就像在线性回归中的那样，假设期望是（隐藏）输入的线性函数，从而得到：
\)</span><span class="math notranslate nohighlight">\(
p(\mathbf{x}_i|\mathbf{z}_i,\mathbf{\theta})=\mathcal{N}(\mathbf{W}\mathbf{z}_i+\mathbf{\mu},\mathbf{\Psi}) \tag{12.2}
\)</span><span class="math notranslate nohighlight">\(
其中\)</span>\mathbf{W}<span class="math notranslate nohighlight">\(是一个\)</span>D\times L<span class="math notranslate nohighlight">\(的矩阵，又被称为**因子载入矩阵（factor loading matrix）**,\)</span>\mathbf{\Psi}<span class="math notranslate nohighlight">\(是一个\)</span>D \times D<span class="math notranslate nohighlight">\(的协方差矩阵。我们令\)</span>\mathbf{\Psi}<span class="math notranslate nohighlight">\(是一个对角矩阵，因为该模型的一个主要特点是“强迫”\)</span>\mathbf{z}_i<span class="math notranslate nohighlight">\(来解释变量之间的相关性，而不是将“这件事”交给观察值的协方差矩阵。整个模型被称为**因子分析(factor analysis,FA)**。一种特殊的情况是\)</span>\mathbf{\Psi}=\sigma^2\mathbf{I}$，此时模型被称为<strong>概率性主元分析(probabilistic principal components,PPCA)</strong>，这个名字的来源我们将在后文介绍。</p>
<p>图12.1展示了当<span class="math notranslate nohighlight">\(L=1,D=2,\mathbf{\Sigma}_0\)</span>为球状协方差矩阵，<span class="math notranslate nohighlight">\(\mathbf{\Psi}\)</span>为对角矩阵时模型的示意图。我们使用一个各向同性的高斯分布作为<span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span>的先验，然后将它沿着<span class="math notranslate nohighlight">\(1\)</span>维直线<span class="math notranslate nohighlight">\(\mathbf{w}z_i+\mathbf{\mu}\)</span>滑动（译者注：注意这里是滑动，不是采样得到单一的<span class="math notranslate nohighlight">\(z_i\)</span>值）。最终导致一个在<span class="math notranslate nohighlight">\(2\)</span>维空间中被拉长的（也就是相关的）高斯分布，该分布包含了沿对角线分布的噪声。
$<span class="math notranslate nohighlight">\(
figure 12.1
\)</span>$</p>
<div class="section" id="famvn">
<h3>12.1.1 FA是MVN的一个低秩版的参数化<a class="headerlink" href="#famvn" title="Permalink to this headline">¶</a></h3>
<p>FA可以认为是一种使用较少参数来描述变量<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>的联合分布的方式。为了说明这一点，根据式4.126，推导出来的边缘分布<span class="math notranslate nohighlight">\(p(\mathbf{x}_i|\mathbf{\theta})\)</span>是一个高斯分布：
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-0b51e280-a134-4e5e-a6b1-83f1b50da76d">
<span class="eqno">(8)<a class="headerlink" href="#equation-0b51e280-a134-4e5e-a6b1-83f1b50da76d" title="Permalink to this equation">¶</a></span>\[\begin{align}
p(\mathbf{x}_i|\mathbf{\theta}) &amp; =\int\mathcal{N}(\mathbf{x}_i|\mathbf{W}\mathbf{z}_i+\mathbf{\mu},\mathbf{\Psi})\mathcal{N}(\mathbf{z}_i|\mathbf{\mu}_0,\mathbf{\Sigma}_0)d\mathbf{z}_i  \tag{12.3} \\ 
&amp; = \mathcal{N}(\mathbf{x}_i|\mathbf{W}\mathbf{\mu}_0+\mathbf{\mu},\mathbf{\Psi}+\mathbf{W}\mathbf{\Sigma}_0\mathbf{W}^T) \tag{12.4}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[
因此$\mathbb{E}[\mathbf{x}]=\mathbf{W}\mathbf{\mu}_0+\mathbf{\mu},\rm{cov}[\mathbf{x}]=\mathbf{W}\mathbb{E}[\mathbf{z}\mathbf{z}^T]\mathbf{W}^T+\mathbf{\Psi}=\mathbf{W}\mathbf{\Sigma}_0\mathbf{W}^T+\mathbf{\Psi}$。不失一般性，我们可以设置$\mathbf{\mu}_0=\mathbf{0}$,因为我们总可以使用$\mu$来包含$\mathbf{W}\mathbf{\mu}_0$的影响。类似地，不失一般性，我们可以令$\mathbf{\Sigma}_0=\mathbf{I}$，因为通过定义一个新的权重矩阵$\tilde{\mathbf{W}}=\mathbf{W}\mathbf{\Sigma}_0^{-\frac{1}{2}}$，我们总可以模拟出一个可以模拟相关性的高斯分布，因为：
\]</div>
<p>\rm{cov}[\mathbf{x}]=\mathbf{W}\Sigma_0\mathbf{W}^T+\mathbf{\Psi}=\tilde{\mathbf{W}}\tilde{\mathbf{W}}^T+\mathbf{\Psi} \tag{12.5}
$<span class="math notranslate nohighlight">\(
因此我们发现FA使用一个低秩的分解，近似地模拟了观测向量的协方差矩阵：
\)</span><span class="math notranslate nohighlight">\(
\mathbf{C}\triangleq\rm{cov}[\mathbf{x}]=\mathbf{W}\mathbf{W}^T+\mathbf{\Psi} \tag{12.6}
\)</span><span class="math notranslate nohighlight">\(
上式需要的参数数量的数量级为\)</span>O(LD)<span class="math notranslate nohighlight">\(,从而实现了在包含\)</span>O(D^2)<span class="math notranslate nohighlight">\(个参数的全协方差矩阵与包含\)</span>O(D)<span class="math notranslate nohighlight">\(个参数的对角协方差矩阵之间的灵活压缩。值得注意的是，如果我们不强制\)</span>\mathbf{\Psi}<span class="math notranslate nohighlight">\(为对角矩阵，反而将其设置为全协方差矩阵，这样的话就可以令\)</span>\mathbf{W}=\mathbf{0}<span class="math notranslate nohighlight">\(，此时，模型也就不再需要隐变量\)</span>\mathbf{z}$。</p>
<p>每个可观测变量的边缘方差定义为<span class="math notranslate nohighlight">\({\rm{var}}[x_j]=\sum_{k=1}^Lw_{jk}^2+\psi_j\)</span>，其中第一项是共享因子导致的方差，第二项<span class="math notranslate nohighlight">\(\psi_j\)</span>被称为<strong>唯一性(uniqueness)</strong>，它特指那个维度的方差项。</p>
</div>
<div class="section" id="id2">
<h3>12.1.2 隐藏因子的推理<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>尽管FA只是被认为是一种定义关于变量<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>概率分布的方式，但我们经常使用它，是因为我们希望隐藏因子<span class="math notranslate nohighlight">\(\mathbf{z}\)</span>可以揭露一些关于数据的有趣的规律。为了实现这一点，我们需要计算关于隐藏因子的后验分布。基于贝叶斯法则，可以得出：
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-1b1cf188-4aec-4737-ad2d-81e7a7bda086">
<span class="eqno">(9)<a class="headerlink" href="#equation-1b1cf188-4aec-4737-ad2d-81e7a7bda086" title="Permalink to this equation">¶</a></span>\[\begin{alignat}{3}
p(\mathbf{z}_i|\mathbf{x}_i,\mathbf{\theta}) &amp; =  \mathcal{N}(\mathbf{z}_i|\mathbf{m}_i,\mathbf{\Sigma}_i)   \tag{12.7} \\
\mathbf{\Sigma}_i   &amp; \triangleq  (\mathbf{\Sigma}_0^{-1}+\mathbf{W}^T\mathbf{\Psi}^{-1}\mathbf{W})^{-1} \tag{12.8} \\
\mathbf{m}_i &amp; \triangleq \mathbf{\Sigma}_i(\mathbf{W}^T\mathbf{\Psi}^{-1}(\mathbf{x}_i-\mathbf{\mu})+\mathbf{\Sigma}_0^{-1}\mathbf{\mu}_0) \tag{12.9}
\end{alignat}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
值得注意的是，在FA模型中，$\mathbf{\Sigma}_i$事实上与$i$无关，所以我们可以将其表示为$\mathbf{\Sigma}$。计算这个矩阵所花费的时间复杂度为$O(L^3+L^2D)$，计算每个$\mathbf{m}_i=\mathbb{E}[\mathbf{z}_i|\mathbf{x}_i,\mathbf{\theta}]$的时间复杂度为$O(L^2+LD)$。$\mathbf{m}_i$有时又被称为隐藏**分值(scores)**或者隐藏**因子(factors)**。\\让我们给出一个简单的例子。考虑一个数据集，其中变量的数量$D=11$,样本数量$N=387$，这些变量描述了不同车辆的各种方面，比如发动机大小，气缸数量，每加仑公里路(miles per gallon,MPG)，价格等等。我们首先训练一个$L=2$的模型。我们可以在二维空间中绘制$\mathbf{m}_i$来实现数据的可视化，结果如图12.2所示。\\为了进一步加深对隐藏因子的理解，我们可以将每个特征维度对应的单位向量$\mathbf{e}_1=(1,0,...,0),\mathbf{e}_2=(0,1,0,...,0)$等投影到一个低维度的空间。这些结果如图中蓝色直线所示，这被称为**双标图(biplot)**。我们发现水平轴代表价格，对应于特征&quot;经销商&quot;和&quot;零售&quot;，且贵的车在右侧。垂直轴对应于燃油效率（以MPG进行衡量）与尺寸的比例：重型车辆效率更低且分布在上面，然而轻型车辆效率更高且分布在下面。我们可以通过点击某些点来验证这种解释，找到在训练集中距离最近的样例，然后打印出它们的名字。然而，通常情况下，对隐藏因子进行解释是非常困难的，这一点我们会在12.1.3节进行解释。\\### 12.1.3 无法辨别性\\FA模型的参数具有不可辨别性。为了说明这一点，假设$\mathbf{R}$是一个任意的正交旋转矩阵，满足$\mathbf{R}\mathbf{R}^T=\mathbf{I}$。定义$\tilde{\mathbf{W}}=\mathbf{WR}$，基于这个变体矩阵的似然函数与没变化之前的结果是一样的，因为
\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight" id="equation-bfc2bed5-6704-43aa-ab00-dfff11e75bed">
<span class="eqno">(10)<a class="headerlink" href="#equation-bfc2bed5-6704-43aa-ab00-dfff11e75bed" title="Permalink to this equation">¶</a></span>\[\begin{align}
{\rm{cov}}[\mathbf{x}] &amp; = \tilde{\mathbf{W}}\mathbb{E}[\mathbf{z}\mathbf{z}^T]\tilde{\mathbf{W}}^T+\mathbb{E}[\mathbf{\epsilon\epsilon}^T] \tag{12.10} \\
&amp; = \mathbf{WR}\mathbf{R}^T\mathbf{W}^T+\mathbf{\Psi}=\mathbf{W}\mathbf{W}^T+\mathbf{\Psi} \tag{12.11}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
几何上，使用一个正交矩阵与$\mathbf{W}$相乘等价于在生成$\mathbf{x}$之前旋转$\mathbf{z}$；但因为$\mathbf{z}$是从各向同性的高斯分布中采样得到的，所以导致最终的似然函数没有任何变化。因此，我们没有办法对$\mathbf{W}$进行唯一确定，也因此我们没有办法唯一确定隐藏因子。\\为了确定一个唯一的解，考虑到正交矩阵$\mathbf{R}$的大小为$L×L$，所以我们需要移除$L(L-1)/2$个自由度。整体上，$FA$模型有$D+LD-L(L-1)/2$个自由参数（不包括期望），第一项为$\mathbf{\Psi}$的参数量。显然我们需要模型参数量小于或者等于$D(D+1)/2$，该值就是一个非受限（但是对称）的协方差矩阵的参数量。从而我们得到一个关于$L$的上限值：
\end{aligned}\end{align} \]</div>
<p>L_{max}=\left \lfloor D+0.5(1-\sqrt{1+8D}) \right \rfloor \tag{12.12}
$<span class="math notranslate nohighlight">\(
举例来说，如果\)</span>D=6<span class="math notranslate nohighlight">\(，则\)</span>L\le3<span class="math notranslate nohighlight">\(。但通常情况下我们并不会选择这个上界，因为这会导致过拟合（12.3节将讨论如何选择\)</span>L$）。</p>
<p>不幸的是，即使我们令<span class="math notranslate nohighlight">\(L&lt;L_{max}\)</span>，我们依然不能唯一的确定参数，因为旋转的不确定性依然存在。不可辨识性并不影响模型的预测性能。然而，它对载入矩阵存在影响，从而导致对潜在因子的解释存在不确定性。因为因子分析通常是用来揭示数据的结构，所以这个问题需要被解决。下面是一些常见的解决方案：</p>
<ul>
<li><p>强制要求<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>为正交矩阵  可能解决不可辨识性问题的最直接的方案是强制<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>为正交矩阵，然后按照对应的潜在因子的方差大小来对矩阵的列进行排序。这是PCA中所使用的方法，这一点我们将在12.2节讨论。其结果不一定更具备可解释性，但至少结果是唯一确定的。</p></li>
<li><p>强制要求<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>为下三角矩阵 一种在贝叶斯领域十分著名的方法是强制要求第一个观测变量只由第一个潜在因子产生。第二个观测变量仅由前两个潜在因子产生，以此类推。举例来说，如果<span class="math notranslate nohighlight">\(L=3,D=4\)</span>，对应的因子载入矩阵为：
$$
\mathbf{W}=</p>
<div class="amsmath math notranslate nohighlight" id="equation-f324526e-4f6f-479b-8a81-653accfb8d58">
<span class="eqno">(11)<a class="headerlink" href="#equation-f324526e-4f6f-479b-8a81-653accfb8d58" title="Permalink to this equation">¶</a></span>\[\begin{pmatrix}
  w_{11} &amp; 0 &amp; 0 \\
  w_{21} &amp; w_{22} &amp; 0 \\
  w_{31} &amp; w_{32} &amp; w_{33} \\
  w_{41} &amp; w_{42} &amp; w_{43}
  \end{pmatrix}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
  同时我们要求$w_{jj}&gt;0,j=1:L$。在这个受约束的矩阵中整体的参数量为$D+DL-L(L-1)/2$,与唯一可辨识的参数量相等。该方法的缺点在于，前$L$个被称为**founder 变量**的可观测变量，会影响潜在因子的解释，所以必须小心地进行选择。\\- 为权重增加稀疏控制先验 相较于提前指定矩阵$\mathbf{W}$中的哪些元素应该为0，我们可以使用$\mathcal{l}_1$正则，ARD或者spike-and-slab先验来鼓励矩阵中的某些元素为0。这被称为稀疏因子分析。这种方法不一定能确保一个唯一的MAP估计，但它鼓励具备可解释性的解。13.8节将介绍有关稀疏编码的细节。\\- 选择一个信息旋转矩阵 存在很多启发式的方式，来尝试去找到一个旋转矩阵$\mathbf{R}$，来修正矩阵$\mathbf{W}$以及潜在因子，从而增加可解释性，典型情况下是鼓励他们变得稀疏。一种著名的方式是$\mathbf{varimax}$（方差最大化旋转）。\\- 对潜在因子使用非高斯先验 如果我们将潜在因子的先验分布$p(\mathbf{z}_i)$替换成一个非高斯分布，有的时候我们可以唯一确定矩阵$\mathbf{W}$以及潜在因子。这个技术被称为独立因素分析或者ICA，这一点将在12.6节进行解释。\\### 12.1.4 混合因子分析\\FA模型假设数据服从一个低维的线性流型。在现实场景中，大部分数据更适合使用一些低维“曲线”流型来建模。我们可以使用多个局部线性流型来近似一个曲线流型。这将导致如下的模型：令第$k$个线性子空间的维度为$L_k$，由$\mathbf{W}_k$表示。其中$k=1:K$。假设我们有一个潜在因子指示变量$q_i \in\{1,...,K\}$，用来指定使用哪一个子空间来生成数据。我们从一个高斯先验分布中采样$\mathbf{z}_i$，并将它传递给矩阵$\mathbf{W}_k(k=q_i)$，并且添加噪声。更加精确的模型表示为：
\end{aligned}\end{align} \]</div>
</li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-10a9848e-d38c-4cac-bf25-740c81a91361">
<span class="eqno">(12)<a class="headerlink" href="#equation-10a9848e-d38c-4cac-bf25-740c81a91361" title="Permalink to this equation">¶</a></span>\[\begin{align}
p(\mathbf{x}_i|\mathbf{z}_i,q_i=k,\mathbf{\theta}) &amp; = \mathcal{N}(\mathbf{x}_i|\mathbf{\mu}_k+\mathbf{W}_k\mathbf{z}_i,\mathbf{\Psi})  \tag{12.14} \\
p(\mathbf{z}_i|\mathbf{\theta}) &amp; = \mathcal{N}(\mathbf{z}_i|\mathbf{0},\mathbf{I}) \tag{12.15} \\
p(q_i|\mathbf{\theta}) &amp; = {\rm{Cat}}(q_i|\mathbf{\pi}) \tag{12.16}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
这被称为**混合因子分析(mixture of factor analysers,MFA)**。图12.3表示了其中的条件独立假设。\\我们也可以将这个模型看做是混合高斯模型的低秩版本，特别地，该模型的空间复杂度为$O(KLD)$，而非$O(KD^2)$，后者对应全协方差矩阵。这将有利于减少过拟合。事实上，对于高维的实数变量数据而言，MFA是一个很好的通用密度模型。\\### 12.1.5 因子分析模型的EM算法\\使用第4章的结果，我们可以直接推导出训练一个FA模型所使用的EM算法。只需要一些额外的工作，我们就可以训练一个混合FA模型。接下来，我们将直接展示结果，而不给出证明。\\为了获得单个因子分析器的结果，只需要在下面的公式中设置$r_{ic}=1$和$c=1$。在12.2.5节，我们将看到这些公式的进一步的简化版，这个版本就是训练PPCA模型的算法，其结果将具备一个特别简单而且优雅的解释。\\回到最一般的情况：在步骤$\rm{E}$中，我们计算数据点$i$属于聚簇$c$的后验概率：
\end{aligned}\end{align} \]</div>
<p>r_{ic}\triangleq p(q_i=c|\mathbf{x}_i,\mathbf{\mathbf{\theta}}) = \pi_c\mathcal{N}(\mathbf{x}_i|\mathbf{\mu}_c,\mathbf{W}_c\mathbf{W}_c^T+\mathbf{\Psi}) \tag{12.17}
$<span class="math notranslate nohighlight">\(
关于\)</span>\mathbf{z}_i<span class="math notranslate nohighlight">\(的条件后验分布为:
\)</span>$</p>
<div class="amsmath math notranslate nohighlight" id="equation-6c3d28bd-06ac-4699-9f1b-20ea0775712d">
<span class="eqno">(13)<a class="headerlink" href="#equation-6c3d28bd-06ac-4699-9f1b-20ea0775712d" title="Permalink to this equation">¶</a></span>\[\begin{align}
p(\mathbf{z}_i|\mathbf{x}_i,q_i=c,\mathbf{\theta}) &amp; = \mathcal{N}(\mathbf{z}_i|\mathbf{m}_{ic},\mathbf{\Sigma}_{ic}) \tag{12.18} \\
\mathbf{\Sigma}_{ic} &amp;=(\mathbf{I}_L+\mathbf{W}_c^T\mathbf{\Psi}_c^{-1}\mathbf{W}_c)^{-1} \tag{12.19} \\
\mathbf{m}_{ic} &amp;= \mathbf{\Sigma}_{ic}(\mathbf{W}_c^T\mathbf{\Psi}_c^{-1}(\mathbf{x}_i-\mathbf{\mu}_c)) \tag{12.20}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
待定\\##  \\### 12.1.6 含缺失值的FA模型训练\\## 12.2 主元分析(Principal components analysis,PCA)\\考虑在FA模型中添加约束:$\mathbf{\Psi}=\sigma^2\mathbf{I}$,$\mathbf{W}$为正交矩阵。结果表明，当$\sigma^2\rightarrow 0$时，该模型将退化为典型的(非概率性的)主元分析(PCA)，又被称为Karhunen Loeve变换。如果$\sigma^2 \gt 0$，则该模型被称为**概率性PCA(probabilistic PCA)**或者**敏感性PCA(sensible PCA)**。\\为了使上述结果更加直观，我们首先需要了解经典的PCA。接着将PCA与SVD相联系。最后我们回到对PPCA的讨论。\\### 12.2.1 经典PCA:定理阐述\\经典PCA的综合观点可以总结为如下理论。\\**定理12.2.1.** 假设我们希望找到一个由$L$个向量$\mathbf{w}_j\in\mathbb{R}^D$组成的正交基，以及每个样本所对应的分值$\mathbf{z}_i\in \mathbb{R}^L$，从而使得平均重构误差(**reconstruction error**)达到最小
\end{aligned}\end{align} \]</div>
<p>J(\mathbf{W},\mathbf{Z})=\frac{1}{N}||\mathbf{x}_i-\hat{\mathbf{x}}_i||^2 \tag{12.26}
$<span class="math notranslate nohighlight">\(
其中\)</span>\hat{\mathbf{x}}_i=\mathbf{W}\mathbf{z}_i<span class="math notranslate nohighlight">\(，\)</span>\mathbf{W}<span class="math notranslate nohighlight">\(为正交矩阵。等价地，我们可以将上述目标写成如下形式：
\)</span><span class="math notranslate nohighlight">\(
J(\mathbf{W},\mathbf{Z})=||\mathbf{X}-\mathbf{WZ}^T||^2_F \tag{12.27}
\)</span><span class="math notranslate nohighlight">\(
其中\)</span>\mathbf{Z}<span class="math notranslate nohighlight">\(为一个\)</span>N\times L<span class="math notranslate nohighlight">\(的矩阵，\)</span>\mathbf{z}_i<span class="math notranslate nohighlight">\(为它的每一行。\)</span>||\mathbf{A}||_F<span class="math notranslate nohighlight">\(为矩阵\)</span>\mathbf{A}$的<strong>Frobenius</strong>范数。</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./MLAPP_BOOK"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="11.MixtureModelandEMAlgorithm.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">11 混合模型与 EM 算法</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="13.SparseLinearModel.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">13 稀疏线性模型</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Kevin Murphy<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>